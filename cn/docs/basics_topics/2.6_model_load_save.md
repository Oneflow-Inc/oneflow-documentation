### ***2.6 模型的加载与保存***



#### 2.6.1 保存预训练模型的意义

我们知道在目前的深度学习神经网络中，训练过程是基于梯度下降法来进行参数优化的。通过迭代进而求出最小的损失函数与最优的模型权重。在进行梯度下降时，我们需要给每一个参数赋予一个初始值。经过实践证明，通过加载已保存的预训练模型作为初始化，模型的训练速度更快，使模型更快地收敛，并且模型性能更高。这也诠释了我们要保存预训练模型的意义。



#### 2.6.2 模型的保存与加载

关于模型的保存与加载，我们可以通过以下两个方法来实现保存/加载模型：

- `oneflow.checkpoint.save` : 负责保存当前的模型到指定路径
- `oneflow.checkpoint.get` : 从指定路径中导入模型

`save` 的原型如下，可以将模型保存至 `path` 所指定的路径。

```
def save(path, var_dict=None)
```

可选参数 `var_dict` 如果不为 `None`，则将 `var_dict` 中指定的对象保存到指定路径。

`get` 的原型如下，可以加载之前已经保存的，由 `path` 路径所指定的模型。

```
def get(path)
```

它将返回一个字典，该字典可以用上文介绍的 `load_variables` 方法更新到模型中：

```
flow.load_variables(flow.checkpoint.get(save_dir))
```

**注意**：

- `save` 参数所指定路径对应的目录要么不存在，要么应该为空目录，否则 `save` 会报错(防止覆盖掉原有保存的模型)
- OneFlow 模型以一定的组织形式保存在指定的路径中，具体结构参见下文中的 OneFlow 模型的存储结构
- 虽然 OneFlow 对 `save` 的频率没有限制，但是过高的保存频率，会加重磁盘及带宽等资源的负担。





#### 2.6.3 OneFlow 的模型保存格式

OneFlow 模型是一组已经被训练好的网络的 **参数值** 。模型所保存的路径下，有多个子目录，每个子目录对应了 `作业函数` 中模型的 `name`。 比如，我们先通过代码定义以下的模型：

```
def lenet(data, train=False):
    initializer = flow.truncated_normal(0.1)
    conv1 = flow.layers.conv2d(
        data,
        32,
        5,
        padding="SAME",
        activation=flow.nn.relu,
        name="conv1",
        kernel_initializer=initializer,
    )
    pool1 = flow.nn.max_pool2d(
        conv1, ksize=2, strides=2, padding="SAME", name="pool1", data_format="NCHW"
    )
    conv2 = flow.layers.conv2d(
        pool1,
        64,
        5,
        padding="SAME",
        activation=flow.nn.relu,
        name="conv2",
        kernel_initializer=initializer,
    )
    pool2 = flow.nn.max_pool2d(
        conv2, ksize=2, strides=2, padding="SAME", name="pool2", data_format="NCHW"
    )
    reshape = flow.reshape(pool2, [pool2.shape[0], -1])
    hidden = flow.layers.dense(
        reshape,
        512,
        activation=flow.nn.relu,
        kernel_initializer=initializer,
        name="dense1",
    )
    if train:
        hidden = flow.nn.dropout(hidden, rate=0.5, name="dropout")
    return flow.layers.dense(hidden, 10, kernel_initializer=initializer, name="dense2")
```

假设在训练过程中，我们调用以下代码保存模型：

```
flow.checkpoint.save('./lenet_models_name')
```

那么 `lenet_models_name` 及其子目录结构为：

```
lenet_models_name/
├── conv1-bias
│   ├── meta
│   └── out
├── conv1-weight
│   ├── meta
│   └── out
├── conv2-bias
│   ├── meta
│   └── out
├── conv2-weight
│   ├── meta
│   └── out
├── dense1-bias
│   ├── meta
│   └── out
├── dense1-weight
│   ├── meta
│   └── out
├── dense2-bias
│   ├── meta
│   └── out
├── dense2-weight
│   ├── meta
│   └── out
├── snapshot_done
└── System-Train-TrainStep-train_job
    ├── meta
    └── out
```

可以看到：

- 作业函数中的网络模型，每个变量对应一个子目录
- 以上每个子目录中，都有 `out` 和 `meta` 文件，`out` 以二进制的形式存储了网络参数的值，`meta` 以文本的形式存储了网络的结构信息
- `snapshot_done` 是一个空文件，如果它存在，表示网络已经训练完成
- `System-Train-TrainStep-train_job` 中保存有快照的训练步数



在了解了oneflow中`模型的保存格式`后，我们可能经常会遇到模型的微调和迁移学习，这时我们需要

- 模型中的一部分参数加载自原有模型
- 模型中的另一部分（新增的）参数需要初始化

我们可以使用 `oneflow.load_variables` 完成以上操作。以下举一个用于阐述概念的简单例子。

首先，我们先定义一个模型，训练后保存至 `./mlp_models_1`：

```
@flow.global_function(type="train")
def train_job(
    images: tp.Numpy.Placeholder((BATCH_SIZE, 1, 28, 28), dtype=flow.float),
    labels: tp.Numpy.Placeholder((BATCH_SIZE,), dtype=flow.int32),
) -> tp.Numpy:
    with flow.scope.placement("cpu", "0:0"):
        initializer = flow.truncated_normal(0.1)
        reshape = flow.reshape(images, [images.shape[0], -1])
        hidden = flow.layers.dense(
            reshape,
            512,
            activation=flow.nn.relu,
            kernel_initializer=initializer,
            name="dense1",
        )
        dense2 = flow.layers.dense(
            hidden, 10, kernel_initializer=initializer, name="dense2"
        )

        loss = flow.nn.sparse_softmax_cross_entropy_with_logits(labels, dense2)

    lr_scheduler = flow.optimizer.PiecewiseConstantScheduler([], [0.1])
    flow.optimizer.SGD(lr_scheduler, momentum=0).minimize(loss)

    return loss
```

然后，我们拓展网络结构，为以上模型多增加一层 `dense3`：

```
@flow.global_function(type="train")
def train_job(
    images: tp.Numpy.Placeholder((BATCH_SIZE, 1, 28, 28), dtype=flow.float),
    labels: tp.Numpy.Placeholder((BATCH_SIZE,), dtype=flow.int32),
) -> tp.Numpy:
    with flow.scope.placement("cpu", "0:0"):
        #... 原有网络结构

        dense3 = flow.layers.dense(
            dense2, 10, kernel_initializer=initializer, name="dense3"
        )
        loss = flow.nn.sparse_softmax_cross_entropy_with_logits(labels, dense3)

    #...
```

最后，从原来保存的模型加载参数，并开始训练：

```
if __name__ == "__main__":
    flow.load_variables(flow.checkpoint.get("./mlp_models_1"))

    (train_images, train_labels), (test_images, test_labels) = flow.data.load_mnist(
        BATCH_SIZE, BATCH_SIZE
    )
    for i, (images, labels) in enumerate(zip(train_images, train_labels)):
        loss = train_job(images, labels)
        if i % 20 == 0:
            print(loss.mean())
    flow.checkpoint.save("./mlp_ext_models_1")
```

新增的 `dense3` 层参数，在原模型中不存在，OneFlow 会自动初始化它们的值。

### 代码

脚本 [mlp_mnist_origin.py](https://github.com/Oneflow-Inc/oneflow-documentation/blob/modeldoc/cn/docs/code/basics_topics/mlp_mnist_origin.py) 中构建了“骨干网络”，并将训练好的模型保存至 `./mlp_models_1`。

运行：

```
wget https://docs.oneflow.org/code/basics_topics/mlp_mnist_origin.py
python3 mlp_mnist_origin.py
```

训练完成后，将会在当前工作路径下得到 `mlp_models_1` 目录。

脚本 [mlp_mnist_finetune.py](https://github.com/Oneflow-Inc/oneflow-documentation/blob/modeldoc/cn/docs/code/basics_topics/mlp_mnist_finetune.py) 中的网络在原有基础上进行“微调”（为骨干网络增加一层`dense3`）后，加载 `./mlp_models_1`，并继续训练。

运行：

```
wget https://docs.oneflow.org/code/basics_topics/mlp_mnist_finetune.py
python3 mlp_mnist_finetune.py
```

微调后的模型，保存在 `./mlp_ext_models_1` 中。