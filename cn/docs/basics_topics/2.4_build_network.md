### 2.4 搭建神经网络

​	神经网络由对数据执行操作的层/模块组成。 oneflow.nn 命名空间提供了构建自己的神经网络所需的所有构建块。 oneflow 中的每个模块都是 nn.Module 的子类。神经网络是一个模块本身，由其他模块（层）组成。这种嵌套结构允许其轻松构建和管理复杂的架构。 

#### 2.4.1 `flow.nn.Module` 与 `flow.F`之间的关系

待补充

#### 2.4.2 `nn.Module` 的常见方法及常见 `Module`

​	`nn.Module` 有 12 个属性，其中有8个是`OrderDict`(有序字典)。 我们在创建神经网络的时候， `__init__()`方法中会调用父类`nn.Module`的`__init__()`方法，创建这 8 个属性。 

```python
class Module(object):
    def __init__(self):
            self.training = True
            self._consistent = False
            self._non_persistent_buffers_set = set()
            self._is_full_backward_hook = None

            self._parameters = OrderedDict()
            self._buffers = OrderedDict()
            self._backward_hooks = OrderedDict()
            self._forward_hooks = OrderedDict()
            self._forward_pre_hooks = OrderedDict()
            self._state_dict_hooks = OrderedDict()
            self._load_state_dict_pre_hooks = OrderedDict()
            self._modules = OrderedDict()
```

-  _parameters 属性：存储管理 nn.Parameter 类型的参数 

-  _modules 属性：存储管理 nn.Module 类型的参数 

-  _buffers 属性：存储管理缓冲属性

-  5 个 ***_hooks 属性：存储管理钩子函数



其中比较重要的是`parameters`和`modules`属性。 

常见的`modules`，例如`nn.Conv1d()`、`nn.Conv2d()` 、`nn.Conv3d()`和`nn.Linear()`都是 继承于`nn.module`，也就是说一个 module 都是包含多个子 module 的。 



#### 2.4.3 `Module Container`

除了上述模块，`Module `另外一个重要的概念是模型容器 (Containers)，常用的容器有 3 个，这些容器都是继承自`nn.Module`。 

-  nn.Sequetial：按照顺序包装多个网络层 

- nn.ModuleList： 像 python 的 list 一样包装多个网络层，可以进行迭代

- nn.ModuleDict：像 python 的 dict一样包装多个网络层，通过 (key, value) 的方式为每个网络层指定名称。 



##### nn.Sequetial

 `nn.Sequetial`是`nn.Module`的容器，用于按顺序包装一组网络层，有以下两个特性。 

-  顺序性：各网络层之间严格按照顺序构建，我们在构建网络时，一定要注意前后网络层之间输入和输出数据之间的形状是否匹配 
-  自带`forward()`函数：在`nn.Sequetial`的`forward()`函数里通过 for 循环依次读取每个网络层，执行前向传播运算。这使得我们我们构建的模型更加简洁 

```python
import oneflow as flow
import oneflow.nn as nn

class LeNet(nn.Module):
    def __init__(self):
        super(LeNet, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 6, 5), # in_channels, out_channels, kernel_size
            nn.Sigmoid(),
            nn.MaxPool2d(2, 2), # kernel_size, stride
            nn.Conv2d(6, 16, 5),
            nn.Sigmoid(),
            nn.MaxPool2d(2, 2)
        )
        self.fc = nn.Sequential(
            nn.Linear(16*4*4, 120),
            nn.Sigmoid(),
            nn.Linear(120, 84),
            nn.Sigmoid(),
            nn.Linear(84, 10)
        )

    def forward(self, img):
        feature = self.conv(img)
        output = self.fc(feature.view(img.shape[0], -1))
        return output
        
net = LeNet()
print("LeNet \n",net)       
```





##### nn.ModuleList

 `nn.ModuleList`是`nn.Module`的容器，用于包装一组网络层，以迭代的方式调用网络层，主要有以下 3 个方法： 

-  append()：在 ModuleList 后面添加网络层 

-   extend()：拼接两个 ModuleList 

-   insert()：在 ModuleList 的指定位置中插入网络层 

```python
import oneflow as flow
import oneflow.nn as nn

class ModuleList(nn.Module):
    def __init__(self):
        super(ModuleList, self).__init__()
        self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(20)])

    def forward(self, x):
        for i, linear in enumerate(self.linears):
            x = linear(x)
        return x

net = ModuleList()
print(net)
```

#####  

##### nn.ModuleDict 

 `nn.ModuleDict`是`nn.Module`的容器，用于包装一组网络层，以索引的方式调用网络层，主要有以下 5 个方法： 

-  clear()：清空 ModuleDict 

-  items()：返回可迭代的键值对 (key, value) 

-  keys()：返回字典的所有 key 

-  values()：返回字典的所有 value 

-  pop()：返回一对键值，并从字典中删除 

下面的模型创建了两个`ModuleDict`：`self.choices`和`self.activations`，在前向传播时通过传入对应的 key 来执行对应的网络层。

```python
import oneflow as flow
import oneflow.nn as nn

class ModuleDict(nn.Module):
    def __init__(self):
        super(ModuleDict, self).__init__()
        self.choices = nn.ModuleDict({
            'conv': nn.Conv2d(10, 10, 3),
            'pool': nn.MaxPool2d(3)
        })

        self.activations = nn.ModuleDict({
            'relu': nn.ReLU(),
            'prelu': nn.PReLU()
        })

    def forward(self, x, choice, act):
        x = self.choices[choice](x)
        x = self.activations[act](x)
        return x


net = ModuleDict()
print(net)
```






#### 2.4.4 神经网络实例AlexNet 

AlexNet 是 Hinton 和他的学生等人在 2012 年提出的卷积神经网络，以高出第二名 10 多个百分点的准确率获得 ImageNet 分类任务冠军，从此卷积神经网络开始在世界上流行，是划时代的贡献。 

 AlexNet 特点如下： 

-  采用 ReLU 替换饱和激活 函数，减轻梯度消失 
-  采用 LRN (Local Response Normalization) 对数据进行局部归一化，减轻梯度消失 
-  采用 Dropout 提高网络的鲁棒性，增加泛化能力 
-  使用 Data Augmentation，包括 TenCrop 和一些色彩修改 

```python
import oneflow as flow
import oneflow.nn as nn

class AlexNet(nn.Module):

    def __init__(self, num_classes=1000):
        super(AlexNet, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),
            nn.Conv2d(64, 192, kernel_size=5, padding=2),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),
            nn.Conv2d(192, 384, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(384, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),
        )
        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))
        self.classifier = nn.Sequential(
            nn.Dropout(),
            nn.Linear(256 * 6 * 6, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Linear(4096, num_classes),
        )

    def forward(self, x):
        x = self.features(x)
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.classifier(x)
        return x
    
net=AlexNet()
print('AlexNet network',net)
```

