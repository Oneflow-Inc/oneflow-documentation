
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="OneFlow: a efficient distributed deep learning framework.">
      
      
      
      
        <link rel="canonical" href="https://docs.oneflow.org/v0.4.0/extended_topics/user_op.html">
      
      <link rel="icon" href="../assets/favicon.png">
      <meta name="generator" content="mkdocs-1.2.2, mkdocs-material-7.1.11">
    
    
      
        <title>User Defined OP - OneFlow</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.3754935a.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.f1a3b89f.min.css">
        
          
          
          <meta name="theme-color" content="#4051b5">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font-family:"Roboto";--md-code-font-family:"Roboto Mono"}</style>
      
    
    
    
    
      


    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <script>function __prefix(e){return new URL("..",location).pathname+"."+e}function __get(e,t=localStorage){return JSON.parse(t.getItem(__prefix(e)))}</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#create-an-new-operator" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../index.html" title="OneFlow" class="md-header__button md-logo" aria-label="OneFlow" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 89 89">
  <path d="M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z" />
  <path d="M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z" style="fill-opacity: 0.5" />
  <path d="M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z" />
  <path d="M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z" style="fill-opacity: 0.25" />
</svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            OneFlow
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              User Defined OP
            
          </span>
        </div>
      </div>
    </div>
    
    
      <div class="md-header__option">
        <div class="md-select">
          
          <button class="md-header__button md-icon" aria-label="Select language">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m12.87 15.07-2.54-2.51.03-.03A17.52 17.52 0 0 0 14.07 6H17V4h-7V2H8v2H1v2h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04M18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12m-2.62 7 1.62-4.33L19.12 17h-3.24z"/></svg>
          </button>
          <div class="md-select__inner">
            <ul class="md-select__list">
              
                <li class="md-select__item">
                  <a href="https://docs.oneflow.org/en" hreflang="en" class="md-select__link">
                    English
                  </a>
                </li>
                
                <li class="md-select__item">
                  <a href="https://docs.oneflow.org" hreflang="zh" class="md-select__link">
                    中文
                  </a>
                </li>
                
            </ul>
          </div>
        </div>
      </div>
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" data-md-state="active" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        
<a href="https://www.github.com/oneflow-Inc/oneflow" title="Go to repository" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    OneFlow
  </div>
</a>
      </div>
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  <li class="md-tabs__item">
    <a href="../index.html" class="md-tabs__link">
      Home
    </a>
  </li>

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../quick_start/install.html" class="md-tabs__link">
        Quick Start
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../basics_topics/data_input.html" class="md-tabs__link">
        Basic Topics
      </a>
    </li>
  

      
        
  
  
    
  


  
  
  
    <li class="md-tabs__item">
      <a href="job_function_define_call.html" class="md-tabs__link md-tabs__link--active">
        Extended Topics
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="https://oneflow.readthedocs.io/en/master/" class="md-tabs__link">
        API
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../contribute/intro.html" class="md-tabs__link">
        Contribute
      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../index.html" title="OneFlow" class="md-nav__button md-logo" aria-label="OneFlow" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 89 89">
  <path d="M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z" />
  <path d="M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z" style="fill-opacity: 0.5" />
  <path d="M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z" />
  <path d="M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z" style="fill-opacity: 0.25" />
</svg>

    </a>
    OneFlow
  </label>
  
    <div class="md-nav__source">
      
<a href="https://www.github.com/oneflow-Inc/oneflow" title="Go to repository" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    OneFlow
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../index.html" class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" >
      
      <label class="md-nav__link" for="__nav_2">
        Quick Start
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="Quick Start" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          Quick Start
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../quick_start/install.html" class="md-nav__link">
        Installation
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../quick_start/quickstart_in_3_min.html" class="md-nav__link">
        Quick Start in 3 Minutes
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../quick_start/lenet_mnist.html" class="md-nav__link">
        Recognition of MNIST Handwritten Digits
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" >
      
      <label class="md-nav__link" for="__nav_3">
        Basic Topics
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="Basic Topics" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Basic Topics
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../basics_topics/data_input.html" class="md-nav__link">
        Data Input
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../basics_topics/build_nn_with_op_and_layer.html" class="md-nav__link">
        Build a Neural Network
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../basics_topics/optimizer_in_function_config.html" class="md-nav__link">
        Optimization Algorithm and Parameter Configuration
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../basics_topics/async_get.html" class="md-nav__link">
        Get results from job function
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../basics_topics/model_load_save.html" class="md-nav__link">
        Loading and saving of model
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../basics_topics/distributed_train.html" class="md-nav__link">
        Distributed training
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../basics_topics/concept_explanation.html" class="md-nav__link">
        Term & Concept Explanation
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../basics_topics/essentials_of_oneflow.html" class="md-nav__link">
        OneFlow System Design
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" checked>
      
      <label class="md-nav__link" for="__nav_4">
        Extended Topics
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="Extended Topics" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          Extended Topics
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="job_function_define_call.html" class="md-nav__link">
        The Definition and Call of Job Function
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="consistent_mirrored.html" class="md-nav__link">
        Consistent & Mirrored View
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="model_mixed_parallel.html" class="md-nav__link">
        Features of Parallelism in OneFlow
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="ofrecord.html" class="md-nav__link">
        The OFRecord Data Format
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="how_to_make_ofdataset.html" class="md-nav__link">
        Loading and Preparing OFRecord Dataset
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="how_to_convert_image_to_ofrecord.html" class="md-nav__link">
        Convert Image Files to OFRecord Datasets
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="watch_watch_diff.html" class="md-nav__link">
        Obtain Runtime Data
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="debug_by_vscode.html" class="md-nav__link">
        Use VS Code to Debug OneFlow
      </a>
    </li>
  

          
            
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          User Defined OP
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="user_op.html" class="md-nav__link md-nav__link--active">
        User Defined OP
      </a>
      
        
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#background" class="md-nav__link">
    Background
  </a>
  
    <nav class="md-nav" aria-label="Background">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#what-is-a-custom-op" class="md-nav__link">
    What is a Custom Op
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#basic-concepts" class="md-nav__link">
    Basic Concepts
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#process-of-writing-a-custom-op" class="md-nav__link">
    Process of Writing a Custom Op
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#example" class="md-nav__link">
    Example
  </a>
  
    <nav class="md-nav" aria-label="Example">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#implementation-and-registration-of-op" class="md-nav__link">
    Implementation and Registration of Op
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#implementation-and-registration-of-cpu-kernel" class="md-nav__link">
    Implementation and Registration of CPU Kernel
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#implementation-and-registration-of-gpu-kernel" class="md-nav__link">
    Implementation and Registration of GPU Kernel
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compiling-option-description" class="md-nav__link">
    Compiling Option Description
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get-dynamic-library-by-compilation-and-linking" class="md-nav__link">
    Get Dynamic Library by Compilation and Linking
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#using-the-custom-op-in-python" class="md-nav__link">
    Using the Custom Op in Python
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#detailed-introduction-of-opregistry" class="md-nav__link">
    Detailed Introduction of OpRegistry
  </a>
  
    <nav class="md-nav" aria-label="Detailed Introduction of OpRegistry">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attr" class="md-nav__link">
    Attr
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setcheckattrfn" class="md-nav__link">
    SetCheckAttrFn
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multiple-inoutput" class="md-nav__link">
    Multiple In/Output
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setgetsbpfn" class="md-nav__link">
    SetGetSbpFn
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#detailed-introduction-of-opkernelregistry" class="md-nav__link">
    Detailed Introduction of OpKernelRegistry
  </a>
  
    <nav class="md-nav" aria-label="Detailed Introduction of OpKernelRegistry">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#setinfertmpsizefn" class="md-nav__link">
    SetInferTmpSizeFn
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#detailed-introduction-of-opgradregistry" class="md-nav__link">
    Detailed Introduction of OpGradRegistry
  </a>
  
    <nav class="md-nav" aria-label="Detailed Introduction of OpGradRegistry">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#setbackwardopconfgenfn" class="md-nav__link">
    SetBackwardOpConfGenFn
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#backwardopconfcontext" class="md-nav__link">
    BackwardOpConfContext
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#detailed-introduction-of-backwardopbuilder" class="md-nav__link">
    Detailed Introduction of BackwardOpBuilder
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#detailed-introduction-of-useropwrapper" class="md-nav__link">
    Detailed Introduction of UserOpWrapper
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#customized-op-for-calculating-gradients" class="md-nav__link">
    Customized Op for Calculating Gradients
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#detailed-introduction-of-useropconfbuilder" class="md-nav__link">
    Detailed Introduction of UserOpConfBuilder
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="oneflow_convert_tools.html" class="md-nav__link">
        OneFlow And ONNX Convert
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5" type="checkbox" id="__nav_5" >
      
      <label class="md-nav__link" for="__nav_5">
        API
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="API" data-md-level="1">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          API
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="https://oneflow.readthedocs.io/en/master/" class="md-nav__link">
        API
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_6" type="checkbox" id="__nav_6" >
      
      <label class="md-nav__link" for="__nav_6">
        Contribute
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="Contribute" data-md-level="1">
        <label class="md-nav__title" for="__nav_6">
          <span class="md-nav__icon md-icon"></span>
          Contribute
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../contribute/intro.html" class="md-nav__link">
        Contribute to OneFlow
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#background" class="md-nav__link">
    Background
  </a>
  
    <nav class="md-nav" aria-label="Background">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#what-is-a-custom-op" class="md-nav__link">
    What is a Custom Op
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#basic-concepts" class="md-nav__link">
    Basic Concepts
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#process-of-writing-a-custom-op" class="md-nav__link">
    Process of Writing a Custom Op
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#example" class="md-nav__link">
    Example
  </a>
  
    <nav class="md-nav" aria-label="Example">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#implementation-and-registration-of-op" class="md-nav__link">
    Implementation and Registration of Op
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#implementation-and-registration-of-cpu-kernel" class="md-nav__link">
    Implementation and Registration of CPU Kernel
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#implementation-and-registration-of-gpu-kernel" class="md-nav__link">
    Implementation and Registration of GPU Kernel
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compiling-option-description" class="md-nav__link">
    Compiling Option Description
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get-dynamic-library-by-compilation-and-linking" class="md-nav__link">
    Get Dynamic Library by Compilation and Linking
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#using-the-custom-op-in-python" class="md-nav__link">
    Using the Custom Op in Python
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#detailed-introduction-of-opregistry" class="md-nav__link">
    Detailed Introduction of OpRegistry
  </a>
  
    <nav class="md-nav" aria-label="Detailed Introduction of OpRegistry">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attr" class="md-nav__link">
    Attr
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setcheckattrfn" class="md-nav__link">
    SetCheckAttrFn
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multiple-inoutput" class="md-nav__link">
    Multiple In/Output
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setgetsbpfn" class="md-nav__link">
    SetGetSbpFn
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#detailed-introduction-of-opkernelregistry" class="md-nav__link">
    Detailed Introduction of OpKernelRegistry
  </a>
  
    <nav class="md-nav" aria-label="Detailed Introduction of OpKernelRegistry">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#setinfertmpsizefn" class="md-nav__link">
    SetInferTmpSizeFn
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#detailed-introduction-of-opgradregistry" class="md-nav__link">
    Detailed Introduction of OpGradRegistry
  </a>
  
    <nav class="md-nav" aria-label="Detailed Introduction of OpGradRegistry">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#setbackwardopconfgenfn" class="md-nav__link">
    SetBackwardOpConfGenFn
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#backwardopconfcontext" class="md-nav__link">
    BackwardOpConfContext
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#detailed-introduction-of-backwardopbuilder" class="md-nav__link">
    Detailed Introduction of BackwardOpBuilder
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#detailed-introduction-of-useropwrapper" class="md-nav__link">
    Detailed Introduction of UserOpWrapper
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#customized-op-for-calculating-gradients" class="md-nav__link">
    Customized Op for Calculating Gradients
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#detailed-introduction-of-useropconfbuilder" class="md-nav__link">
    Detailed Introduction of UserOpConfBuilder
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/OneFlow-Inc/oneflow-documentation/blob/master/en/docs/extended_topics/user_op.md" title="Edit this page" class="md-content__button md-icon">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg>
                  </a>
                
                
                <h1 id="create-an-new-operator">Create an New Operator<a class="headerlink" href="#create-an-new-operator" title="Permanent link">&para;</a></h1>
<h2 id="background">Background<a class="headerlink" href="#background" title="Permanent link">&para;</a></h2>
<h3 id="what-is-a-custom-op">What is a Custom Op<a class="headerlink" href="#what-is-a-custom-op" title="Permanent link">&para;</a></h3>
<p>OneFlow abstracts all kinds of data processing into op (operator). Op acts on the input tensor and writes the result of the operation to the output tensor. OneFlow provides relatively comprehensive ops and they can be found in <a href="https://github.com/Oneflow-Inc/oneflow/tree/master/oneflow/python/ops">ops directory</a>.</p>
<p>When OneFlow's existing Python operators are not sufficient to build a neural network or when Python operators do not meet performance requirements. You can use C++ to develop custom op in OneFlow.</p>
<p>OneFlow provides a mechanism with which you can create custom op and register it in OneFlow then use custom op in Python.</p>
<p>The following diagram demonstrates the registration system for a custom op in OneFlow.</p>
<p><img alt="OneFlow UserOp Existing System" src="imgs/oneflow_system_userop.png" /></p>
<p>In the OneFlow framework, there are three types of registries associated with custom op.</p>
<ul>
<li>
<p><code>OpGradRegistry</code>：Manage gradient registration for automatic gradient calculation in backward graph.</p>
</li>
<li>
<p><code>OpRegistry</code>：Manage op registrations for generating forward digraph and building <code>Task Graph</code>.</p>
</li>
<li>
<p><code>OpKernelRegistry</code>：Manage kernel registrations for performing user logic at runtime.</p>
</li>
</ul>
<p>We actually write custom op in C++ and generate a dynamic link library (so file). By loading the corresponding so file in Python that you can use the custom op.</p>
<p>The data structure of user op can be  viewed at <a href="https://github.com/Oneflow-Inc/oneflow/blob/master/oneflow/core/framework/user_op_conf.proto">user_op_conf.proto</a>：</p>
<div class="highlight"><pre><span></span><code>syntax = &quot;proto2&quot;;
package oneflow;

import &quot;oneflow/core/framework/user_op_attr.proto&quot;;

message UserOpConf {
  message ListString {
    repeated string s = 1;
  }
  required string op_type_name = 1;
  map&lt;string, ListString&gt; input = 2;
  map&lt;string, ListString&gt; output = 3;
  map&lt;string, UserOpAttrVal&gt; attr = 4;
}
</code></pre></div>
<p>The <code>op_type_name</code> is a string which representing the class of op and indicate the globally unique ID of the op class. OneFlow queries and confirms the op class by <code>op_type_name</code> which will appear several times in the rest of this document.</p>
<h3 id="basic-concepts">Basic Concepts<a class="headerlink" href="#basic-concepts" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p>Op_type_name：As mentioned above, op_type_name is the unique ID of op class. OneFlow queries and confirms op class by op_type_name, and then instantiates the op. The relationship between op class and op is similar to the relationship between class and object.</p>
</li>
<li>
<p>Op：Logical operators contain information of input and output shapes for mapping and reasoning, but do not contain logic for processing the data.</p>
</li>
<li>
<p>Kernel：When a logical op running,  the processing logic will affect by physical device and data type. The specific processing logic is done by the kernel. Generally op has a one-to-many relationship with the kernel and we need to register the kernel for all the physical devices and data types that op supports.</p>
</li>
<li>
<p>Registration：Registration can be used to establish a link between a custom op and the OneFlow framework. A series of macros named <code>REGISTER_XXX</code> are provided in OneFlow to help with registration of op.</p>
</li>
<li>
<p>Loading the dynamic library：The custom op and its kernel are linked as dynamic library so files that need to be loaded before using them in Python and OneFlow provides <code>oneflow.config.load_library</code> to load the so files of custom op.</p>
</li>
<li>
<p>Python wrapper：Calling a custom op implemented at the C++ layer in Python requires writing a wrapper at the Python layer and OneFlow provides <code>oneflow.user_op_builder</code> to do this task.</p>
</li>
</ul>
<h3 id="process-of-writing-a-custom-op">Process of Writing a Custom Op<a class="headerlink" href="#process-of-writing-a-custom-op" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p>Implementation and registration of op：The implementation of op is primarily used for forward digraph composition which includes specifying the name of op, inputs, outputs, configuration attributes and the necessary functions to infer the shape and data type of the tensor.</p>
</li>
<li>
<p>Implementation and registration of the kernel for an op: The kernel is responsible for the specific computational process during running and an op may correspond to multiple kernels</p>
</li>
<li>
<p>(optional) Implementation and registration of op's corresponding grad: If the custom op needs to support backward spreading. Then we need to implement and register a backward function for it.</p>
</li>
<li>
<p>Compile and link to get so file</p>
</li>
<li>
<p>Load the so file in Python and use <code>oneflow.user_op_builder</code> to wrap a custom op written in C++.</p>
</li>
<li>
<p>Testing.</p>
</li>
</ol>
<h2 id="example">Example<a class="headerlink" href="#example" title="Permanent link">&para;</a></h2>
<p>We will implement a custom op called "myrelu" which supports both CPU and GPU operations.
For the complete code please refer to:  <a href="https://github.com/Oneflow-Inc/oneflow-documentation/tree/master/cn/docs/code/extended_topics/create_user_op">code/extended_topics/create_user_op</a>.</p>
<h3 id="implementation-and-registration-of-op">Implementation and Registration of Op<a class="headerlink" href="#implementation-and-registration-of-op" title="Permanent link">&para;</a></h3>
<p>We defined op and completed the registration in <code>myrelu_op.cpp</code>:</p>
<div class="highlight"><pre><span></span><code><span class="cp">#include</span> <span class="cpf">&quot;oneflow/core/framework/framework.h&quot;</span><span class="cp"></span>

<span class="k">namespace</span> <span class="nn">oneflow</span> <span class="p">{</span>

<span class="k">namespace</span> <span class="p">{</span>

<span class="n">REGISTER_USER_OP</span><span class="p">(</span><span class="s">&quot;myrelu&quot;</span><span class="p">)</span>
  <span class="p">.</span><span class="n">Input</span><span class="p">(</span><span class="s">&quot;in&quot;</span><span class="p">)</span>
  <span class="p">.</span><span class="n">Output</span><span class="p">(</span><span class="s">&quot;out&quot;</span><span class="p">)</span>
  <span class="p">.</span><span class="n">SetTensorDescInferFn</span><span class="p">(</span>
      <span class="p">[](</span><span class="n">user_op</span><span class="o">::</span><span class="n">InferContext</span> <span class="o">*</span><span class="n">ctx</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Maybe</span><span class="o">&lt;</span><span class="kt">void</span><span class="o">&gt;</span> <span class="p">{</span>
        <span class="o">*</span><span class="n">ctx</span><span class="o">-&gt;</span><span class="n">Shape4ArgNameAndIndex</span><span class="p">(</span><span class="s">&quot;out&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">=</span>
            <span class="o">*</span><span class="n">ctx</span><span class="o">-&gt;</span><span class="n">Shape4ArgNameAndIndex</span><span class="p">(</span><span class="s">&quot;in&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
        <span class="o">*</span><span class="n">ctx</span><span class="o">-&gt;</span><span class="n">Dtype4ArgNameAndIndex</span><span class="p">(</span><span class="s">&quot;out&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">=</span>
            <span class="o">*</span><span class="n">ctx</span><span class="o">-&gt;</span><span class="n">Dtype4ArgNameAndIndex</span><span class="p">(</span><span class="s">&quot;in&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
        <span class="k">return</span> <span class="n">Maybe</span><span class="o">&lt;</span><span class="kt">void</span><span class="o">&gt;::</span><span class="n">Ok</span><span class="p">();</span>
      <span class="p">});</span>
<span class="p">}</span> <span class="c1">// namespace</span>

<span class="p">}</span> <span class="c1">// namespace oneflow</span>
</code></pre></div>
<p>Analysis of the above codes:</p>
<ul>
<li>
<p><code>oneflow/core/framework/framework.h</code> contains all the controllers we need to create an op.</p>
</li>
<li>
<p>Almost all the APIs related to user op are in the namespace <code>oneflow::user_op</code>, so we use the namespace <code>oneflow</code> to simplify the type name.</p>
</li>
<li>
<p>The macro <code>REGISTER_USER_OP</code> is used to register the op and accepts <code>myrelu</code> as <code>op_type_name</code>.</p>
</li>
<li>
<p>After registering with <code>REGISTER_USER_OP</code>, it actually returns an <code>OpRegistry</code> class (path: <code>oneflow\coreframework\user_op_registry.h</code>) which can be called to complete the setting of a custom op:</p>
</li>
<li>
<p><code>Input("in")</code> means that it has an input named "in".</p>
</li>
<li><code>Output("out")</code> means that it has an output named "out".</li>
<li><code>SetTensorDescInferFn</code> is used to set the shape and data type of the inferring function which describe the relationship between the input of this operator and shape and type of the output of this operator. In the above code, the shape and data type of the output is consistent with input.</li>
</ul>
<h3 id="implementation-and-registration-of-cpu-kernel">Implementation and Registration of CPU Kernel<a class="headerlink" href="#implementation-and-registration-of-cpu-kernel" title="Permanent link">&para;</a></h3>
<p>We implemented the CPU kernel in <code>myrelu_cpu_kernel.cpp</code> and registered it：</p>
<div class="highlight"><pre><span></span><code><span class="cp">#include</span> <span class="cpf">&quot;oneflow/core/framework/framework.h&quot;</span><span class="cp"></span>

<span class="k">namespace</span> <span class="nn">oneflow</span> <span class="p">{</span>

<span class="k">namespace</span> <span class="p">{</span>

<span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="nc">T</span><span class="o">&gt;</span>
<span class="kt">void</span> <span class="n">MyRelu</span><span class="p">(</span><span class="n">DeviceCtx</span> <span class="o">*</span><span class="n">ctx</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int64_t</span> <span class="n">n</span><span class="p">,</span> <span class="k">const</span> <span class="n">T</span> <span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="n">T</span> <span class="o">*</span><span class="n">y</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">T</span> <span class="n">zero</span> <span class="o">=</span> <span class="p">(</span><span class="n">T</span><span class="p">)(</span><span class="mi">0</span><span class="p">);</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int64_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">n</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">zero</span><span class="p">);</span>
  <span class="p">}</span>
<span class="p">}</span>

<span class="k">template</span> <span class="o">&lt;</span><span class="n">DeviceType</span> <span class="n">device_type</span><span class="p">,</span> <span class="k">typename</span> <span class="nc">T</span><span class="o">&gt;</span>
<span class="k">class</span> <span class="nc">ReluKernel</span> <span class="k">final</span> <span class="o">:</span> <span class="k">public</span> <span class="n">user_op</span><span class="o">::</span><span class="n">OpKernel</span> <span class="p">{</span>
<span class="k">public</span><span class="o">:</span>
  <span class="n">ReluKernel</span><span class="p">()</span> <span class="o">=</span> <span class="k">default</span><span class="p">;</span>
  <span class="o">~</span><span class="n">ReluKernel</span><span class="p">()</span> <span class="o">=</span> <span class="k">default</span><span class="p">;</span>

<span class="k">private</span><span class="o">:</span>
  <span class="kt">void</span> <span class="n">Compute</span><span class="p">(</span><span class="n">user_op</span><span class="o">::</span><span class="n">KernelComputeContext</span> <span class="o">*</span><span class="n">ctx</span><span class="p">)</span> <span class="k">const</span> <span class="k">override</span> <span class="p">{</span>
    <span class="k">const</span> <span class="n">user_op</span><span class="o">::</span><span class="n">Tensor</span> <span class="o">*</span><span class="n">in_tensor</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">-&gt;</span><span class="n">Tensor4ArgNameAndIndex</span><span class="p">(</span><span class="s">&quot;in&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
    <span class="n">user_op</span><span class="o">::</span><span class="n">Tensor</span> <span class="o">*</span><span class="n">out_tensor</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">-&gt;</span><span class="n">Tensor4ArgNameAndIndex</span><span class="p">(</span><span class="s">&quot;out&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
    <span class="n">MyRelu</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span><span class="p">(</span><span class="n">ctx</span><span class="o">-&gt;</span><span class="n">device_ctx</span><span class="p">(),</span>
           <span class="n">in_tensor</span><span class="o">-&gt;</span><span class="n">shape</span><span class="p">().</span><span class="n">elem_cnt</span><span class="p">(),</span>
           <span class="n">in_tensor</span><span class="o">-&gt;</span><span class="n">dptr</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span><span class="p">(),</span>
           <span class="n">out_tensor</span><span class="o">-&gt;</span><span class="n">mut_dptr</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span><span class="p">());</span>
  <span class="p">}</span>
  <span class="kt">bool</span> <span class="n">AlwaysComputeWhenAllOutputsEmpty</span><span class="p">()</span> <span class="k">const</span> <span class="k">override</span> <span class="p">{</span> <span class="k">return</span> <span class="nb">false</span><span class="p">;</span> <span class="p">}</span>
<span class="p">};</span>

<span class="cp">#define REGISTER_RELU_KERNEL(device, dtype)          \</span>
<span class="cp">  REGISTER_USER_KERNEL(&quot;myrelu&quot;)                     \</span>
<span class="cp">      .SetCreateFn&lt;ReluKernel&lt;device, dtype&gt;&gt;()      \</span>
<span class="cp">      .SetIsMatchedHob(                              \</span>
<span class="cp">          (user_op::HobDeviceTag() == device) &amp;     \</span>
<span class="cp">          (user_op::HobDataType(&quot;out&quot;, 0)            \</span>
<span class="cp">            == GetDataType&lt;dtype&gt;::value));</span>

<span class="n">REGISTER_RELU_KERNEL</span><span class="p">(</span><span class="n">DeviceType</span><span class="o">::</span><span class="n">kCPU</span><span class="p">,</span> <span class="kt">float</span><span class="p">)</span>
<span class="n">REGISTER_RELU_KERNEL</span><span class="p">(</span><span class="n">DeviceType</span><span class="o">::</span><span class="n">kCPU</span><span class="p">,</span> <span class="kt">double</span><span class="p">)</span>
<span class="p">}</span> <span class="c1">// namespace</span>

<span class="p">}</span> <span class="c1">// namespace oneflow</span>
</code></pre></div>
<p>To implement the kernel in OneFlow, you must define a class which inherits from <code>oneflow::user_op::OpKernel</code> and rewrite the virtual functions of it.</p>
<p>In the above code, we rewrite <code>Compute</code> and <code>AlwaysComputeWhenAllOutputsEmpty</code> and their respective meanings are:</p>
<ul>
<li>
<p><code>Compute</code> must be rewritten to implement the specific operating logic.</p>
</li>
<li>
<p><code>AlwaysComputeWhenAllOutputsEmpty</code> must be rewritten to return <code>false</code> in most cases. For very few ops that need to maintain state internally, and therefore need to call the kernel for calculation even if the output is empty, it should return <code>true</code>.</p>
</li>
</ul>
<p>After implementing the kernel class, you need to call <code>REGISTER_USER_KERNEL</code> to register it. The string parameter that <code>REGISTER_USER_KERNEL("myrelu")</code> accepts is <code>op_type_name</code> which is used to complete registration and querying. You also need to use <code>op_type_name</code> when wrapping op at the Python layer.</p>
<p><code>REGISTER_USER_KERNEL("myrelu")</code> returns an <code>OpKernelRegistry</code> object. The methods that need to be called to set the registration information are  mention in the code above.</p>
<ul>
<li>
<p><code>SetCreateFn&lt;T&gt;()</code>: The method of this template's parameter <code>T</code>  is our implementation of the kernel class which OneFlow will use it to create the kernel object.</p>
</li>
<li>
<p><code>SetIsMatchedHob</code>：Because an op may have more than one kernels. You need to call <code>SetIsMatchedHob</code> to select a specific kernel for the calculation according to the physical device and data format. This method accepts an expression and when the expression is <code>true</code>, OneFlow will call the kernel to complete the calculation.</p>
</li>
</ul>
<h3 id="implementation-and-registration-of-gpu-kernel">Implementation and Registration of GPU Kernel<a class="headerlink" href="#implementation-and-registration-of-gpu-kernel" title="Permanent link">&para;</a></h3>
<p>We implemented the GPU version of the kernel in  <code>myrelu_gpu_kernel.cu</code>  and registered it：</p>
<div class="highlight"><pre><span></span><code><span class="cp">#include</span> <span class="cpf">&quot;oneflow/core/framework/framework.h&quot;</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&lt;cub/cub.cuh&gt;</span><span class="cp"></span>

<span class="k">namespace</span> <span class="nn">oneflow</span> <span class="p">{</span>
<span class="k">namespace</span> <span class="p">{</span>
<span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="nc">T</span><span class="o">&gt;</span>
<span class="n">__global__</span> <span class="kt">void</span> <span class="n">ReluForwardGpu</span><span class="p">(</span><span class="k">const</span> <span class="kt">int</span> <span class="n">n</span><span class="p">,</span> <span class="k">const</span> <span class="n">T</span> <span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="n">T</span> <span class="o">*</span><span class="n">y</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">CUDA_1D_KERNEL_LOOP</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="o">?</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">:</span> <span class="mi">0</span><span class="p">;</span> <span class="p">}</span>
<span class="p">}</span>

<span class="k">class</span> <span class="nc">ReluGpuFloatKernel</span> <span class="k">final</span> <span class="o">:</span> <span class="k">public</span> <span class="n">user_op</span><span class="o">::</span><span class="n">OpKernel</span> <span class="p">{</span>
<span class="k">public</span><span class="o">:</span>
  <span class="n">ReluGpuFloatKernel</span><span class="p">()</span> <span class="o">=</span> <span class="k">default</span><span class="p">;</span>
  <span class="o">~</span><span class="n">ReluGpuFloatKernel</span><span class="p">()</span> <span class="o">=</span> <span class="k">default</span><span class="p">;</span>

<span class="k">private</span><span class="o">:</span>
  <span class="kt">void</span> <span class="n">Compute</span><span class="p">(</span><span class="n">user_op</span><span class="o">::</span><span class="n">KernelComputeContext</span> <span class="o">*</span><span class="n">ctx</span><span class="p">)</span> <span class="k">const</span> <span class="k">override</span> <span class="p">{</span>
    <span class="k">const</span> <span class="n">user_op</span><span class="o">::</span><span class="n">Tensor</span> <span class="o">*</span><span class="n">in_tensor</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">-&gt;</span><span class="n">Tensor4ArgNameAndIndex</span><span class="p">(</span><span class="s">&quot;in&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
    <span class="n">user_op</span><span class="o">::</span><span class="n">Tensor</span> <span class="o">*</span><span class="n">out_tensor</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">-&gt;</span><span class="n">Tensor4ArgNameAndIndex</span><span class="p">(</span><span class="s">&quot;out&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>

    <span class="kt">int32_t</span> <span class="n">n</span> <span class="o">=</span> <span class="n">in_tensor</span><span class="o">-&gt;</span><span class="n">shape</span><span class="p">().</span><span class="n">elem_cnt</span><span class="p">();</span>
    <span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="n">in_ptr</span> <span class="o">=</span> <span class="n">in_tensor</span><span class="o">-&gt;</span><span class="n">dptr</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">();</span>
    <span class="kt">float</span> <span class="o">*</span><span class="n">out_ptr</span> <span class="o">=</span> <span class="n">out_tensor</span><span class="o">-&gt;</span><span class="n">mut_dptr</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">();</span>
    <span class="n">ReluForwardGpu</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span>
        <span class="o">&lt;&lt;&lt;</span><span class="mi">32</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">ctx</span><span class="o">-&gt;</span><span class="n">device_ctx</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">cuda_stream</span><span class="p">()</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">in_ptr</span><span class="p">,</span> <span class="n">out_ptr</span><span class="p">);</span>
  <span class="p">}</span>

  <span class="kt">bool</span> <span class="n">AlwaysComputeWhenAllOutputsEmpty</span><span class="p">()</span> <span class="k">const</span> <span class="k">override</span> <span class="p">{</span> <span class="k">return</span> <span class="nb">false</span><span class="p">;</span> <span class="p">}</span>
<span class="p">};</span>

<span class="cp">#define REGISTER_RELU_KERNEL(device, dtype)          \</span>
<span class="cp">  REGISTER_USER_KERNEL(&quot;myrelu&quot;)                     \</span>
<span class="cp">      .SetCreateFn&lt;ReluGpuFloatKernel&gt;()             \</span>
<span class="cp">      .SetIsMatchedHob(                              \</span>
<span class="cp">          (user_op::HobDeviceTag() == device) &amp;     \</span>
<span class="cp">          (user_op::HobDataType(&quot;out&quot;, 0)            \</span>
<span class="cp">            == GetDataType&lt;dtype&gt;::value));</span>

<span class="n">REGISTER_RELU_KERNEL</span><span class="p">(</span><span class="n">DeviceType</span><span class="o">::</span><span class="n">kGPU</span><span class="p">,</span> <span class="kt">float</span><span class="p">)</span>
<span class="n">REGISTER_RELU_KERNEL</span><span class="p">(</span><span class="n">DeviceType</span><span class="o">::</span><span class="n">kGPU</span><span class="p">,</span> <span class="kt">double</span><span class="p">)</span>
<span class="p">}</span> <span class="c1">// namespace</span>
<span class="p">}</span> <span class="c1">// namespace oneflow</span>
</code></pre></div>
<p>The process of implementing and registering the GPU kernel is almost identical to the CPU kernel. The main differences are:</p>
<ul>
<li>
<p>Because CUDA programming is used, the CUDA header files are included.</p>
</li>
<li>
<p><code>Compute</code> uses GPU methods.</p>
</li>
<li>
<p><code>SetIsMatchedHob</code> set the matching device as GPU.</p>
</li>
</ul>
<p>Besides that, because of the use of CUDA, we need to use the nvcc compiler (instead of g++) to compile the GPU kernel.</p>
<h3 id="compiling-option-description">Compiling Option Description<a class="headerlink" href="#compiling-option-description" title="Permanent link">&para;</a></h3>
<p>The <code>oneflow.sysconfig</code> contains the <code>get_compile_flags</code>, <code>get_include</code>, <code>get_lib</code>, and <code>get_link_flags</code> which corresponding to:</p>
<ul>
<li>Compiling Options</li>
<li>Dictionary of header file</li>
<li>Dictionary of link library</li>
<li>Linking options</li>
</ul>
<p>For example：</p>
<div class="highlight"><pre><span></span><code>&gt;&gt;&gt; import oneflow
&gt;&gt;&gt; oneflow.sysconfig.get_compile_flags()
[&#39;-I/home/yaochi/oneflow/build/python_scripts/oneflow/include&#39;, &#39;-DHALF_ENABLE_CPP11_USER_LITERALS=0&#39;, &#39;-DWITH_CUDA&#39;, &#39;-D_GLIBCXX_USE_CXX11_ABI=0&#39;]
</code></pre></div>
<p>You can also get compile and link options directly by using command：</p>
<div class="highlight"><pre><span></span><code>python -c &quot;import oneflow; print(&#39; &#39;.join(oneflow.sysconfig.get_compile_flags()))&quot;
python -c &quot;import oneflow; print(&#39; &#39;.join(oneflow.sysconfig.get_link_flags()))&quot;
</code></pre></div>
<p>For the GPU kernel, the <code>cudart</code> library also needs to be specified when linking.</p>
<h3 id="get-dynamic-library-by-compilation-and-linking">Get Dynamic Library by Compilation and Linking<a class="headerlink" href="#get-dynamic-library-by-compilation-and-linking" title="Permanent link">&para;</a></h3>
<p>For this simple example, you can use the following Makefile to build:</p>
<div class="highlight"><pre><span></span><code><span class="nv">CFLAGS</span> <span class="o">=</span> <span class="k">$(</span>shell python -c <span class="s2">&quot;import oneflow; print(&#39; &#39;.join(oneflow.sysconfig.get_compile_flags()))&quot;</span><span class="k">)</span>
<span class="nv">LFLAGS</span> <span class="o">=</span> <span class="k">$(</span>shell python -c <span class="s2">&quot;import oneflow; print(&#39; &#39;.join(oneflow.sysconfig.get_link_flags()))&quot;</span><span class="k">)</span>
<span class="nv">CUDAPATH</span> <span class="o">=</span> /usr/local/cuda-10.1/lib64

all: final_relu.so

myrelu_op.o: myrelu_op.cpp
    g++ -std<span class="o">=</span>c++11 -c myrelu_op.cpp <span class="se">\</span>
    -o myrelu_op.o                  <span class="se">\</span>
    -fPIC                           <span class="se">\</span>
    <span class="si">${</span><span class="nv">CFLAGS</span><span class="si">}</span>                       <span class="se">\</span>
    <span class="si">${</span><span class="nv">LFLAGS</span><span class="si">}</span>                       <span class="se">\</span>
    -O2

myrelu_cpu_kernel.o: myrelu_cpu_kernel.cpp
    g++ -std<span class="o">=</span>c++11 -c myrelu_cpu_kernel.cpp <span class="se">\</span>
    -o myrelu_cpu_kernel.o                  <span class="se">\</span>
    <span class="k">$(</span>CFLAGS<span class="k">)</span> -fPIC

myrelu_gpu_kernel.o: myrelu_gpu_kernel.cu
    nvcc -std<span class="o">=</span>c++11 -c myrelu_gpu_kernel.cu <span class="se">\</span>
    -o myrelu_gpu_kernel.o                  <span class="se">\</span>
    <span class="k">$(</span>CFLAGS<span class="k">)</span> -x cu -Xcompiler -fPIC

final_relu.so: myrelu_op.o myrelu_cpu_kernel.o myrelu_gpu_kernel.o
    g++ -std<span class="o">=</span>c++11 myrelu_op.o <span class="se">\</span>
    myrelu_cpu_kernel.o        <span class="se">\</span>
    myrelu_gpu_kernel.o        <span class="se">\</span>
    -shared -o final_relu.so   <span class="se">\</span>
    <span class="k">$(</span>CFLAGS<span class="k">)</span>                  <span class="se">\</span>
    -fPIC                      <span class="se">\</span>
    -L<span class="k">$(</span>CUDAPATH<span class="k">)</span>              <span class="se">\</span>
    -lcudart                   <span class="se">\</span>
    <span class="k">$(</span>LFLAGS<span class="k">)</span>

clean:
    rm -rf *.so *.o
</code></pre></div>
<p>We use <code>g++</code> to compile <code>myrelu_op.cpp</code> and <code>myrelu_cpu_kernel.cpp</code>, use <code>nvcc</code> to compile <code>myrelu_gpu_kernel.cpp</code>. Then get the target file (".o" file) and link the target file to <code>final_ relu.so</code>.</p>
<p>We are going to load <code>final_relu.so</code> in Python then use wrappers and custom op.</p>
<h3 id="using-the-custom-op-in-python">Using the Custom Op in Python<a class="headerlink" href="#using-the-custom-op-in-python" title="Permanent link">&para;</a></h3>
<p>Using a custom op in Python needs the following steps:</p>
<ul>
<li>
<p>Load the so file by <code>oneflow.config.load_library</code>.</p>
</li>
<li>
<p>Use <code>oneflow.user_op_builder</code> to generating Python wrapper for custom op.</p>
</li>
<li>
<p>Call the above result of Python wrapper.</p>
</li>
</ul>
<p>The following code encapsulates <code>myrelu</code> at the Python layer and call it:</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">oneflow</span> <span class="k">as</span> <span class="nn">flow</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">oneflow.typing</span> <span class="k">as</span> <span class="nn">tp</span>

<span class="c1"># load modules</span>
<span class="n">flow</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">load_library</span><span class="p">(</span><span class="s2">&quot;final_relu.so&quot;</span><span class="p">)</span>

<span class="c1"># default configuration</span>
<span class="n">flow</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">gpu_device_num</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># python op wrapper function</span>
<span class="k">def</span> <span class="nf">myrelu</span><span class="p">(</span><span class="n">input_blob</span><span class="p">):</span>
    <span class="n">op</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">flow</span><span class="o">.</span><span class="n">user_op_builder</span><span class="p">(</span><span class="s2">&quot;op_myrelu&quot;</span><span class="p">)</span>
        <span class="o">.</span><span class="n">Op</span><span class="p">(</span><span class="s2">&quot;myrelu&quot;</span><span class="p">)</span>
        <span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="s2">&quot;in&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">input_blob</span><span class="p">])</span>
        <span class="o">.</span><span class="n">Output</span><span class="p">(</span><span class="s2">&quot;out&quot;</span><span class="p">)</span>
        <span class="o">.</span><span class="n">Build</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">op</span><span class="o">.</span><span class="n">InferAndTryRun</span><span class="p">()</span><span class="o">.</span><span class="n">SoleOutputBlob</span><span class="p">()</span>


<span class="c1"># network code</span>
<span class="nd">@flow</span><span class="o">.</span><span class="n">global_function</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">MyJob</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">tp</span><span class="o">.</span><span class="n">Numpy</span><span class="o">.</span><span class="n">Placeholder</span><span class="p">((</span><span class="mi">5</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">flow</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span> <span class="o">-&gt;</span> <span class="n">tp</span><span class="o">.</span><span class="n">Numpy</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">myrelu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">MyJob</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</code></pre></div>
<p>The expected results are：</p>
<div class="highlight"><pre><span></span><code>[-2. -1.  0.  1.  2.]
[0. 0. 0. 1. 2.]
</code></pre></div>
<p>In the above code: <code>flow.config.load_library("final_relu.so")</code> is to load the so file.</p>
<p>We are focus on the process of building and running the python wrapper in <code>myrelu</code>.</p>
<p><code>flow.user_op_builder("op_myrelu")</code> actually returns a <code>UserOpConfBuilder</code> object named <code>op_myrelu</code>.</p>
<div class="highlight"><pre><span></span><code>    <span class="n">op</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">flow</span><span class="o">.</span><span class="n">user_op_builder</span><span class="p">(</span><span class="s2">&quot;op_myrelu&quot;</span><span class="p">)</span>
        <span class="o">.</span><span class="n">Op</span><span class="p">(</span><span class="s2">&quot;myrelu&quot;</span><span class="p">)</span>
        <span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="s2">&quot;in&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">input_blob</span><span class="p">])</span>
        <span class="o">.</span><span class="n">Output</span><span class="p">(</span><span class="s2">&quot;out&quot;</span><span class="p">)</span>
        <span class="o">.</span><span class="n">Build</span><span class="p">()</span>
    <span class="p">)</span>
</code></pre></div>
<p>This object contains <code>Op</code>, <code>Input</code> and and etc methods which are used to encapsulate custom op. Details explanation are as follows:</p>
<ul>
<li>
<p><code>Op("myrelu")</code>: The parameter must be the <code>op_type_name</code> from the previous C++ registration which OneFlow uses to find the registered op type and instantiate the op.</p>
</li>
<li>
<p><code>Input("in", [input_blob])</code>: Corresponds to <code>Input</code> when op is registered in C++ that the first parameter must be the same as the string set by <code>Input</code> when op is registered in C++. The second parameter is the blob of the input which is a <code>list</code>. Because an op allows multiple inputs.</p>
</li>
<li>
<p><code>Output("out")</code>: Corresponds to <code>Output</code> when op registered in C++.</p>
</li>
<li>
<p><code>Build</code>：After the above settings are complete, call <code>Build</code> to get the Python wrapper from the custom op.</p>
</li>
</ul>
<p>The following code will get the blob of the custom op:</p>
<div class="highlight"><pre><span></span><code><span class="k">return</span> <span class="n">op</span><span class="o">.</span><span class="n">InferAndTryRun</span><span class="p">()</span><span class="o">.</span><span class="n">SoleOutputBlob</span><span class="p">()</span>
</code></pre></div>
<p><code>InferAndTryRun</code> completes the derivation and returns <code>UserOp</code>. If the returned blob has only one output. We cab use <code>SoleOutputBlob</code> to get the unique output. Otherwise use <code>RemoteBlobList</code> to get a list of multiple blobs.</p>
<p>So far, we have built the <code>myrelu</code> which is a relatively simple op. But if we need to build a more complex op, we should use some additional features in the registration process.
We'll introduce it from the aspects of op registration, kernel registration, gradient registration and Python layer wrapping.</p>
<h2 id="detailed-introduction-of-opregistry">Detailed Introduction of OpRegistry<a class="headerlink" href="#detailed-introduction-of-opregistry" title="Permanent link">&para;</a></h2>
<h3 id="attr"><code>Attr</code><a class="headerlink" href="#attr" title="Permanent link">&para;</a></h3>
<p>Some ops require configuration properties in addition to inputs and outputs. For example, the <code>reshape</code> needs to be configured the shape and the <code>conv</code> needs to be configured the alignment method. We can use the <code>Attr</code> at registration to set attributes for op. For example:</p>
<div class="highlight"><pre><span></span><code><span class="n">OpRegistry</span><span class="o">&amp;</span> <span class="n">Attr</span><span class="o">&lt;</span><span class="n">cpp_type</span><span class="o">&gt;</span><span class="p">(</span><span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&amp;</span> <span class="n">name</span><span class="p">);</span>
</code></pre></div>
<p>We just need to specify the name and type of the attribute.
For example：</p>
<div class="highlight"><pre><span></span><code><span class="n">REGISTER_USER_OP</span><span class="p">(</span><span class="s">&quot;reshape&quot;</span><span class="p">)</span>
    <span class="p">.</span><span class="n">Input</span><span class="p">(</span><span class="s">&quot;in&quot;</span><span class="p">)</span>
    <span class="p">.</span><span class="n">Output</span><span class="p">(</span><span class="s">&quot;out&quot;</span><span class="p">)</span>
    <span class="p">.</span><span class="n">Attr</span><span class="o">&lt;</span><span class="n">shape</span><span class="o">&gt;</span><span class="p">(</span><span class="s">&quot;shape&quot;</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">REGISTER_USER_OP</span><span class="p">(</span><span class="s">&quot;conv2d&quot;</span><span class="p">)</span>
    <span class="p">.</span><span class="n">Input</span><span class="p">(</span><span class="s">&quot;in&quot;</span><span class="p">)</span>
    <span class="p">.</span><span class="n">Input</span><span class="p">(</span><span class="s">&quot;weight&quot;</span><span class="p">)</span>
    <span class="p">.</span><span class="n">Output</span><span class="p">(</span><span class="s">&quot;out&quot;</span><span class="p">)</span>
    <span class="p">.</span><span class="n">Attr</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;&gt;</span><span class="p">(</span><span class="s">&quot;padding_before&quot;</span><span class="p">)</span>
</code></pre></div>
<p>In OneFlow, we currently support the following C++ data:</p>
<table>
<thead>
<tr>
<th>UserOpAttrType</th>
<th>Corresponding C++ data types</th>
</tr>
</thead>
<tbody>
<tr>
<td>kAtInt32</td>
<td>int32_t</td>
</tr>
<tr>
<td>kAtInt64</td>
<td>int64_t</td>
</tr>
<tr>
<td>kAtBool</td>
<td>bool</td>
</tr>
<tr>
<td>kAtFloat</td>
<td>float</td>
</tr>
<tr>
<td>kAtDouble</td>
<td>double</td>
</tr>
<tr>
<td>kAtShape</td>
<td>oneflow::Shape</td>
</tr>
<tr>
<td>kAtListInt32</td>
<td>std::vector<int32_t></td>
</tr>
<tr>
<td>kAtListInt64</td>
<td>std::vector<int64_t></td>
</tr>
<tr>
<td>kAtListFloat</td>
<td>std::vector&lt; float &gt;</td>
</tr>
<tr>
<td>kAtString</td>
<td>std::string</td>
</tr>
</tbody>
</table>
<p>We can pass an additional parameter and configure a default value for it which is the corresponding C++ datatype in the table. Such as：</p>
<div class="highlight"><pre><span></span><code><span class="p">.</span><span class="n">Attr</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="p">(</span><span class="s">&quot;is_transpose&quot;</span><span class="p">,</span> <span class="nb">false</span><span class="p">)</span>

<span class="p">.</span><span class="n">Attr</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;</span><span class="p">(</span><span class="s">&quot;size&quot;</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

<span class="p">.</span><span class="n">Attr</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;&gt;</span><span class="p">(</span><span class="s">&quot;vector_of_size&quot;</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;</span><span class="p">{</span><span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">})</span>
</code></pre></div>
<h3 id="setcheckattrfn"><code>SetCheckAttrFn</code><a class="headerlink" href="#setcheckattrfn" title="Permanent link">&para;</a></h3>
<p>For some  <code>Attributes</code>, they require a more detailed delineation of the range which can be specified by  <code>SetCheckAttrFn</code>  when registering the Op.</p>
<p>Take <code>Conv</code> op as an example, it has a configuration option called <code>data_format</code> which is a string type but the data must be <code>channels_first</code> or <code>channels_last</code>.</p>
<div class="highlight"><pre><span></span><code><span class="p">.</span><span class="n">Attr</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&gt;</span><span class="p">(</span><span class="s">&quot;data_format&quot;</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="p">(</span><span class="s">&quot;NCHW&quot;</span><span class="p">))</span>
<span class="p">.</span><span class="n">SetCheckAttrFn</span><span class="p">(</span>
  <span class="p">[](</span><span class="k">const</span> <span class="n">user_op</span><span class="o">::</span><span class="n">UserOpDefWrapper</span><span class="o">&amp;</span> <span class="n">def</span><span class="p">,</span>
    <span class="k">const</span> <span class="n">user_op</span><span class="o">::</span><span class="n">UserOpConfWrapper</span><span class="o">&amp;</span> <span class="n">conf</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Maybe</span><span class="o">&lt;</span><span class="kt">void</span><span class="o">&gt;</span> <span class="p">{</span>
   <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">data_format</span> <span class="o">=</span> <span class="n">conf</span><span class="p">.</span><span class="n">attr</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&gt;</span><span class="p">(</span><span class="s">&quot;data_format&quot;</span><span class="p">);</span>
   <span class="k">if</span> <span class="p">(</span><span class="n">data_format</span> <span class="o">==</span> <span class="s">&quot;channels_first&quot;</span> <span class="o">||</span> <span class="n">data_format</span> <span class="o">==</span> <span class="s">&quot;channels_last&quot;</span><span class="p">)</span> <span class="p">{</span>
     <span class="k">return</span> <span class="n">Maybe</span><span class="o">&lt;</span><span class="kt">void</span><span class="o">&gt;::</span><span class="n">Ok</span><span class="p">();</span>
   <span class="p">}</span>
   <span class="k">return</span> <span class="n">oneflow</span><span class="o">::</span><span class="n">Error</span><span class="o">::</span><span class="n">CheckFailed</span><span class="p">()</span>
         <span class="o">&lt;&lt;</span> <span class="s">&quot;data_format value: &quot;</span>
         <span class="o">&lt;&lt;</span> <span class="n">data_format</span>
         <span class="o">&lt;&lt;</span> <span class="s">&quot; for Conv op is illegal.&quot;</span><span class="p">;</span>
<span class="p">})</span>
</code></pre></div>
<p>Set a function to check that returns <code>Maybe&lt;void&gt;::Ok()</code> when the value of the attribute matches the requirement. Otherwise returns <code>oneflow::Error::CheckFailed()</code>.</p>
<h3 id="multiple-inoutput">Multiple In/Output<a class="headerlink" href="#multiple-inoutput" title="Permanent link">&para;</a></h3>
<p>For some ops, they may have multiple input or output and we need to specify the number of inputs and outputs when we register it.</p>
<p>Input example：</p>
<div class="highlight"><pre><span></span><code><span class="c1">// input must have 1 blob</span>
<span class="p">.</span><span class="n">Input</span><span class="p">(</span><span class="s">&quot;input&quot;</span><span class="p">)</span>

<span class="c1">// input must have 5 blobs</span>
<span class="p">.</span><span class="n">Input</span><span class="p">(</span><span class="s">&quot;input&quot;</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

<span class="c1">// input input must have at least 5  blobs</span>
<span class="p">.</span><span class="n">InputWithMinimum</span><span class="p">(</span><span class="s">&quot;input&quot;</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

<span class="c1">// input can have no blob or 1 blob</span>
<span class="p">.</span><span class="n">OptionalInput</span><span class="p">(</span><span class="s">&quot;input&quot;</span><span class="p">)</span>

<span class="c1">// input can have no blob or 5 blobs</span>
<span class="p">.</span><span class="n">OptionalInput</span><span class="p">(</span><span class="s">&quot;input&quot;</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

<span class="c1">// input can have no blob or at least 5 blobs</span>
<span class="p">.</span><span class="n">OptionalInputWithMininum</span><span class="p">(</span><span class="s">&quot;input&quot;</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</code></pre></div>
<p>Output setting is similar to Input.</p>
<h3 id="setgetsbpfn">SetGetSbpFn<a class="headerlink" href="#setgetsbpfn" title="Permanent link">&para;</a></h3>
<p><code>SetGetSbpFn</code> is for config the <a href="../basics_topics/essentials_of_oneflow.html#sbp">SBP</a> of this <code>op</code>.
Example of "add_n"：</p>
<div class="highlight"><pre><span></span><code><span class="n">REGISTER_USER_OP</span><span class="p">(</span><span class="s">&quot;add_n&quot;</span><span class="p">)</span>
    <span class="p">.</span><span class="n">InputWithMinimum</span><span class="p">(</span><span class="s">&quot;in&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="p">.</span><span class="n">Output</span><span class="p">(</span><span class="s">&quot;out&quot;</span><span class="p">)</span>
    <span class="p">.</span><span class="n">SetGetSbpFn</span><span class="p">([](</span><span class="n">user_op</span><span class="o">::</span><span class="n">SbpContext</span><span class="o">*</span> <span class="n">ctx</span><span class="p">)</span> <span class="p">{</span>
      <span class="kt">int64_t</span> <span class="n">num_axes</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">-&gt;</span><span class="n">LogicalTensorDesc4InputArgNameAndIndex</span><span class="p">(</span><span class="s">&quot;in&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">).</span><span class="n">shape</span><span class="p">().</span><span class="n">NumAxes</span><span class="p">();</span>
      <span class="k">for</span> <span class="p">(</span><span class="kt">int64_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">num_axes</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">ctx</span><span class="o">-&gt;</span><span class="n">NewBuilder</span><span class="p">().</span><span class="n">Split</span><span class="p">(</span><span class="n">ctx</span><span class="o">-&gt;</span><span class="n">inputs</span><span class="p">(),</span> <span class="n">i</span><span class="p">).</span><span class="n">Split</span><span class="p">(</span><span class="n">user_op</span><span class="o">::</span><span class="n">OpArg</span><span class="p">(</span><span class="s">&quot;out&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">i</span><span class="p">).</span><span class="n">Build</span><span class="p">();</span>
      <span class="p">}</span>
      <span class="n">ctx</span><span class="o">-&gt;</span><span class="n">NewBuilder</span><span class="p">().</span><span class="n">PartialSum</span><span class="p">(</span><span class="n">ctx</span><span class="o">-&gt;</span><span class="n">inputs</span><span class="p">()).</span><span class="n">PartialSum</span><span class="p">(</span><span class="n">user_op</span><span class="o">::</span><span class="n">OpArg</span><span class="p">(</span><span class="s">&quot;out&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)).</span><span class="n">Build</span><span class="p">();</span>
      <span class="k">return</span> <span class="n">Maybe</span><span class="o">&lt;</span><span class="kt">void</span><span class="o">&gt;::</span><span class="n">Ok</span><span class="p">();</span>
    <span class="p">});</span>
</code></pre></div>
<h2 id="detailed-introduction-of-opkernelregistry">Detailed Introduction of OpKernelRegistry<a class="headerlink" href="#detailed-introduction-of-opkernelregistry" title="Permanent link">&para;</a></h2>
<h3 id="setinfertmpsizefn">SetInferTmpSizeFn<a class="headerlink" href="#setinfertmpsizefn" title="Permanent link">&para;</a></h3>
<p>In some kernel implementations of op, some extra buffer may be required to store temporary data during the <code>Compute</code>.</p>
<p>We can specify the buffer size when registering the kernel by using the <code>SetInferTmpSizeFn</code>. Then we get the buffer and use it in the <code>Compute</code> function.</p>
<p>The following code registers the kernel with <code>SetInferTmpSizeFn</code> to specify a buffer size as 1024 bytes:</p>
<div class="highlight"><pre><span></span><code><span class="n">REGISTER_USER_KERNEL</span><span class="p">(</span><span class="s">&quot;XOp&quot;</span><span class="p">)</span>
    <span class="p">.</span><span class="n">SetInferTmpSizeFn</span><span class="p">(</span>
      <span class="p">[](</span><span class="k">const</span> <span class="n">oneflow</span><span class="o">::</span><span class="n">user_op</span><span class="o">::</span><span class="n">InferContext</span><span class="o">*</span><span class="p">)</span> <span class="p">{</span>
         <span class="k">return</span> <span class="mi">1024</span><span class="p">;</span>
      <span class="p">});</span>
</code></pre></div>
<p>Once the buffer size is set by <code>SetInferTmpSizeFn</code>, this buffer can be retrieved in <code>Compute</code> by calling the <code>KernelContext::Tensor4ArgNameAndIndex</code>. This buffer is encapsulated as <code>oneflow::user_op::Tensor</code> which can be converted to other types of pointers by calling the <code>dptr</code> or <code>mut_dptr</code>.</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">XKernel</span> <span class="k">final</span> <span class="o">:</span> <span class="k">public</span> <span class="n">oneflow</span><span class="o">::</span><span class="n">user_op</span><span class="o">::</span><span class="n">OpKernel</span> <span class="p">{</span>
  <span class="kt">void</span> <span class="nf">Compute</span><span class="p">(</span><span class="n">oneflow</span><span class="o">::</span><span class="n">user_op</span><span class="o">::</span><span class="n">KernelContext</span><span class="o">*</span> <span class="n">ctx</span><span class="p">)</span> <span class="k">override</span> <span class="p">{</span>
    <span class="n">oneflow</span><span class="o">::</span><span class="n">user_op</span><span class="o">::</span><span class="n">Tensor</span><span class="o">*</span> <span class="n">tmp</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">-&gt;</span><span class="n">Tensor4ArgNameAndIndex</span><span class="p">(</span><span class="s">&quot;tmp_buffer&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>

    <span class="c1">//The conversion yields a char* buffer of 1024 bytes.</span>
    <span class="kt">char</span><span class="o">*</span> <span class="n">pBuff</span> <span class="o">=</span> <span class="n">tmp</span><span class="o">-&gt;</span><span class="n">mut_dptr</span><span class="o">&lt;</span><span class="kt">char</span><span class="o">&gt;</span><span class="p">();</span>

    <span class="p">...</span>
  <span class="p">}</span>
<span class="p">};</span>
</code></pre></div>
<h2 id="detailed-introduction-of-opgradregistry">Detailed Introduction of OpGradRegistry<a class="headerlink" href="#detailed-introduction-of-opgradregistry" title="Permanent link">&para;</a></h2>
<p>Oneflow is automatically get gradient during backward map expansion and the OneFlow framework uses <a href="https://en.wikipedia.org/wiki/Automatic_differentiation">Automatic Differentiation</a> to get the gradient which means automatically find the gradient of the entire expression using the chain rule.</p>
<p>In order to automatically get gradient a custom op, we need to register it with <code>REGISTER_USER_OP_GRAD</code>. From a mathematical point of view, the registration process is the computation of the backward derivation that we specify for our custom op. From a programming point of view, it is to set up a backward-generating function for a custom op. Within that function, we write code that specifies how the input gradient of that op is to be calculated.</p>
<p>In order to calculate the gradient of a custom op, we need to construct the gradient of the input base on the input and output of the custom op. In most cases, we can represent the process of calculating the gradient of the input through the existing operators and their combination in OneFlow.</p>
<p>The calculation of the input gradient usually consists of the following steps:</p>
<ol>
<li>
<p>Use <code>ctx-&gt;DefineOp()</code> and <code>BackwardOpBuilder</code> to represent methods for calculating input gradients. Because input gradient calculations may be combinations of multiple operations. Therefore <code>DefineOp</code> and <code>BackwardOpBuilder</code> may be used for multiple times.</p>
</li>
<li>
<p>After defining the calculation process in the previous step, the required gradient is finally recorded in the output of some operator. We need to call the <code>ctx-&gt;FwOp().InputGradBind()</code> to combine the result of the previous calculation to the input gradient of the custom op.</p>
</li>
</ol>
<p>The following example (the complete code, including tests, can be found in <a href="https://github.com/Oneflow-Inc/oneflow-documentation/tree/master/cn/docs/code/extended_topics/myop_grad">myop_grad repository</a>). A custom op called <code>myop</code> will be used to register backward generating functions. This op is only used in this document to show the registration process which compute function is set as <code>3*x*x</code>.</p>
<p>Then it is easy to obtain the relationship between its forward and backward propagation as shown below. The gradient of <code>x</code> in the reverse process is computed as <code>6*x*dy</code>.</p>
<div align="center">
  <img src="imgs/chainrule.png">
  </img>
</div>

<p>The forward op of <code>myop</code> is defined as follows:</p>
<div class="highlight"><pre><span></span><code><span class="n">REGISTER_USER_OP</span><span class="p">(</span><span class="s">&quot;myop&quot;</span><span class="p">).</span><span class="n">Input</span><span class="p">(</span><span class="s">&quot;in&quot;</span><span class="p">).</span><span class="n">Output</span><span class="p">(</span><span class="s">&quot;out&quot;</span><span class="p">).</span><span class="n">SetTensorDescInferFn</span><span class="p">(</span>
    <span class="p">[](</span><span class="n">user_op</span><span class="o">::</span><span class="n">InferContext</span> <span class="o">*</span><span class="n">ctx</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Maybe</span><span class="o">&lt;</span><span class="kt">void</span><span class="o">&gt;</span> <span class="p">{</span>
      <span class="o">*</span><span class="n">ctx</span><span class="o">-&gt;</span><span class="n">Shape4ArgNameAndIndex</span><span class="p">(</span><span class="s">&quot;out&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">=</span>
          <span class="o">*</span><span class="n">ctx</span><span class="o">-&gt;</span><span class="n">Shape4ArgNameAndIndex</span><span class="p">(</span><span class="s">&quot;in&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
      <span class="o">*</span><span class="n">ctx</span><span class="o">-&gt;</span><span class="n">Dtype4ArgNameAndIndex</span><span class="p">(</span><span class="s">&quot;out&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">=</span>
          <span class="o">*</span><span class="n">ctx</span><span class="o">-&gt;</span><span class="n">Dtype4ArgNameAndIndex</span><span class="p">(</span><span class="s">&quot;in&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
      <span class="k">return</span> <span class="n">Maybe</span><span class="o">&lt;</span><span class="kt">void</span><span class="o">&gt;::</span><span class="n">Ok</span><span class="p">();</span>
    <span class="p">});</span>
</code></pre></div>
<p>That is <code>myop</code> contains the only input <code>in</code> and the only output <code>out</code>.</p>
<p>The reverse gradient registration of <code>myop</code> is as follows:</p>
<div class="highlight"><pre><span></span><code><span class="n">REGISTER_USER_OP_GRAD</span><span class="p">(</span><span class="s">&quot;myop&quot;</span><span class="p">).</span><span class="n">SetBackwardOpConfGenFn</span><span class="p">(</span>
    <span class="p">[](</span><span class="n">user_op</span><span class="o">::</span><span class="n">BackwardOpConfContext</span><span class="o">*</span> <span class="n">ctx</span><span class="p">)</span> <span class="p">{</span>

      <span class="k">const</span> <span class="k">auto</span> <span class="n">op1_name</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">-&gt;</span><span class="n">FwOp</span><span class="p">().</span><span class="n">op_name</span><span class="p">()</span> <span class="o">+</span> <span class="s">&quot;_grad1&quot;</span><span class="p">;</span>

      <span class="c1">// The operator op1_name is used to calculate the gradient of myop.in</span>
      <span class="n">ctx</span><span class="o">-&gt;</span><span class="n">DefineOp</span><span class="p">(</span><span class="n">op1_name</span><span class="p">,</span>
        <span class="p">[</span><span class="o">&amp;</span><span class="n">ctx</span><span class="p">](</span><span class="n">user_op</span><span class="o">::</span><span class="n">BackwardOpBuilder</span><span class="o">&amp;</span> <span class="n">builder</span><span class="p">)</span> <span class="p">{</span>
          <span class="k">return</span> <span class="n">builder</span><span class="p">.</span><span class="n">OpTypeName</span><span class="p">(</span><span class="s">&quot;multiply&quot;</span><span class="p">)</span>
              <span class="p">.</span><span class="n">InputBind</span><span class="p">(</span><span class="s">&quot;x&quot;</span><span class="p">,</span> <span class="n">ctx</span><span class="o">-&gt;</span><span class="n">FwOp</span><span class="p">().</span><span class="n">input</span><span class="p">(</span><span class="s">&quot;in&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span> <span class="c1">//multiply.x &lt;- myop.in</span>
              <span class="p">.</span><span class="n">InputBind</span><span class="p">(</span><span class="s">&quot;y&quot;</span><span class="p">,</span> <span class="n">ctx</span><span class="o">-&gt;</span><span class="n">FwOp</span><span class="p">().</span><span class="n">output_grad</span><span class="p">(</span><span class="s">&quot;out&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span> <span class="c1">//multiply.y &lt;-  the gradient of myop.out</span>
              <span class="p">.</span><span class="n">Output</span><span class="p">(</span><span class="s">&quot;out&quot;</span><span class="p">)</span>
              <span class="p">.</span><span class="n">Build</span><span class="p">();</span>
        <span class="p">});</span>

      <span class="k">const</span> <span class="k">auto</span> <span class="n">op2_name</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">-&gt;</span><span class="n">FwOp</span><span class="p">().</span><span class="n">op_name</span><span class="p">()</span> <span class="o">+</span> <span class="s">&quot;_grad2&quot;</span><span class="p">;</span>
      <span class="c1">// The operator op2_name is used to calculate 6*op1_name.</span>
      <span class="n">ctx</span><span class="o">-&gt;</span><span class="n">DefineOp</span><span class="p">(</span><span class="n">op2_name</span><span class="p">,</span>
        <span class="p">[</span><span class="o">&amp;</span><span class="n">ctx</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">op1_name</span><span class="p">](</span><span class="n">user_op</span><span class="o">::</span><span class="n">BackwardOpBuilder</span><span class="o">&amp;</span> <span class="n">builder</span><span class="p">)</span> <span class="p">{</span>
          <span class="k">return</span> <span class="n">builder</span><span class="p">.</span><span class="n">OpTypeName</span><span class="p">(</span><span class="s">&quot;scalar_mul&quot;</span><span class="p">)</span>
              <span class="p">.</span><span class="n">InputBind</span><span class="p">(</span><span class="s">&quot;in&quot;</span><span class="p">,</span> <span class="n">ctx</span><span class="o">-&gt;</span><span class="n">GetOp</span><span class="p">(</span><span class="n">op1_name</span><span class="p">).</span><span class="n">output</span><span class="p">(</span><span class="s">&quot;out&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
              <span class="p">.</span><span class="n">Attr</span><span class="p">(</span><span class="s">&quot;has_float_operand&quot;</span><span class="p">,</span> <span class="nb">true</span><span class="p">)</span>
              <span class="p">.</span><span class="n">Attr</span><span class="p">(</span><span class="s">&quot;has_int_operand&quot;</span><span class="p">,</span> <span class="nb">false</span><span class="p">)</span>
              <span class="p">.</span><span class="n">Attr</span><span class="p">(</span><span class="s">&quot;float_operand&quot;</span><span class="p">,</span> <span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="p">(</span><span class="mi">6</span><span class="p">))</span>
              <span class="p">.</span><span class="n">Attr</span><span class="p">(</span><span class="s">&quot;int_operand&quot;</span><span class="p">,</span> <span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="p">(</span><span class="mi">6</span><span class="p">))</span>
              <span class="p">.</span><span class="n">Output</span><span class="p">(</span><span class="s">&quot;out&quot;</span><span class="p">)</span>
              <span class="p">.</span><span class="n">Build</span><span class="p">();</span>
        <span class="p">});</span>

      <span class="c1">// (the gradient of myop.in) &lt;- op1_name.out</span>
      <span class="n">ctx</span><span class="o">-&gt;</span><span class="n">FwOp</span><span class="p">().</span><span class="n">InputGradBind</span><span class="p">(</span><span class="n">user_op</span><span class="o">::</span><span class="n">OpArg</span><span class="p">(</span><span class="s">&quot;in&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
        <span class="p">[</span><span class="o">&amp;</span><span class="n">ctx</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">op2_name</span><span class="p">]()</span> <span class="o">-&gt;</span> <span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&amp;</span> <span class="p">{</span>
          <span class="k">return</span> <span class="n">ctx</span><span class="o">-&gt;</span><span class="n">GetOp</span><span class="p">(</span><span class="n">op2_name</span><span class="p">)</span>
                <span class="p">.</span><span class="n">output</span><span class="p">(</span><span class="s">&quot;out&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
        <span class="p">});</span>
  <span class="p">});</span>
</code></pre></div>
<p>The string parameter accepted by <code>REGISTER_USER_OP_GRAD("myop")</code> is <code>op_type_name</code> which needs to be the same as registered with <code>REGISTER_USER_OP</code>.</p>
<p><code>REGISTER_USER_OP_GRAD("myop")</code> returns an <code>oneflow::user_op::OpGradRegistry</code> object that we can call it to set the custom op's backward generating function.</p>
<p>In the above gradient registration process, the expression for the gradient of <code>myop</code> is <code>6*x*dy</code> which is demonstrated in the code.</p>
<p>First <code>op1_name</code> is defined and <code>x*dy</code> is solved by using the existing operator <code>multiply</code>:</p>
<div class="highlight"><pre><span></span><code><span class="c1">//  The operator op1_name is used to calculate the gradient of myop.in</span>
<span class="n">ctx</span><span class="o">-&gt;</span><span class="n">DefineOp</span><span class="p">(</span><span class="n">op1_name</span><span class="p">,</span>
  <span class="p">[</span><span class="o">&amp;</span><span class="n">ctx</span><span class="p">](</span><span class="n">user_op</span><span class="o">::</span><span class="n">BackwardOpBuilder</span><span class="o">&amp;</span> <span class="n">builder</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">return</span> <span class="n">builder</span><span class="p">.</span><span class="n">OpTypeName</span><span class="p">(</span><span class="s">&quot;multiply&quot;</span><span class="p">)</span>
        <span class="p">.</span><span class="n">InputBind</span><span class="p">(</span><span class="s">&quot;x&quot;</span><span class="p">,</span> <span class="n">ctx</span><span class="o">-&gt;</span><span class="n">FwOp</span><span class="p">().</span><span class="n">input</span><span class="p">(</span><span class="s">&quot;in&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span> <span class="c1">//multiply.x &lt;- myop.in</span>
        <span class="p">.</span><span class="n">InputBind</span><span class="p">(</span><span class="s">&quot;y&quot;</span><span class="p">,</span> <span class="n">ctx</span><span class="o">-&gt;</span><span class="n">FwOp</span><span class="p">().</span><span class="n">output_grad</span><span class="p">(</span><span class="s">&quot;out&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span> <span class="c1">//multiply.y &lt;- myop.out的梯度</span>
        <span class="p">.</span><span class="n">Output</span><span class="p">(</span><span class="s">&quot;out&quot;</span><span class="p">)</span>
        <span class="p">.</span><span class="n">Build</span><span class="p">();</span>
  <span class="p">});</span>
</code></pre></div>
<p>Then <code>op2_name</code> is defined and use the existing operator <code>op2_name</code> to solve for <code>6*op1_name</code>.</p>
<div class="highlight"><pre><span></span><code><span class="c1">// The operator op2_name is used to calculate 6*op1_name.</span>
<span class="n">ctx</span><span class="o">-&gt;</span><span class="n">DefineOp</span><span class="p">(</span><span class="n">op2_name</span><span class="p">,</span>
  <span class="p">[</span><span class="o">&amp;</span><span class="n">ctx</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">op1_name</span><span class="p">](</span><span class="n">user_op</span><span class="o">::</span><span class="n">BackwardOpBuilder</span><span class="o">&amp;</span> <span class="n">builder</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">return</span> <span class="n">builder</span><span class="p">.</span><span class="n">OpTypeName</span><span class="p">(</span><span class="s">&quot;scalar_mul&quot;</span><span class="p">)</span>
        <span class="p">.</span><span class="n">InputBind</span><span class="p">(</span><span class="s">&quot;in&quot;</span><span class="p">,</span> <span class="n">ctx</span><span class="o">-&gt;</span><span class="n">GetOp</span><span class="p">(</span><span class="n">op1_name</span><span class="p">).</span><span class="n">output</span><span class="p">(</span><span class="s">&quot;out&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
        <span class="p">.</span><span class="n">Attr</span><span class="p">(</span><span class="s">&quot;has_float_operand&quot;</span><span class="p">,</span> <span class="nb">true</span><span class="p">)</span>
        <span class="p">.</span><span class="n">Attr</span><span class="p">(</span><span class="s">&quot;has_int_operand&quot;</span><span class="p">,</span> <span class="nb">false</span><span class="p">)</span>
        <span class="p">.</span><span class="n">Attr</span><span class="p">(</span><span class="s">&quot;float_operand&quot;</span><span class="p">,</span> <span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="p">(</span><span class="mi">6</span><span class="p">))</span>
        <span class="p">.</span><span class="n">Attr</span><span class="p">(</span><span class="s">&quot;int_operand&quot;</span><span class="p">,</span> <span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="p">(</span><span class="mi">6</span><span class="p">))</span>
        <span class="p">.</span><span class="n">Output</span><span class="p">(</span><span class="s">&quot;out&quot;</span><span class="p">)</span>
        <span class="p">.</span><span class="n">Build</span><span class="p">();</span>
  <span class="p">});</span>
</code></pre></div>
<p>Finally bind the output of <code>op2_name</code> (i.e., <code>6*x*dy</code>) to the input of <code>myop</code> to complete the registration.</p>
<div class="highlight"><pre><span></span><code><span class="c1">// (the gradient of myop.in) &lt;- op1_name.out</span>
<span class="n">ctx</span><span class="o">-&gt;</span><span class="n">FwOp</span><span class="p">().</span><span class="n">InputGradBind</span><span class="p">(</span><span class="n">user_op</span><span class="o">::</span><span class="n">OpArg</span><span class="p">(</span><span class="s">&quot;in&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
  <span class="p">[</span><span class="o">&amp;</span><span class="n">ctx</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">op2_name</span><span class="p">]()</span> <span class="o">-&gt;</span> <span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&amp;</span> <span class="p">{</span>
    <span class="k">return</span> <span class="n">ctx</span><span class="o">-&gt;</span><span class="n">GetOp</span><span class="p">(</span><span class="n">op2_name</span><span class="p">)</span>
          <span class="p">.</span><span class="n">output</span><span class="p">(</span><span class="s">&quot;out&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
  <span class="p">});</span>
</code></pre></div>
<p>The above code is the complete process of registering a gradient and the related classes and methods will be described in below.</p>
<h3 id="setbackwardopconfgenfn">SetBackwardOpConfGenFn<a class="headerlink" href="#setbackwardopconfgenfn" title="Permanent link">&para;</a></h3>
<p>We use <code>OpGradRegistry::SetBackwardOpConfGenFn(fn)</code> to set the backward generating function <code>fn</code> which has the following prototype:</p>
<div class="highlight"><pre><span></span><code><span class="kt">void</span> <span class="nf">fn</span><span class="p">(</span><span class="n">BackwardOpConfContext</span><span class="o">*</span> <span class="n">ctx</span><span class="p">);</span>
</code></pre></div>
<p><code>BackwardOpConfContext* ctx</code> has all information needed to generate the op.</p>
<h3 id="backwardopconfcontext">BackwardOpConfContext<a class="headerlink" href="#backwardopconfcontext" title="Permanent link">&para;</a></h3>
<p>The common methods and their purpose used in <code>BackwardOpConfContext</code> as follows:</p>
<ul>
<li>
<p><code>UserOpWrapper&amp; FwOp();</code>: Get forward op.</p>
</li>
<li>
<p><code>GetOp(op_name)</code>: Create and get the corresponding <code>op</code> based on <code>op_name</code>. <code>GetOp</code> uses a lazy init mechanism and the corresponding op is not actually created until <code>GetOp</code> is called.</p>
</li>
<li>
<p><code>void DefineOp(op_name, fn)</code>：Define <code>fn</code> of the op named <code>op_name</code>. When <code>ctx-&gt;GetOp(op_name)</code> is called, <code>fn</code> is triggered in the OneFlow for Op creation and if the op has already been created. Then the result is retrieved directly. The <code>fn</code> receives a <code>BackwardOpBuilder</code> parameter for constructing the reverse op. We will introduce <code>BackwardOpBuilder</code> later on.</p>
</li>
</ul>
<h3 id="detailed-introduction-of-backwardopbuilder">Detailed Introduction of BackwardOpBuilder<a class="headerlink" href="#detailed-introduction-of-backwardopbuilder" title="Permanent link">&para;</a></h3>
<p><code>BackwardOpBuilder</code> is used to build a reverse op. The fragment of above code is an example:</p>
<div class="highlight"><pre><span></span><code><span class="n">ctx</span><span class="o">-&gt;</span><span class="n">DefineOp</span><span class="p">(</span><span class="n">op1_name</span><span class="p">,</span>
  <span class="p">[</span><span class="o">&amp;</span><span class="n">ctx</span><span class="p">](</span><span class="n">user_op</span><span class="o">::</span><span class="n">BackwardOpBuilder</span><span class="o">&amp;</span> <span class="n">builder</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">return</span> <span class="n">builder</span><span class="p">.</span><span class="n">OpTypeName</span><span class="p">(</span><span class="s">&quot;multiply&quot;</span><span class="p">)</span>
        <span class="p">.</span><span class="n">InputBind</span><span class="p">(</span><span class="s">&quot;x&quot;</span><span class="p">,</span> <span class="n">ctx</span><span class="o">-&gt;</span><span class="n">FwOp</span><span class="p">().</span><span class="n">input</span><span class="p">(</span><span class="s">&quot;in&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span> <span class="c1">//multiply.x &lt;- myop.in</span>
        <span class="p">.</span><span class="n">InputBind</span><span class="p">(</span><span class="s">&quot;y&quot;</span><span class="p">,</span> <span class="n">ctx</span><span class="o">-&gt;</span><span class="n">FwOp</span><span class="p">().</span><span class="n">output_grad</span><span class="p">(</span><span class="s">&quot;out&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span> <span class="c1">//multiply.y &lt;- myop.out的梯度</span>
        <span class="p">.</span><span class="n">Output</span><span class="p">(</span><span class="s">&quot;out&quot;</span><span class="p">)</span>
        <span class="p">.</span><span class="n">Build</span><span class="p">();</span>
  <span class="p">});</span>
</code></pre></div>
<p>In this function, we call <code>Build</code> to build a reverse op for computing <code>x*dy</code>.
The purpose of each operator is as follows:</p>
<ul>
<li>
<p><code>OpTypeName("multiply")</code> specifies the <code>op_type_name</code> of an op that is used to help us compute the reverse gradient.</p>
</li>
<li>
<p><code>InputBind(arg_name, blob)</code> binds the input <code>arg_name</code> of <code>multiply</code> to the specified <code>blob</code> and can be called for multiple times. If the <code>arg_name</code> corresponds to multiple  blob which means the order of <code>Input</code> is the order of the corresponding index.</p>
</li>
<li>
<p><code>Output(arg_name, num)</code> Specifies the number of output blobes that actually correspond to the <code>arg_name</code> which defaults to 1 if <code>num</code> is not filled in.</p>
</li>
<li>
<p><code>Attr(attr_name, val)</code> sets the value of the attribute which same in the registration.</p>
</li>
<li>
<p>Calling <code>Build()</code> after above configuration, then the construction of the reverse op is completed.</p>
</li>
</ul>
<h3 id="detailed-introduction-of-useropwrapper">Detailed Introduction of UserOpWrapper<a class="headerlink" href="#detailed-introduction-of-useropwrapper" title="Permanent link">&para;</a></h3>
<p>Calling <code>ctx-&gt;FwOp()</code> will return the <code>UserOpWrapper</code>of <code>myop</code> and complete the gradient binding by calling the <code>UserOpWrapper</code>.</p>
<div class="highlight"><pre><span></span><code><span class="n">ctx</span><span class="o">-&gt;</span><span class="n">FwOp</span><span class="p">().</span><span class="n">InputGradBind</span><span class="p">(</span><span class="n">user_op</span><span class="o">::</span><span class="n">OpArg</span><span class="p">(</span><span class="s">&quot;in&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
  <span class="p">[</span><span class="o">&amp;</span><span class="n">ctx</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">op2_name</span><span class="p">]()</span> <span class="o">-&gt;</span> <span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&amp;</span> <span class="p">{</span>
    <span class="k">return</span> <span class="n">ctx</span><span class="o">-&gt;</span><span class="n">GetOp</span><span class="p">(</span><span class="n">op2_name</span><span class="p">)</span>
          <span class="p">.</span><span class="n">output</span><span class="p">(</span><span class="s">&quot;out&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
  <span class="p">});</span>
</code></pre></div>
<p>Common methods for <code>UserOpWrapper</code> are:</p>
<ul>
<li>
<p><code>InputGradBind(input, grad_fn)</code>：Bind the input of the forward op and get the gradient function <code>grad_fn</code>.  OneFlow automatically determines whether input needs to generate a backward gradient, if needed, it will trigger <code>grad_fn</code> and binds the input.</p>
</li>
<li>
<p><code>input(arg_name, index)</code>：Get the blob corresponding to the <code>arg_name</code> of input.</p>
</li>
<li>
<p><code>output(arg_name,index)</code>：Get the blob corresponding to the <code>arg_name</code> of output.</p>
</li>
<li>
<p><code>output_grad(output_arg_name, index)</code>：Get the <code>output_arg_name</code> of the forward op which is the blob of the corresponding backward gradient.</p>
</li>
<li>
<p><code>attr(attr_name)</code>：Get the value corresponding to the <code>attr_name</code>.</p>
</li>
<li>
<p><code>arg_tensor_desc(arg_name, index)</code>：Returns the input/output tensor information of the forward op which including <code>shape</code>, <code>dtype</code> and etc.</p>
</li>
</ul>
<h3 id="customized-op-for-calculating-gradients">Customized Op for Calculating Gradients<a class="headerlink" href="#customized-op-for-calculating-gradients" title="Permanent link">&para;</a></h3>
<p>As we mentioned earlier, in most cases, the process of calculating a gradient can be represented by a combination of existing ops. However, when it is difficult to use an existing op to solve the gradient for a particular forward op that we need to design and create operators specifically for the gradient calculation. Example can be found in: <a href="https://github.com/Oneflow-Inc/oneflow/blob/master/oneflow/user/ops/relu_op.cpp">relu_op.cpp</a>.</p>
<h2 id="detailed-introduction-of-useropconfbuilder">Detailed Introduction of UserOpConfBuilder<a class="headerlink" href="#detailed-introduction-of-useropconfbuilder" title="Permanent link">&para;</a></h2>
<p>In Python frontend of OneFlow, we provide <code>UserOpConfBuilder</code> to build the wrapper of custom op which is used in <a href="user_op.html#python-op">Use custom opp in Python</a> previously. Here is the summary of the relationship between <code>UserOpConfBuilder</code> in Python layer and C++ layer.</p>
<p>For example, we have wrapped a <code>cast</code>:</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">cast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="n">flow</span><span class="o">.</span><span class="n">user_op_builder</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
        <span class="o">.</span><span class="n">Op</span><span class="p">(</span><span class="s2">&quot;cast&quot;</span><span class="p">)</span>
        <span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="s2">&quot;in&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">])</span>
        <span class="o">.</span><span class="n">Output</span><span class="p">(</span><span class="s2">&quot;out&quot;</span><span class="p">)</span>
        <span class="o">.</span><span class="n">Attr</span><span class="p">(</span><span class="s2">&quot;dtype&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
        <span class="o">.</span><span class="n">Build</span><span class="p">()</span>
        <span class="o">.</span><span class="n">InferAndTryRun</span><span class="p">()</span>
        <span class="o">.</span><span class="n">RemoteBlobList</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
    <span class="p">)</span>
<span class="p">)</span>
</code></pre></div>
<ul>
<li>
<p><code>Op(op_type_name)</code>：The accepted parameter is <code>op_type_name</code> when it is registered in C++.</p>
</li>
<li>
<p><code>Input(input_name, input_blob_list)</code>：<code>input_name</code> should be the same as the first parameter of <code>Input</code> when registering this op in C++.</p>
</li>
<li>
<p><code>Output(output_name, num=1)</code>：<code>output_name</code> and <code>num</code> should be the same as <code>Output</code> of op when registration in C++.</p>
</li>
<li>
<p><code>Attr(attr_name, attr_value)</code>：<code>attr_name</code> corresponds to the attribute of <code>OpRegistry::Attr</code> used for C++ registration and <code>attr_value</code> should be the same type as the attribute type when declaration.</p>
</li>
<li>
<p><code>Build()</code>：Build the user op for the Python layer.</p>
</li>
</ul>
<p>The derivation can be done by calling <code>InferAndTryRun</code> in the user op and the result can be retrieved by calling <code>RemoteBlobList</code> or <code>SoleOutputBlob</code>.</p>
<ul>
<li>
<p><code>RemoteBlobList</code>：Get all outputs which applies to op with multiple outputs and all ops are placed in a list.</p>
</li>
<li>
<p><code>SoleOutputBlob</code>：Get unique outputs which applies to op with one output.</p>
</li>
</ul>
                
              
              
                


              
            </article>
          </div>
        </div>
        
      </main>
      
        
<footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="Footer">
      
        
        <a href="debug_by_vscode.html" class="md-footer__link md-footer__link--prev" aria-label="Previous: Use VS Code to Debug OneFlow" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              Use VS Code to Debug OneFlow
            </div>
          </div>
        </a>
      
      
        
        <a href="oneflow_convert_tools.html" class="md-footer__link md-footer__link--next" aria-label="Next: OneFlow And ONNX Convert" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              OneFlow And ONNX Convert
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            Copyright &copy; 2017 - 2021 OneFlow
          </div>
        
        Made with
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs
        </a>
        
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "..", "features": ["navigation.tabs"], "translations": {"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "search": "../assets/javascripts/workers/search.477d984a.min.js", "version": {"provider": "mike"}}</script>
    
    
      <script src="../assets/javascripts/bundle.ddd52ceb.min.js"></script>
      
    
  </body>
</html>