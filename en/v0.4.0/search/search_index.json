{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"OneFlow: A Whole New Generation Of Deep Learning Framework \u00b6 An open source deep learning framework with whole new frame design and the world's leading technology for distributed system. Key Features & Capabilities \u00b6 Make distributed training with multi-machines and multi-devices so simple as on single device. Perfectly support container platforms(k8s & docker) Handle large models easily Almost zero runtime overhead & linear speedup Support multiple deep learning compilers(XLA, TensorRT etc) Support automatic mixed precision Will support more operators and models sustainsouly etc. We are trying to build a deep learning framework that will amaze everyone! Looking forward to your feedbacks and welcome to join our contributor community . Get Started \u00b6 Follow the instructions to install OneFlow. Tackle the common tasks of machine learning with OneFlow in Basic topics . Such as building network, hyper-parameters configuration, loading data, distributed training and so on. If you want to know more about the characteristics of OneFlow, such as the format of OneFlow's dataset, the parallelism view of OneFlow or how to debug OneFlow framework with vscode, please refer to extended topic . In advanced examples , we introduce models in OneFlow Model Zoo repository . It is helpful for users to understand the models and other details. We highly expect developers and geeks to join our contributor community . Together we can build a perfect deep learning framework.","title":"Home"},{"location":"index.html#oneflow-a-whole-new-generation-of-deep-learning-framework","text":"An open source deep learning framework with whole new frame design and the world's leading technology for distributed system.","title":"OneFlow: A Whole New Generation Of Deep Learning Framework"},{"location":"index.html#key-features-capabilities","text":"Make distributed training with multi-machines and multi-devices so simple as on single device. Perfectly support container platforms(k8s & docker) Handle large models easily Almost zero runtime overhead & linear speedup Support multiple deep learning compilers(XLA, TensorRT etc) Support automatic mixed precision Will support more operators and models sustainsouly etc. We are trying to build a deep learning framework that will amaze everyone! Looking forward to your feedbacks and welcome to join our contributor community .","title":"Key Features &amp; Capabilities"},{"location":"index.html#get-started","text":"Follow the instructions to install OneFlow. Tackle the common tasks of machine learning with OneFlow in Basic topics . Such as building network, hyper-parameters configuration, loading data, distributed training and so on. If you want to know more about the characteristics of OneFlow, such as the format of OneFlow's dataset, the parallelism view of OneFlow or how to debug OneFlow framework with vscode, please refer to extended topic . In advanced examples , we introduce models in OneFlow Model Zoo repository . It is helpful for users to understand the models and other details. We highly expect developers and geeks to join our contributor community . Together we can build a perfect deep learning framework.","title":"Get Started"},{"location":"adv_examples/bert.html","text":"Summary \u00b6 BERT(Bidirectional Encoder Representations from Transformers) is a technique for NLP. In our case, we implement BERT based on the paper BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding using OneFlow. Model \u00b6 Model Hidden layers Hidden unit size Attention heads Feedforward filter size Max sequence length Parameters BERTBASE 12 encoder 768 12 4 x 768 512 110M There are commonly two steps in BERT: First, BERT pretrained model is obtained by pre-training; Then, on the basis of the obtained pretrained model, an additional layer of network is added and finetuned to get the downstream application. Quickstart \u00b6 Get dataset \u00b6 We provide OFRecord dataset and relevant other files , you can get and unzip it by running commands below: wget https://oneflow-static.oss-cn-beijing.aliyuncs.com/oneflow-tutorial-attachments/bert_squad_dataset.zip unzip bert_squad_dataset.zip The list of files is as follows: bert_config.json\u3001vocab.txt\uff1aFiles needed to generate \"prediction json\" file from google bert dev-v1.1/, dev-v1.1.json\uff1aSQuAD test set for evaluation part-0\uff1apre-trained training set contains 40 samples train-v1.1\uff1aSQuAD training set that has been coverted to OFRecords The above files will be used in the following pretraining tasks and squad finetune. BERT pretrained \u00b6 Firstly, clone the OneFlow-Benchmark : git clone https://github.com/Oneflow-Inc/OneFlow-Benchmark.git cd OneFlow-Benchmark/LanguageModeling/BERT/ Then, with the following command, we can use our pretraining model and small sample set to start the BERT pre-training. python ./run_pretraining.py \\ --gpu_num_per_node = 1 \\ --learning_rate = 3e-5 \\ --batch_size_per_device = 1 \\ --iter_num = 3 \\ --loss_print_every_n_iter = 50 \\ --seq_length = 128 \\ --max_predictions_per_seq = 20 \\ --num_hidden_layers = 12 \\ --num_attention_heads = 12 \\ --max_position_embeddings = 512 \\ --type_vocab_size = 2 \\ --vocab_size = 30522 \\ --attention_probs_dropout_prob = 0 .0 \\ --hidden_dropout_prob = 0 .0 \\ --hidden_size_per_head = 64 \\ --use_boxing_v2 = True \\ --data_dir = ./dataset/ \\ --data_part_num = 1 \\ --log_dir = ./bert_regresssioin_test/of \\ --loss_print_every_n_iter = 5 \\ --model_save_dir = ./bert_regresssioin_test/of \\ --warmup_batches 831 \\ --save_last_snapshot True We will see the output similar to the following: ================================================================== Running bert: num_gpu_per_node = 1, num_nodes = 1. ================================================================== gpu_num_per_node = 1 node_num = 1 node_list = None learning_rate = 3e-05 weight_decay_rate = 0.01 batch_size_per_device = 1 iter_num = 20 warmup_batches = 831 log_every_n_iter = 1 data_dir = ./dataset/ data_part_num = 1 use_fp16 = None use_boxing_v2 = True loss_print_every_n_iter = 5 model_save_every_n_iter = 10000 model_save_dir = ./bert_regresssioin_test/of save_last_snapshot = True model_load_dir = None log_dir = ./bert_regresssioin_test/of seq_length = 128 max_predictions_per_seq = 20 num_hidden_layers = 12 num_attention_heads = 12 max_position_embeddings = 512 type_vocab_size = 2 vocab_size = 30522 attention_probs_dropout_prob = 0.0 hidden_dropout_prob = 0.0 hidden_size_per_head = 64 ------------------------------------------------------------------ Time stamp: 2020-07-06-19:09:29 I0706 19:09:29.605840639 34801 ev_epoll_linux.c:82] Use of signals is disabled. Epoll engine will not be used Init model on demand iter 4, total_loss: 11.032, mlm_loss: 10.281, nsp_loss: 0.751, speed: 33.086(sec/batch), 0.151(sentences/sec) iter 9, total_loss: 11.548, mlm_loss: 10.584, nsp_loss: 0.965, speed: 0.861(sec/batch), 5.806(sentences/sec) iter 14, total_loss: 10.697, mlm_loss: 10.249, nsp_loss: 0.448, speed: 0.915(sec/batch), 5.463(sentences/sec) iter 19, total_loss: 10.685, mlm_loss: 10.266, nsp_loss: 0.419, speed: 1.087(sec/batch), 4.602(sentences/sec) Saving model to ./bert_regresssioin_test/of/last_snapshot. ------------------------------------------------------------------ average speed: 0.556(sentences/sec) ------------------------------------------------------------------ Detailed description \u00b6 Scripts \u00b6 Files Description Belongs to pretrain.py\u3001bert.py Define the BERT model BERT run_pretraining.py Start BERT training. The user can configure the training environment and parameters of the BERT training through the command line parameters. The specific meanings of each option will be described in the script options below. BERT squad.py define SQuAD network SQuAD run_squad.py Run the SQuAD training SQuAD run_squad_predict.py Run the trained SQuAD model to predict. SQuAD npy2json.py Script required to overt OneFlow's prediction results to json. SQuAD convert_tf_ckpt_to_of.py Convert model from TensorFlow to OneFlow BERT/SQuAD Options \u00b6 The script run_pretraining.py runs the pretraining and configured by command line options. You can run run_pretraining.py --help to see the options. The following is a detailed description of each option\uff1a gpu_num_per_node: count of devices on each node which must be consistent on each machine node_num: count of nodes, that is, the count of hosts in distributed system node_list: list of nodes. When thec count of nodes is more than one, we should spcifiy list of nodes by node_list. It's a string seperated by commans like --node_num=2 --node_list=\"192.168.1.12,192.168.1.14\" learning_rate: learning rate weight_decay_rate: decay rate of weight batch_size_per_device: batch size on each device iter_num ITER_NUM: count of iterations warmup_batches: batches of warmup, default to 10000 data_dir: path to OFRecord dataset data_part_num: number of files in the folder of OFRecord dataset use_fp16: use float16 or not use_boxing_v2: use boxing v2 or not loss_print_every_n_iter: print loss every n iterations model_save_every_n_iter: save the model every n iterations model_save_dir: path to save the model save_last_snapshot: whether save the model when training is finished model_load_dir: path to load the model log_dir LOG_DIR: specify the path of log seq_length: length of sequence, default to 512 max_predictions_per_seq: default to 80 num_hidden_layers: number of hidden layers, defaul to 24 num_attention_heads: number of attentoion heads\uff0cdefault to 16 Use Wikipedia + BookCorpus dataset \u00b6 If it is necessary to carry out the pretraining of BERT from scratch, a large dataset should be used. If necessary, we can download TFRecord dataset from google-research BERT and then make OFRecord dataset from it by methods in the article Loading and preparing OFRecord dataset . OneFlow BERT model converted from Tensorflow Model \u00b6 If you want to directly use the pretrained model for finetune tasks (such as the SQuAD shown below), you can consider downloading directly it from google-research BERT and then use the script convert_tf_ckpt_to_of.py we provided to convert it to OneFlow model. The conversion process is as follows: Firstly, download and unzip a BERT pretrained model of specified version, eg: uncased_L-12_H-768_A-12 . wget https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-12_H-768_A-12.zip unzip uncased_L-12_H-768_A-12.zip -d uncased_L-12_H-768_A-12 And then, run commands below: cd uncased_L-12_H-768_A-12/ cat > checkpoint <<ONEFLOW model_checkpoint_path: \"bert_model.ckpt\" all_model_checkpoint_paths: \"bert_model.ckpt\" ONEFLOW It will create a file named checkpoint in the directory and write content below into it: model_checkpoint_path: \"bert_model.ckpt\" all_model_checkpoint_paths: \"bert_model.ckpt\" Now that the TensorFlow model directory to be converted is ready, the hierarchy is: uncased_L-12_H-768_A-12 \u251c\u2500\u2500 bert_config.json \u251c\u2500\u2500 bert_model.ckpt.data-00000-of-00001 \u251c\u2500\u2500 bert_model.ckpt.index \u251c\u2500\u2500 checkpoint \u2514\u2500\u2500 vocab.txt And then we use convert_tf_ckpt_to_of.py to convert model to OneFlow format: python convert_tf_ckpt_to_of.py \\ --tf_checkpoint_path ./uncased_L-12_H-768_A-12 \\ --of_dump_path ./uncased_L-12_H-768_A-12-oneflow The above command saves the converted OneFlow format model in ./uncased_L-12_H-768_A-12-oneflow directory for later use(eg: SQuAD). Finetune task: SQuAD \u00b6 Extend to SQuAD model \u00b6 We only need to add a layer of output on the basis of BERT's backbone and modify the expression of loss. We can see the whole code in squad.py , and there are key modifications: def SQuADTrain (): #... backbone = bert_util.BertBackbone() #add a fully-connected layer base on BERT with flow . name_scope ( \"cls-squad\" ): final_hidden = backbone . sequence_output () final_hidden_matrix = flow . reshape ( final_hidden , [ - 1 , hidden_size ]) logits = bert_util . _FullyConnected ( final_hidden_matrix , hidden_size , units = 2 , weight_initializer = bert_util . CreateInitializer ( initializer_range ), name = 'output' ) logits = flow . reshape ( logits , [ - 1 , seq_length , 2 ]) start_logits = flow . slice ( logits , [ None , None , 0 ], [ None , None , 1 ]) end_logits = flow . slice ( logits , [ None , None , 1 ], [ None , None , 1 ]) #redefine the loss of SQuAD start_loss = _ComputeLoss ( start_logits , start_positions_blob , seq_length ) end_loss = _ComputeLoss ( end_logits , end_positions_blob , seq_length ) total_loss = 0.5 * ( start_loss + end_loss ) return total_loss We run the script below to start SQuAD training to get and save a initialized model. python ./run_squad.py\\ --gpu_num_per_node=1\\ --learning_rate=3e-5\\ --batch_size_per_device=2\\ --iter_num=50\\ --loss_print_every_n_iter=50\\ --seq_length=384\\ --max_predictions_per_seq=20\\ --num_hidden_layers=12\\ --num_attention_heads=12\\ --max_position_embeddings=512\\ --type_vocab_size=2\\ --vocab_size=30522\\ --attention_probs_dropout_prob=0.0\\ --hidden_dropout_prob=0.0\\ --hidden_size_per_head=64\\ --use_boxing_v2=True\\ --data_dir=./dataset/train-v1.1\\ --data_part_num=1\\ --log_dir=./bert_regresssioin_test/of\\ --model_save_dir=./bert_regresssioin_test/of\\ --warmup_batches 831\\ --save_last_snapshot True There will be a initialized model in the path ./bert_regresssioin_test/of/last_snapshot . We will merge it with pretrained BERT model and fintune it. Merge pretrained model into SQuAD \u00b6 SQuAD is extended from pretrained model of BERT. We should merge the pretrained model into SQuAD according to the method introduced in this article Loading and saving of model . cp -R ./bert_regresssioin_test/of/last_snapshot ./squadModel cp -R --remove-destination ./dataset/uncased_L-12_H-768_A-12_oneflow/* ./squadModel/ Problem on training times \u00b6 There is a folder named System-Train-TrainStep-xxx in the path of pretrained model folder and the file named \"out\" contains the count if iterations. The leraning rate changes dynamically with the count of iterations. In order to prevent training of finetuning from the saved iteration affecting, the binary data in the out file should be cleared to zero. cd System-Train-TrainStep-xxx xxd -r > out <<ONEFLOW 00000000: 0000 0000 0000 0000 ONEFLOW If you are using a pretrained model transferred from TensorFlow, you can skip this step. Start SQuAD training \u00b6 Start SQuAD training by running the script run_suqad.py with configuration below: use SQuAD model ./squadModel use SQuAD v1.1 as training set epoch = 3 ( iternum = 88641*3/(4*8) = 8310 ) learning rate = 3e-5 python ./run_squad.py\\ --gpu_num_per_node=4\\ --learning_rate=3e-5\\ --batch_size_per_device=8\\ --iter_num=8310\\ --loss_print_every_n_iter=50\\ --seq_length=384\\ --max_predictions_per_seq=20\\ --num_hidden_layers=12\\ --num_attention_heads=12\\ --max_position_embeddings=512\\ --type_vocab_size=2\\ --vocab_size=30522\\ --attention_probs_dropout_prob=0.0\\ --hidden_dropout_prob=0.0\\ --hidden_size_per_head=64\\ --use_boxing_v2=True\\ --data_dir=./dataset/train-v1.1\\ --data_part_num=8\\ --log_dir=./bert_regresssioin_test/of\\ --model_save_dir=./bert_regresssioin_test/of\\ --warmup_batches 831\\ --save_last_snapshot True\\ --model_load_dir=./squadModel Prediction and evaluatoin \u00b6 In order to generate Preidiction File , we should generate npy file fist. And then we use write_predictions function in google BERT's run_squad.py to convert it to json format. Run the script run_squad_predict.py to generate all_results.npy : python run_squad_predict.py \\ --gpu_num_per_node = 1 \\ --batch_size_per_device = 4 \\ --iter_num = 2709 \\ --seq_length = 384 \\ --max_predictions_per_seq = 20 \\ --num_hidden_layers = 12 \\ --num_attention_heads = 12 \\ --max_position_embeddings = 512 \\ --type_vocab_size = 2 \\ --vocab_size = 30522 \\ --attention_probs_dropout_prob = 0 .0 \\ --hidden_dropout_prob = 0 .0 \\ --hidden_size_per_head = 64 \\ --use_boxing_v2 = True \\ --data_part_num = 1 \\ --data_dir = ./dataset/dev-v1.1 \\ --log_dir = ./bert_regresssioin_test/of \\ --model_load_dir = path/to/squadModel \\ --warmup_batches 831 Attention: the model_load_dir should be the trained model of SQuAD. After we get the all_results.npy file, run the script npy2json.py in the repository of google bert (the version of TensorFlow should be v1). The npy2json.py we provide is modified from google bert's run_squad.py : python npy2json.py\\ --vocab_file=./dataset/vocab.txt \\ --bert_config_file=./dataset/bert_config.json \\ --do_train=False \\ --do_predict=True \\ --all_results_file=./all_results.npy \\ --predict_file=./dataset/dev-v1.1.json \\ --max_seq_length=384 \\ --doc_stride=128 \\ --output_dir=./squad_base/ Remember to set the all_results_file to the path of all_results.npy we obtained in the last step. We will get predictions.json after that which can be evaluated by evaluate-v1.1.py . python evaluate-v1.1.py \\ ./dataset/dev-v1.1.json \\ path/to/squad_base/predictions.json Distributed training \u00b6 As described when we introduce the command line options, we can start distributed training easily by adding the options node_num and node_list : python run_squad_predict.py \\ --gpu_num_per_node = 1 \\ --batch_size_per_device = 4 \\ --iter_num = 2709 \\ --seq_length = 384 \\ --max_predictions_per_seq = 20 \\ --num_hidden_layers = 12 \\ --num_attention_heads = 12 \\ --max_position_embeddings = 512 \\ --type_vocab_size = 2 \\ --vocab_size = 30522 \\ --attention_probs_dropout_prob = 0 .0 \\ --hidden_dropout_prob = 0 .0 \\ --hidden_size_per_head = 64 \\ --use_boxing_v2 = True \\ --data_part_num = 1 \\ --data_dir = ./dataset/dev-v1.1 \\ --log_dir = ./bert_regresssioin_test/of \\ --model_load_dir = path/to/squadModel \\ --warmup_batches 831 \\ --node_num = 2 \\ --node_list = \"192.168.1.12,192.168.1.14\"","title":"Bert"},{"location":"adv_examples/bert.html#summary","text":"BERT(Bidirectional Encoder Representations from Transformers) is a technique for NLP. In our case, we implement BERT based on the paper BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding using OneFlow.","title":"Summary"},{"location":"adv_examples/bert.html#model","text":"Model Hidden layers Hidden unit size Attention heads Feedforward filter size Max sequence length Parameters BERTBASE 12 encoder 768 12 4 x 768 512 110M There are commonly two steps in BERT: First, BERT pretrained model is obtained by pre-training; Then, on the basis of the obtained pretrained model, an additional layer of network is added and finetuned to get the downstream application.","title":"Model"},{"location":"adv_examples/bert.html#quickstart","text":"","title":"Quickstart"},{"location":"adv_examples/bert.html#get-dataset","text":"We provide OFRecord dataset and relevant other files , you can get and unzip it by running commands below: wget https://oneflow-static.oss-cn-beijing.aliyuncs.com/oneflow-tutorial-attachments/bert_squad_dataset.zip unzip bert_squad_dataset.zip The list of files is as follows: bert_config.json\u3001vocab.txt\uff1aFiles needed to generate \"prediction json\" file from google bert dev-v1.1/, dev-v1.1.json\uff1aSQuAD test set for evaluation part-0\uff1apre-trained training set contains 40 samples train-v1.1\uff1aSQuAD training set that has been coverted to OFRecords The above files will be used in the following pretraining tasks and squad finetune.","title":"Get dataset"},{"location":"adv_examples/bert.html#bert-pretrained","text":"Firstly, clone the OneFlow-Benchmark : git clone https://github.com/Oneflow-Inc/OneFlow-Benchmark.git cd OneFlow-Benchmark/LanguageModeling/BERT/ Then, with the following command, we can use our pretraining model and small sample set to start the BERT pre-training. python ./run_pretraining.py \\ --gpu_num_per_node = 1 \\ --learning_rate = 3e-5 \\ --batch_size_per_device = 1 \\ --iter_num = 3 \\ --loss_print_every_n_iter = 50 \\ --seq_length = 128 \\ --max_predictions_per_seq = 20 \\ --num_hidden_layers = 12 \\ --num_attention_heads = 12 \\ --max_position_embeddings = 512 \\ --type_vocab_size = 2 \\ --vocab_size = 30522 \\ --attention_probs_dropout_prob = 0 .0 \\ --hidden_dropout_prob = 0 .0 \\ --hidden_size_per_head = 64 \\ --use_boxing_v2 = True \\ --data_dir = ./dataset/ \\ --data_part_num = 1 \\ --log_dir = ./bert_regresssioin_test/of \\ --loss_print_every_n_iter = 5 \\ --model_save_dir = ./bert_regresssioin_test/of \\ --warmup_batches 831 \\ --save_last_snapshot True We will see the output similar to the following: ================================================================== Running bert: num_gpu_per_node = 1, num_nodes = 1. ================================================================== gpu_num_per_node = 1 node_num = 1 node_list = None learning_rate = 3e-05 weight_decay_rate = 0.01 batch_size_per_device = 1 iter_num = 20 warmup_batches = 831 log_every_n_iter = 1 data_dir = ./dataset/ data_part_num = 1 use_fp16 = None use_boxing_v2 = True loss_print_every_n_iter = 5 model_save_every_n_iter = 10000 model_save_dir = ./bert_regresssioin_test/of save_last_snapshot = True model_load_dir = None log_dir = ./bert_regresssioin_test/of seq_length = 128 max_predictions_per_seq = 20 num_hidden_layers = 12 num_attention_heads = 12 max_position_embeddings = 512 type_vocab_size = 2 vocab_size = 30522 attention_probs_dropout_prob = 0.0 hidden_dropout_prob = 0.0 hidden_size_per_head = 64 ------------------------------------------------------------------ Time stamp: 2020-07-06-19:09:29 I0706 19:09:29.605840639 34801 ev_epoll_linux.c:82] Use of signals is disabled. Epoll engine will not be used Init model on demand iter 4, total_loss: 11.032, mlm_loss: 10.281, nsp_loss: 0.751, speed: 33.086(sec/batch), 0.151(sentences/sec) iter 9, total_loss: 11.548, mlm_loss: 10.584, nsp_loss: 0.965, speed: 0.861(sec/batch), 5.806(sentences/sec) iter 14, total_loss: 10.697, mlm_loss: 10.249, nsp_loss: 0.448, speed: 0.915(sec/batch), 5.463(sentences/sec) iter 19, total_loss: 10.685, mlm_loss: 10.266, nsp_loss: 0.419, speed: 1.087(sec/batch), 4.602(sentences/sec) Saving model to ./bert_regresssioin_test/of/last_snapshot. ------------------------------------------------------------------ average speed: 0.556(sentences/sec) ------------------------------------------------------------------","title":"BERT pretrained"},{"location":"adv_examples/bert.html#detailed-description","text":"","title":"Detailed description"},{"location":"adv_examples/bert.html#scripts","text":"Files Description Belongs to pretrain.py\u3001bert.py Define the BERT model BERT run_pretraining.py Start BERT training. The user can configure the training environment and parameters of the BERT training through the command line parameters. The specific meanings of each option will be described in the script options below. BERT squad.py define SQuAD network SQuAD run_squad.py Run the SQuAD training SQuAD run_squad_predict.py Run the trained SQuAD model to predict. SQuAD npy2json.py Script required to overt OneFlow's prediction results to json. SQuAD convert_tf_ckpt_to_of.py Convert model from TensorFlow to OneFlow BERT/SQuAD","title":"Scripts"},{"location":"adv_examples/bert.html#options","text":"The script run_pretraining.py runs the pretraining and configured by command line options. You can run run_pretraining.py --help to see the options. The following is a detailed description of each option\uff1a gpu_num_per_node: count of devices on each node which must be consistent on each machine node_num: count of nodes, that is, the count of hosts in distributed system node_list: list of nodes. When thec count of nodes is more than one, we should spcifiy list of nodes by node_list. It's a string seperated by commans like --node_num=2 --node_list=\"192.168.1.12,192.168.1.14\" learning_rate: learning rate weight_decay_rate: decay rate of weight batch_size_per_device: batch size on each device iter_num ITER_NUM: count of iterations warmup_batches: batches of warmup, default to 10000 data_dir: path to OFRecord dataset data_part_num: number of files in the folder of OFRecord dataset use_fp16: use float16 or not use_boxing_v2: use boxing v2 or not loss_print_every_n_iter: print loss every n iterations model_save_every_n_iter: save the model every n iterations model_save_dir: path to save the model save_last_snapshot: whether save the model when training is finished model_load_dir: path to load the model log_dir LOG_DIR: specify the path of log seq_length: length of sequence, default to 512 max_predictions_per_seq: default to 80 num_hidden_layers: number of hidden layers, defaul to 24 num_attention_heads: number of attentoion heads\uff0cdefault to 16","title":"Options"},{"location":"adv_examples/bert.html#use-wikipedia-bookcorpus-dataset","text":"If it is necessary to carry out the pretraining of BERT from scratch, a large dataset should be used. If necessary, we can download TFRecord dataset from google-research BERT and then make OFRecord dataset from it by methods in the article Loading and preparing OFRecord dataset .","title":"Use Wikipedia + BookCorpus dataset"},{"location":"adv_examples/bert.html#oneflow-bert-model-converted-from-tensorflow-model","text":"If you want to directly use the pretrained model for finetune tasks (such as the SQuAD shown below), you can consider downloading directly it from google-research BERT and then use the script convert_tf_ckpt_to_of.py we provided to convert it to OneFlow model. The conversion process is as follows: Firstly, download and unzip a BERT pretrained model of specified version, eg: uncased_L-12_H-768_A-12 . wget https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-12_H-768_A-12.zip unzip uncased_L-12_H-768_A-12.zip -d uncased_L-12_H-768_A-12 And then, run commands below: cd uncased_L-12_H-768_A-12/ cat > checkpoint <<ONEFLOW model_checkpoint_path: \"bert_model.ckpt\" all_model_checkpoint_paths: \"bert_model.ckpt\" ONEFLOW It will create a file named checkpoint in the directory and write content below into it: model_checkpoint_path: \"bert_model.ckpt\" all_model_checkpoint_paths: \"bert_model.ckpt\" Now that the TensorFlow model directory to be converted is ready, the hierarchy is: uncased_L-12_H-768_A-12 \u251c\u2500\u2500 bert_config.json \u251c\u2500\u2500 bert_model.ckpt.data-00000-of-00001 \u251c\u2500\u2500 bert_model.ckpt.index \u251c\u2500\u2500 checkpoint \u2514\u2500\u2500 vocab.txt And then we use convert_tf_ckpt_to_of.py to convert model to OneFlow format: python convert_tf_ckpt_to_of.py \\ --tf_checkpoint_path ./uncased_L-12_H-768_A-12 \\ --of_dump_path ./uncased_L-12_H-768_A-12-oneflow The above command saves the converted OneFlow format model in ./uncased_L-12_H-768_A-12-oneflow directory for later use(eg: SQuAD).","title":"OneFlow BERT model converted from Tensorflow Model"},{"location":"adv_examples/bert.html#finetune-task-squad","text":"","title":"Finetune task: SQuAD"},{"location":"adv_examples/bert.html#extend-to-squad-model","text":"We only need to add a layer of output on the basis of BERT's backbone and modify the expression of loss. We can see the whole code in squad.py , and there are key modifications: def SQuADTrain (): #... backbone = bert_util.BertBackbone() #add a fully-connected layer base on BERT with flow . name_scope ( \"cls-squad\" ): final_hidden = backbone . sequence_output () final_hidden_matrix = flow . reshape ( final_hidden , [ - 1 , hidden_size ]) logits = bert_util . _FullyConnected ( final_hidden_matrix , hidden_size , units = 2 , weight_initializer = bert_util . CreateInitializer ( initializer_range ), name = 'output' ) logits = flow . reshape ( logits , [ - 1 , seq_length , 2 ]) start_logits = flow . slice ( logits , [ None , None , 0 ], [ None , None , 1 ]) end_logits = flow . slice ( logits , [ None , None , 1 ], [ None , None , 1 ]) #redefine the loss of SQuAD start_loss = _ComputeLoss ( start_logits , start_positions_blob , seq_length ) end_loss = _ComputeLoss ( end_logits , end_positions_blob , seq_length ) total_loss = 0.5 * ( start_loss + end_loss ) return total_loss We run the script below to start SQuAD training to get and save a initialized model. python ./run_squad.py\\ --gpu_num_per_node=1\\ --learning_rate=3e-5\\ --batch_size_per_device=2\\ --iter_num=50\\ --loss_print_every_n_iter=50\\ --seq_length=384\\ --max_predictions_per_seq=20\\ --num_hidden_layers=12\\ --num_attention_heads=12\\ --max_position_embeddings=512\\ --type_vocab_size=2\\ --vocab_size=30522\\ --attention_probs_dropout_prob=0.0\\ --hidden_dropout_prob=0.0\\ --hidden_size_per_head=64\\ --use_boxing_v2=True\\ --data_dir=./dataset/train-v1.1\\ --data_part_num=1\\ --log_dir=./bert_regresssioin_test/of\\ --model_save_dir=./bert_regresssioin_test/of\\ --warmup_batches 831\\ --save_last_snapshot True There will be a initialized model in the path ./bert_regresssioin_test/of/last_snapshot . We will merge it with pretrained BERT model and fintune it.","title":"Extend to SQuAD model"},{"location":"adv_examples/bert.html#merge-pretrained-model-into-squad","text":"SQuAD is extended from pretrained model of BERT. We should merge the pretrained model into SQuAD according to the method introduced in this article Loading and saving of model . cp -R ./bert_regresssioin_test/of/last_snapshot ./squadModel cp -R --remove-destination ./dataset/uncased_L-12_H-768_A-12_oneflow/* ./squadModel/","title":"Merge pretrained model into SQuAD"},{"location":"adv_examples/bert.html#problem-on-training-times","text":"There is a folder named System-Train-TrainStep-xxx in the path of pretrained model folder and the file named \"out\" contains the count if iterations. The leraning rate changes dynamically with the count of iterations. In order to prevent training of finetuning from the saved iteration affecting, the binary data in the out file should be cleared to zero. cd System-Train-TrainStep-xxx xxd -r > out <<ONEFLOW 00000000: 0000 0000 0000 0000 ONEFLOW If you are using a pretrained model transferred from TensorFlow, you can skip this step.","title":"Problem on training times"},{"location":"adv_examples/bert.html#start-squad-training","text":"Start SQuAD training by running the script run_suqad.py with configuration below: use SQuAD model ./squadModel use SQuAD v1.1 as training set epoch = 3 ( iternum = 88641*3/(4*8) = 8310 ) learning rate = 3e-5 python ./run_squad.py\\ --gpu_num_per_node=4\\ --learning_rate=3e-5\\ --batch_size_per_device=8\\ --iter_num=8310\\ --loss_print_every_n_iter=50\\ --seq_length=384\\ --max_predictions_per_seq=20\\ --num_hidden_layers=12\\ --num_attention_heads=12\\ --max_position_embeddings=512\\ --type_vocab_size=2\\ --vocab_size=30522\\ --attention_probs_dropout_prob=0.0\\ --hidden_dropout_prob=0.0\\ --hidden_size_per_head=64\\ --use_boxing_v2=True\\ --data_dir=./dataset/train-v1.1\\ --data_part_num=8\\ --log_dir=./bert_regresssioin_test/of\\ --model_save_dir=./bert_regresssioin_test/of\\ --warmup_batches 831\\ --save_last_snapshot True\\ --model_load_dir=./squadModel","title":"Start SQuAD training"},{"location":"adv_examples/bert.html#prediction-and-evaluatoin","text":"In order to generate Preidiction File , we should generate npy file fist. And then we use write_predictions function in google BERT's run_squad.py to convert it to json format. Run the script run_squad_predict.py to generate all_results.npy : python run_squad_predict.py \\ --gpu_num_per_node = 1 \\ --batch_size_per_device = 4 \\ --iter_num = 2709 \\ --seq_length = 384 \\ --max_predictions_per_seq = 20 \\ --num_hidden_layers = 12 \\ --num_attention_heads = 12 \\ --max_position_embeddings = 512 \\ --type_vocab_size = 2 \\ --vocab_size = 30522 \\ --attention_probs_dropout_prob = 0 .0 \\ --hidden_dropout_prob = 0 .0 \\ --hidden_size_per_head = 64 \\ --use_boxing_v2 = True \\ --data_part_num = 1 \\ --data_dir = ./dataset/dev-v1.1 \\ --log_dir = ./bert_regresssioin_test/of \\ --model_load_dir = path/to/squadModel \\ --warmup_batches 831 Attention: the model_load_dir should be the trained model of SQuAD. After we get the all_results.npy file, run the script npy2json.py in the repository of google bert (the version of TensorFlow should be v1). The npy2json.py we provide is modified from google bert's run_squad.py : python npy2json.py\\ --vocab_file=./dataset/vocab.txt \\ --bert_config_file=./dataset/bert_config.json \\ --do_train=False \\ --do_predict=True \\ --all_results_file=./all_results.npy \\ --predict_file=./dataset/dev-v1.1.json \\ --max_seq_length=384 \\ --doc_stride=128 \\ --output_dir=./squad_base/ Remember to set the all_results_file to the path of all_results.npy we obtained in the last step. We will get predictions.json after that which can be evaluated by evaluate-v1.1.py . python evaluate-v1.1.py \\ ./dataset/dev-v1.1.json \\ path/to/squad_base/predictions.json","title":"Prediction and evaluatoin"},{"location":"adv_examples/bert.html#distributed-training","text":"As described when we introduce the command line options, we can start distributed training easily by adding the options node_num and node_list : python run_squad_predict.py \\ --gpu_num_per_node = 1 \\ --batch_size_per_device = 4 \\ --iter_num = 2709 \\ --seq_length = 384 \\ --max_predictions_per_seq = 20 \\ --num_hidden_layers = 12 \\ --num_attention_heads = 12 \\ --max_position_embeddings = 512 \\ --type_vocab_size = 2 \\ --vocab_size = 30522 \\ --attention_probs_dropout_prob = 0 .0 \\ --hidden_dropout_prob = 0 .0 \\ --hidden_size_per_head = 64 \\ --use_boxing_v2 = True \\ --data_part_num = 1 \\ --data_dir = ./dataset/dev-v1.1 \\ --log_dir = ./bert_regresssioin_test/of \\ --model_load_dir = path/to/squadModel \\ --warmup_batches 831 \\ --node_num = 2 \\ --node_list = \"192.168.1.12,192.168.1.14\"","title":"Distributed training"},{"location":"adv_examples/dcgan.html","text":"DCGAN tutorial \u00b6 \u7b80\u4ecb \u00b6 \u751f\u6210\u5bf9\u6297\u7f51\u7edc(GANs)\u5c5e\u4e8e\u4e00\u79cd\u751f\u6210\u7f51\u7edc\uff0c\u5b83\u901a\u8fc7\u4e24\u4e2a\u7f51\u7edc\u7684\u76f8\u4e92\u535a\u5f08\u7684\u65b9\u5f0f\u6765\u5b66\u4e60\u7279\u5b9a\u7684\u6570\u636e\u5206\u5e03\u3002\u800cDCGAN\u5219\u662f\u4e00\u79cd\u57fa\u4e8e\u5377\u79ef/\u53cd\u5377\u79ef\u8fd0\u7b97\u7684\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff0c\u88ab\u5e7f\u6cdb\u5e94\u7528\u4e8e\u56fe\u50cf\u751f\u6210\u9886\u57df\u800cDCGAN\u5219\u662f\u4e00\u79cd\u57fa\u4e8e\u5377\u79ef/\u53cd\u5377\u79ef\u8fd0\u7b97\u7684\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff0c\u88ab\u5e7f\u6cdb\u5e94\u7528\u4e8e\u56fe\u50cf\u751f\u6210\u9886\u57df \u672c\u4f8b\u7a0b\u5c06\u4e3b\u8981\u6f14\u793a\u5982\u4f55\u5728Oneflow\u4e2d\u8fd0\u884cDCGAN\u7f51\u7edc\uff0c\u800c\u4e0d\u91cd\u70b9\u8ba8\u8bba\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u7684\u539f\u7406\u548c\u7ec6\u8282\u3002\u5982\u679c\u611f\u5174\u8da3\u7684\u8bdd\uff0c\u53ef\u4ee5\u53c2\u8003\uff1a\u5982\u679c\u611f\u5174\u8da3\u7684\u8bdd\uff0c\u53ef\u4ee5\u53c2\u8003\uff1a Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks NLPS 2016 tutorial:generative adversarial networks \u5bf9\u9f50\u6d4b\u8bd5 \u00b6 \u672c\u4f8b\u7a0b\u7684\u6838\u5fc3\u4ee3\u7801\u5728 dcgan.py \u6587\u4ef6\u4e2d\uff0c\u5176\u4e2d\u7684\u6a21\u578b\u7ed3\u6784\u548c\u53c2\u6570\u53c2\u8003\u4e86tensorflow\u7684 \u5b98\u65b9\u793a\u4f8b \u901a\u8fc7\u4ee5\u4e0b\u4ee3\u7801\uff0c\u53ef\u4ee5\u8fd0\u884c\u4e00\u4e2a\u7b80\u5355\u7684\u5bf9\u9f50\u6d4b\u8bd5\uff0c\u4fdd\u8bc1oneflow\u7684\u6a21\u578b\u7ed3\u679c\u4e0etensorflow\u7684\u7ed3\u679c\u662f\u4e00\u81f4\u7684 dcgan = DCGAN () dcgan . compare_with_tensorflow () \u6570\u636e\u96c6\u51c6\u5907 \u00b6 \u4f8b\u7a0b\u63d0\u4f9b\u4e86\u6570\u636e\u96c6\u4e0b\u8f7d\u811a\u672c\uff0c\u8fd0\u884c download.py \u4e0b\u8f7dmnist\u6570\u636e\u96c6\uff0c \u6570\u636e\u96c6\u9ed8\u8ba4\u4fdd\u5b58\u5728 ./data/minst \u76ee\u5f55\u4e2d python download.py mnist \u8bad\u7ec3 \u00b6 \u5728\u51c6\u5907\u597d\u6570\u636e\u96c6\u540e\uff0c\u53ef\u901a\u8fc7DCGAN\u5b9e\u4f8b\u7684 train \u65b9\u6cd5\u8fdb\u884cDCGAN\u7684\u8bad\u7ec3 dcgan . train ( epochs = 2 ) \u8bad\u7ec3\u5c06\u6bcf\u9694 self.eval_interval \u4e2abatch\u8f93\u51fa\u751f\u6210\u7684\u56fe\u50cf \u5bfc\u51fa\u52a8\u56fe \u00b6 \u518d\u5b8c\u6210\u8bad\u7ec3\u540e\uff0c\u53ef\u4ee5\u901a\u8fc7DCGAN\u5b9e\u4f8b\u7684 save_to_gif \u65b9\u6cd5\u5c06\u56fe\u50cf\u5bfc\u51fa\u4e3a\u52a8\u56fe dcgan . save_to_gif ()","title":"DCGAN tutorial"},{"location":"adv_examples/dcgan.html#dcgan-tutorial","text":"","title":"DCGAN tutorial"},{"location":"adv_examples/dcgan.html#_1","text":"\u751f\u6210\u5bf9\u6297\u7f51\u7edc(GANs)\u5c5e\u4e8e\u4e00\u79cd\u751f\u6210\u7f51\u7edc\uff0c\u5b83\u901a\u8fc7\u4e24\u4e2a\u7f51\u7edc\u7684\u76f8\u4e92\u535a\u5f08\u7684\u65b9\u5f0f\u6765\u5b66\u4e60\u7279\u5b9a\u7684\u6570\u636e\u5206\u5e03\u3002\u800cDCGAN\u5219\u662f\u4e00\u79cd\u57fa\u4e8e\u5377\u79ef/\u53cd\u5377\u79ef\u8fd0\u7b97\u7684\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff0c\u88ab\u5e7f\u6cdb\u5e94\u7528\u4e8e\u56fe\u50cf\u751f\u6210\u9886\u57df\u800cDCGAN\u5219\u662f\u4e00\u79cd\u57fa\u4e8e\u5377\u79ef/\u53cd\u5377\u79ef\u8fd0\u7b97\u7684\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff0c\u88ab\u5e7f\u6cdb\u5e94\u7528\u4e8e\u56fe\u50cf\u751f\u6210\u9886\u57df \u672c\u4f8b\u7a0b\u5c06\u4e3b\u8981\u6f14\u793a\u5982\u4f55\u5728Oneflow\u4e2d\u8fd0\u884cDCGAN\u7f51\u7edc\uff0c\u800c\u4e0d\u91cd\u70b9\u8ba8\u8bba\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u7684\u539f\u7406\u548c\u7ec6\u8282\u3002\u5982\u679c\u611f\u5174\u8da3\u7684\u8bdd\uff0c\u53ef\u4ee5\u53c2\u8003\uff1a\u5982\u679c\u611f\u5174\u8da3\u7684\u8bdd\uff0c\u53ef\u4ee5\u53c2\u8003\uff1a Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks NLPS 2016 tutorial:generative adversarial networks","title":"\u7b80\u4ecb"},{"location":"adv_examples/dcgan.html#_2","text":"\u672c\u4f8b\u7a0b\u7684\u6838\u5fc3\u4ee3\u7801\u5728 dcgan.py \u6587\u4ef6\u4e2d\uff0c\u5176\u4e2d\u7684\u6a21\u578b\u7ed3\u6784\u548c\u53c2\u6570\u53c2\u8003\u4e86tensorflow\u7684 \u5b98\u65b9\u793a\u4f8b \u901a\u8fc7\u4ee5\u4e0b\u4ee3\u7801\uff0c\u53ef\u4ee5\u8fd0\u884c\u4e00\u4e2a\u7b80\u5355\u7684\u5bf9\u9f50\u6d4b\u8bd5\uff0c\u4fdd\u8bc1oneflow\u7684\u6a21\u578b\u7ed3\u679c\u4e0etensorflow\u7684\u7ed3\u679c\u662f\u4e00\u81f4\u7684 dcgan = DCGAN () dcgan . compare_with_tensorflow ()","title":"\u5bf9\u9f50\u6d4b\u8bd5"},{"location":"adv_examples/dcgan.html#_3","text":"\u4f8b\u7a0b\u63d0\u4f9b\u4e86\u6570\u636e\u96c6\u4e0b\u8f7d\u811a\u672c\uff0c\u8fd0\u884c download.py \u4e0b\u8f7dmnist\u6570\u636e\u96c6\uff0c \u6570\u636e\u96c6\u9ed8\u8ba4\u4fdd\u5b58\u5728 ./data/minst \u76ee\u5f55\u4e2d python download.py mnist","title":"\u6570\u636e\u96c6\u51c6\u5907"},{"location":"adv_examples/dcgan.html#_4","text":"\u5728\u51c6\u5907\u597d\u6570\u636e\u96c6\u540e\uff0c\u53ef\u901a\u8fc7DCGAN\u5b9e\u4f8b\u7684 train \u65b9\u6cd5\u8fdb\u884cDCGAN\u7684\u8bad\u7ec3 dcgan . train ( epochs = 2 ) \u8bad\u7ec3\u5c06\u6bcf\u9694 self.eval_interval \u4e2abatch\u8f93\u51fa\u751f\u6210\u7684\u56fe\u50cf","title":"\u8bad\u7ec3"},{"location":"adv_examples/dcgan.html#_5","text":"\u518d\u5b8c\u6210\u8bad\u7ec3\u540e\uff0c\u53ef\u4ee5\u901a\u8fc7DCGAN\u5b9e\u4f8b\u7684 save_to_gif \u65b9\u6cd5\u5c06\u56fe\u50cf\u5bfc\u51fa\u4e3a\u52a8\u56fe dcgan . save_to_gif ()","title":"\u5bfc\u51fa\u52a8\u56fe"},{"location":"adv_examples/resnet.html","text":"Introduction \u00b6 Image classification and CNN \u00b6 Image classification is an image processing method that divided different features reflected in image information into different categories of targets. It is the basis of other tasks in computer vision, such as detection, semantic segmentation, face recognition and other high-level visual tasks. ImageNet Large-scale Visual Recognition Challenge (ILSVRC), often called ImageNet copetition, including image classification, object orientation, object detection and other tasks. It is one of the most important competition to promote the development of computer vision. In the 2012 ImageNet competition, deep convolution network Alexnet was born. With a top-5 accuracy rate more than 10% higher than the second place, it won the champion of 2012 ImageNet competition. Since then, the deep learning method represented by CNN(Convolutional neural network) has been applied in the field of computer vision. More and deeper CNN networks have been proposed, such as VGGNet, the champion of 2014 ImageNet competition, ResNet, the champion of 2015 ImageNet competition. ResNet \u00b6 ResNet is the champion of 2015 competition. At present, compared with traditional machine learning classification algorithm, ResNet has achieved excellent results. After that, a large number of detection, segmentation, classification and other tasks are completed on the base of ResNet. In OneFlow-Benchmark repository, we provide OneFlow implementation of ResNet50 v1.5. After 90 epochs of training on ImageNet-2012 dataset, the accuracy of evaluation can reach 77.318% (Top 1), 93.622% (Top 5). For more detailed network parameter alignment, you can refer to OneFlow-Benchmark's cnns part. Some notes on ResNet50 v1.5 ResNet50 v1.5 is an improved version of the original ResNet50 v1 , compared with the original model, the accuracy improve slightly Top1(~0.5%), you can refer to there for more details. Next, we take the above ResNet50 network as an example to show how to use OneFlow to train and predict step by step. The main contents include\uff1a Preparation The installation and preparation of project Quick start Predict / Inference Train / Predict Evaluation More details Distributed training Hybrid precision training and prediction Advanced Parameter alignment Preparing dataset (ImageNet 2012) Convert OneFlow model to ONNX model Requirements \u00b6 Don't worry, it is easy to use OneFlow. You can start OneFlow's image recognition journey with three steps as follow. Install OneFlow\uff0cyou can refer to OneFlow project home page to finish installation. Clone / Download OneFlow-Benchmark repository. git clone git@github.com:Oneflow-Inc/OneFlow-Benchmark.git cd OneFlow-Benchmark/Classification/cnns Preparing Dataset (optional) Use synthetic virtual dataset directly. Download the ImageNet 2012 mini-dataset we created and unzip it into the data directory Or: Make a complete OFRecord format ImageNet dataset (see the advanced section below) We provide general scripts: train.sh and inference.sh , which are applicable to the training, validation and inference of all cnn networks in this repository. You can train different models and dataset by setting parameters in scripts. Some notes on model By default, we use ResNet50, you can also assign other model by setting the --model parameter. Such as: --model=\"resnet50\" , --model=\"vgg\" and so on. Description of dataset 1) To get reader quickly start, we provide synthetic virtual dataset, which refers to data is generated directly in memory as a random source of neural network. 2) At the same time, we provide a mini-dataset. You can download and unzip it into data directory, you can start training quickly. After getting familiar with the process, readers can refer to the making dataset part to make a complete ImageNet 2012 dataset. 3) Using OFRecord dataset can improve the efficientcy of data loading (But this is not necessary, refer to Data Input , OneFlow supports loading numpy data directly). Quick Start \u00b6 So, let's start OneFlow's image classification journey ! First, switch to the directory: cd OneFlow-Benchmark/Classification/cnns Pretrained Model \u00b6 resnet50 \u00b6 resnet50_v1.5_model (validation accuracy: 77.318% top1\uff0c93.622% top5 ) Predict / Inference \u00b6 After downloading pretrained model, unzip it and put it into the current directory. Then execute: sh inference.sh This script will call the model to classify the goldfish picture: The prediction is successful if the following is output. data/fish.jpg 0.87059885 goldfish, Carassius auratus As you can see, model judge this picture with 87.05% probability is goldfish. Train & Validation \u00b6 Training model is also easy as we just need to execute: sh train.sh You can start training model and you will see the follow output Loading synthetic data. Loading synthetic data. Saving model to ./output/snapshots/model_save-20200723124215/snapshot_initial_model. Init model on demand. train: epoch 0, iter 10, loss: 7.197278, top_1: 0.000000, top_k: 0.000000, samples/s: 61.569 train: epoch 0, iter 20, loss: 6.177684, top_1: 0.000000, top_k: 0.000000, samples/s: 122.555 Saving model to ./output/snapshots/model_save-20200723124215/snapshot_epoch_0. train: epoch 0, iter 30, loss: 3.988656, top_1: 0.525000, top_k: 0.812500, samples/s: 120.337 train: epoch 1, iter 10, loss: 1.185733, top_1: 1.000000, top_k: 1.000000, samples/s: 80.705 train: epoch 1, iter 20, loss: 1.042017, top_1: 1.000000, top_k: 1.000000, samples/s: 118.478 Saving model to ./output/snapshots/model_save-20200723124215/snapshot_epoch_1. ... To facilitate running the demonstration, we use synthetic virtual dataset by default so that you can quickly see the model in action. Also, you can use mini-dataset , after downloading it and unzip it in data directory, and then modify the training script as follows: rm -rf core.* rm -rf ./output/snapshots/* DATA_ROOT=data/imagenet/ofrecord python3 of_cnn_train_val.py \\ --train_data_dir=$DATA_ROOT/train \\ --num_examples=50 \\ --train_data_part_num=1 \\ --val_data_dir=$DATA_ROOT/validation \\ --num_val_examples=50 \\ --val_data_part_num=1 \\ --num_nodes=1 \\ --gpu_num_per_node=1 \\ --model_update=\"momentum\" \\ --learning_rate=0.001 \\ --loss_print_every_n_iter=1 \\ --batch_size_per_device=16 \\ --val_batch_size_per_device=10 \\ --num_epoch=10 \\ --model=\"resnet50\" Running this script, we will train a classfication model on the mini-ImageNet dataset with only 50 goldfish images. We can use this model to classify the goldfish image. Don't worry, if you need to train model on the complete ImageNet2012 dataset, please refer to OneFlow-Benchmark repository. Evaluate \u00b6 You can evaluate the accuracy of the Resnet50 model using either your own trained model or the resnet50_v1.5_model (unzip it and put it in current directory) provided by us. Run this script: sh evaluate.sh The accuracy of the trained model on validation dataset with 50000 images can be obtained: Time stamp: 2020-07-27-09:28:28 Restoring model from resnet_v15_of_best_model_val_top1_77318. I0727 09:28:28.773988162 8411 ev_epoll_linux.c:82] Use of signals is disabled. Epoll engine will not be used Loading data from /dataset/ImageNet/ofrecord/validation validation: epoch 0, iter 195, top_1: 0.773277, top_k: 0.936058, samples/s: 1578.325 validation: epoch 0, iter 195, top_1: 0.773237, top_k: 0.936078, samples/s: 1692.303 validation: epoch 0, iter 195, top_1: 0.773297, top_k: 0.936018, samples/s: 1686.896 Before executing sh evaluate.sh , make sure you have prepared the validation dataset of ImageNet 2012. Please refer to OneFlow-Benchmark repository to learn how to make validation dataset. From the evaluation results of the three rounds, out model has achieved 77.32+% Top1 accuracy. Finally, congratulations! You complete the training / validating, inference and evaluation of ResNet model on ImageNet dataset. Applause for yourself! Details \u00b6 Distributed training \u00b6 Simple and easy-to-use distributed training is one of OneFlow's main features OneFlow is designed to support efficient distributed training natively. Especially for distributed data parallelism, user do not have to worry about how to divide and synchronize the data when the algorithm expands from single machine to multiple machines. That is to say, in OneFlow, User only need to write algorithm from the view of single machine, and the code automatically has the ability of distributed training. How to configure and run distributed training? \u00b6 We still use the code shown in the \"Quick Start\", in train.sh , the distributed configuration is easily accomplished by specifying the number of nodes (machines) with --num_nodes , the IP address of the nodes with --node_ips , and the number of devices to be used on each node with --gpu_num_per_node . For example, we want to do distributed training on 2 machines with 8 devices, configure it like this: # train.sh python3 of_cnn_train_val.py \\ --num_nodes=2 \\ --node_ips=\"192.168.1.1, 192.168.1.2\" --gpu_num_per_node=4 \\ ... --model=\"resnet50\" Then execute the following script on the two machines at the same time: ./train.sh After the program starts, you can see through the command watch -n 0.1 nvidia-smi that both machines' devices start working. After a while, the output is printed on the screen of the first machine set by --node_ips . Hybrid precision training and predicting \u00b6 Currently, OneFlow supports float16/float32 hybrid precision training. During training, the model parameters are trained using float16 while retaining float32 as the gradient update and calculation process. Since the storage of parameters is halved, the training speed will be improved. By turning on the hybrid precision training mode in OneFlow, ResNet50's training speed can theoretically reach 1.7 times of acceleration. How to turn on the hybrid precision training mode\uff1f \u00b6 Just add the parameter --use_fp16=True in the train.sh script. Hybrid precision model \u00b6 We provide a hybrid precision model after training 90 epochs on ImageNet2012 dataset, its Top_1 accuracy: 77.33%. You can download and use it directly: resnet50_v15_fp16 Advanced \u00b6 Parameters alignment \u00b6 OneFlow's ResNet50 implementation is aligned with Nvidia's Mxnet edition. We've made careful and almost identical alignment from the learning rate, optimizer, image augmentation to finer per-layer network configuration, bias, weight initialization, and more. The detailed parameters alignment please refer to OneFlow-Benchmark repository. Preparing dataset \u00b6 Introduction of image classification dataset \u00b6 The public dataset used for image classification are CIFAR, ImageNet, etc. These datasets provide original images in JPEG format. CIFAR Hinton's student Alex Krizhevsky and Ilya Sutskever collated a small dataset to classify pervasive objects. It includes CIFAR-10 and CIFAR-100 ImageNet ImageNet dataset are generally referred to as the dataset used in large-scale visual recognition challenge (ILSVRC) between 2010-2017. The ImageNet data has changed slightly since 2010. The commonly used ImageNet-2012 dataset includes 1000 categories, its training dataset contains 1281167 pictures, ranging from 732 to 1300 per category. The validation dataset contains 50000 pictures, with an average of 50 pictures per category. For the complete process of preparing ImageNet-2012 dataset, please refer to README in the tools directory. Convert OneFlow model to ONNX model \u00b6 Introduction \u00b6 ONNX (Open Neural Network Exchange) is a widely used neural network intermediate format. With the ONNX format, the OneFlow model can be used by many serving framework (like OpenVINO, ONNEX Runtime and some mobile framework: ncnn, tnn, TEgine, etc). In this section, we will introduce how to convert the trained ResNet50 v1.5 model to ONNX model and evaluate it. Quick Start \u00b6 We provide complete code: resnet_to_onnx.py , it can help you complete the transformation and testing of the model. **Step1: ** Download the pretrain model: resnet50_v1.5_model , unzip it and put it into current directory **Step2: ** Execute python3 resnet_to_onnx.py This code will complete the transformation of the OneFlow model -> ONNX model, and then use the ONNX Runtime to load the transformed model to test the individual images. The test picture is as follows: \u200b \u56fe\u7247\u6765\u6e90\uff1a https://en.wikipedia.org/wiki/Tiger Output\uff1a Convert to onnx success ! >> onnx / model / resnet_v15_of_best_model_val_top1_77318 . onnx data / tiger . jpg Are the results equal ? Yes Class : tiger , Panthera tigris ; score : 0.8112028241157532 How to generate ONNX model \u00b6 We have introduced how to convert OneFlow's ResNet model to ONNX model and give an example of using the onnx runtime to make predictions in above example. Similarly, you can follow the steps to complete the transformation of your training ResNet model or other models. Step1: Save the model's weight First you should specify the OneFlow model path, and then specify the transformed ONNX model storage path, like the following example. \u9996\u5148\u6307\u5b9a\u5f85\u8f6c\u6362\u7684OneFlow\u6a21\u578b\u8def\u5f84\uff0c\u7136\u540e\u6307\u5b9a\u8f6c\u6362\u540e\u7684ONNX\u6a21\u578b\u5b58\u653e\u8def\u5f84\uff0c\u4f8b\u5982\u793a\u4f8b\u4e2d\uff1a #set up your model path flow_weights_path = 'resnet_v15_of_best_model_val_top1_77318' onnx_model_dir = 'onnx/model' Step2: Create a new job function for inference Then, we create a new job function for inference, which only contains the network structure, except the operator to read the OFRecord, and accepts the form of numpy array input. You can refer to the InferenceNet in resnet_to_onnx.py . Step3: Call flow.onnx.export method In the following code, we call the oneflow_to_onnx() method, this method includes the core model transformation method: flow.onnx.export() . flow.onnx.export will obtain ONNX model from OneFlow network, its first parameter is the job function used to infer. The second parameter is OneFlow model path, the third parameter is the save path of ONNX model. onnx_model = oneflow_to_onnx ( InferenceNet , flow_weights_path , onnx_model_dir , external_data = False ) Evaluate the correctness of ONNX model \u00b6 After the ONNX model is generated, we can use ONNX model by ONNX Runtime to verify that the OneFlow model and the ONNX model give the same results with the same inputs. The corresponding code is check_equality in resnet_to_onnx.py .","title":"Resnet"},{"location":"adv_examples/resnet.html#introduction","text":"","title":"Introduction"},{"location":"adv_examples/resnet.html#image-classification-and-cnn","text":"Image classification is an image processing method that divided different features reflected in image information into different categories of targets. It is the basis of other tasks in computer vision, such as detection, semantic segmentation, face recognition and other high-level visual tasks. ImageNet Large-scale Visual Recognition Challenge (ILSVRC), often called ImageNet copetition, including image classification, object orientation, object detection and other tasks. It is one of the most important competition to promote the development of computer vision. In the 2012 ImageNet competition, deep convolution network Alexnet was born. With a top-5 accuracy rate more than 10% higher than the second place, it won the champion of 2012 ImageNet competition. Since then, the deep learning method represented by CNN(Convolutional neural network) has been applied in the field of computer vision. More and deeper CNN networks have been proposed, such as VGGNet, the champion of 2014 ImageNet competition, ResNet, the champion of 2015 ImageNet competition.","title":"Image classification and CNN"},{"location":"adv_examples/resnet.html#resnet","text":"ResNet is the champion of 2015 competition. At present, compared with traditional machine learning classification algorithm, ResNet has achieved excellent results. After that, a large number of detection, segmentation, classification and other tasks are completed on the base of ResNet. In OneFlow-Benchmark repository, we provide OneFlow implementation of ResNet50 v1.5. After 90 epochs of training on ImageNet-2012 dataset, the accuracy of evaluation can reach 77.318% (Top 1), 93.622% (Top 5). For more detailed network parameter alignment, you can refer to OneFlow-Benchmark's cnns part. Some notes on ResNet50 v1.5 ResNet50 v1.5 is an improved version of the original ResNet50 v1 , compared with the original model, the accuracy improve slightly Top1(~0.5%), you can refer to there for more details. Next, we take the above ResNet50 network as an example to show how to use OneFlow to train and predict step by step. The main contents include\uff1a Preparation The installation and preparation of project Quick start Predict / Inference Train / Predict Evaluation More details Distributed training Hybrid precision training and prediction Advanced Parameter alignment Preparing dataset (ImageNet 2012) Convert OneFlow model to ONNX model","title":"ResNet"},{"location":"adv_examples/resnet.html#requirements","text":"Don't worry, it is easy to use OneFlow. You can start OneFlow's image recognition journey with three steps as follow. Install OneFlow\uff0cyou can refer to OneFlow project home page to finish installation. Clone / Download OneFlow-Benchmark repository. git clone git@github.com:Oneflow-Inc/OneFlow-Benchmark.git cd OneFlow-Benchmark/Classification/cnns Preparing Dataset (optional) Use synthetic virtual dataset directly. Download the ImageNet 2012 mini-dataset we created and unzip it into the data directory Or: Make a complete OFRecord format ImageNet dataset (see the advanced section below) We provide general scripts: train.sh and inference.sh , which are applicable to the training, validation and inference of all cnn networks in this repository. You can train different models and dataset by setting parameters in scripts. Some notes on model By default, we use ResNet50, you can also assign other model by setting the --model parameter. Such as: --model=\"resnet50\" , --model=\"vgg\" and so on. Description of dataset 1) To get reader quickly start, we provide synthetic virtual dataset, which refers to data is generated directly in memory as a random source of neural network. 2) At the same time, we provide a mini-dataset. You can download and unzip it into data directory, you can start training quickly. After getting familiar with the process, readers can refer to the making dataset part to make a complete ImageNet 2012 dataset. 3) Using OFRecord dataset can improve the efficientcy of data loading (But this is not necessary, refer to Data Input , OneFlow supports loading numpy data directly).","title":"Requirements"},{"location":"adv_examples/resnet.html#quick-start","text":"So, let's start OneFlow's image classification journey ! First, switch to the directory: cd OneFlow-Benchmark/Classification/cnns","title":"Quick Start"},{"location":"adv_examples/resnet.html#pretrained-model","text":"","title":"Pretrained Model"},{"location":"adv_examples/resnet.html#resnet50","text":"resnet50_v1.5_model (validation accuracy: 77.318% top1\uff0c93.622% top5 )","title":"resnet50"},{"location":"adv_examples/resnet.html#predict-inference","text":"After downloading pretrained model, unzip it and put it into the current directory. Then execute: sh inference.sh This script will call the model to classify the goldfish picture: The prediction is successful if the following is output. data/fish.jpg 0.87059885 goldfish, Carassius auratus As you can see, model judge this picture with 87.05% probability is goldfish.","title":"Predict / Inference"},{"location":"adv_examples/resnet.html#train-validation","text":"Training model is also easy as we just need to execute: sh train.sh You can start training model and you will see the follow output Loading synthetic data. Loading synthetic data. Saving model to ./output/snapshots/model_save-20200723124215/snapshot_initial_model. Init model on demand. train: epoch 0, iter 10, loss: 7.197278, top_1: 0.000000, top_k: 0.000000, samples/s: 61.569 train: epoch 0, iter 20, loss: 6.177684, top_1: 0.000000, top_k: 0.000000, samples/s: 122.555 Saving model to ./output/snapshots/model_save-20200723124215/snapshot_epoch_0. train: epoch 0, iter 30, loss: 3.988656, top_1: 0.525000, top_k: 0.812500, samples/s: 120.337 train: epoch 1, iter 10, loss: 1.185733, top_1: 1.000000, top_k: 1.000000, samples/s: 80.705 train: epoch 1, iter 20, loss: 1.042017, top_1: 1.000000, top_k: 1.000000, samples/s: 118.478 Saving model to ./output/snapshots/model_save-20200723124215/snapshot_epoch_1. ... To facilitate running the demonstration, we use synthetic virtual dataset by default so that you can quickly see the model in action. Also, you can use mini-dataset , after downloading it and unzip it in data directory, and then modify the training script as follows: rm -rf core.* rm -rf ./output/snapshots/* DATA_ROOT=data/imagenet/ofrecord python3 of_cnn_train_val.py \\ --train_data_dir=$DATA_ROOT/train \\ --num_examples=50 \\ --train_data_part_num=1 \\ --val_data_dir=$DATA_ROOT/validation \\ --num_val_examples=50 \\ --val_data_part_num=1 \\ --num_nodes=1 \\ --gpu_num_per_node=1 \\ --model_update=\"momentum\" \\ --learning_rate=0.001 \\ --loss_print_every_n_iter=1 \\ --batch_size_per_device=16 \\ --val_batch_size_per_device=10 \\ --num_epoch=10 \\ --model=\"resnet50\" Running this script, we will train a classfication model on the mini-ImageNet dataset with only 50 goldfish images. We can use this model to classify the goldfish image. Don't worry, if you need to train model on the complete ImageNet2012 dataset, please refer to OneFlow-Benchmark repository.","title":"Train &amp; Validation"},{"location":"adv_examples/resnet.html#evaluate","text":"You can evaluate the accuracy of the Resnet50 model using either your own trained model or the resnet50_v1.5_model (unzip it and put it in current directory) provided by us. Run this script: sh evaluate.sh The accuracy of the trained model on validation dataset with 50000 images can be obtained: Time stamp: 2020-07-27-09:28:28 Restoring model from resnet_v15_of_best_model_val_top1_77318. I0727 09:28:28.773988162 8411 ev_epoll_linux.c:82] Use of signals is disabled. Epoll engine will not be used Loading data from /dataset/ImageNet/ofrecord/validation validation: epoch 0, iter 195, top_1: 0.773277, top_k: 0.936058, samples/s: 1578.325 validation: epoch 0, iter 195, top_1: 0.773237, top_k: 0.936078, samples/s: 1692.303 validation: epoch 0, iter 195, top_1: 0.773297, top_k: 0.936018, samples/s: 1686.896 Before executing sh evaluate.sh , make sure you have prepared the validation dataset of ImageNet 2012. Please refer to OneFlow-Benchmark repository to learn how to make validation dataset. From the evaluation results of the three rounds, out model has achieved 77.32+% Top1 accuracy. Finally, congratulations! You complete the training / validating, inference and evaluation of ResNet model on ImageNet dataset. Applause for yourself!","title":"Evaluate"},{"location":"adv_examples/resnet.html#details","text":"","title":"Details"},{"location":"adv_examples/resnet.html#distributed-training","text":"Simple and easy-to-use distributed training is one of OneFlow's main features OneFlow is designed to support efficient distributed training natively. Especially for distributed data parallelism, user do not have to worry about how to divide and synchronize the data when the algorithm expands from single machine to multiple machines. That is to say, in OneFlow, User only need to write algorithm from the view of single machine, and the code automatically has the ability of distributed training.","title":"Distributed training"},{"location":"adv_examples/resnet.html#how-to-configure-and-run-distributed-training","text":"We still use the code shown in the \"Quick Start\", in train.sh , the distributed configuration is easily accomplished by specifying the number of nodes (machines) with --num_nodes , the IP address of the nodes with --node_ips , and the number of devices to be used on each node with --gpu_num_per_node . For example, we want to do distributed training on 2 machines with 8 devices, configure it like this: # train.sh python3 of_cnn_train_val.py \\ --num_nodes=2 \\ --node_ips=\"192.168.1.1, 192.168.1.2\" --gpu_num_per_node=4 \\ ... --model=\"resnet50\" Then execute the following script on the two machines at the same time: ./train.sh After the program starts, you can see through the command watch -n 0.1 nvidia-smi that both machines' devices start working. After a while, the output is printed on the screen of the first machine set by --node_ips .","title":"How to configure and run distributed training?"},{"location":"adv_examples/resnet.html#hybrid-precision-training-and-predicting","text":"Currently, OneFlow supports float16/float32 hybrid precision training. During training, the model parameters are trained using float16 while retaining float32 as the gradient update and calculation process. Since the storage of parameters is halved, the training speed will be improved. By turning on the hybrid precision training mode in OneFlow, ResNet50's training speed can theoretically reach 1.7 times of acceleration.","title":"Hybrid precision training and predicting"},{"location":"adv_examples/resnet.html#how-to-turn-on-the-hybrid-precision-training-mode","text":"Just add the parameter --use_fp16=True in the train.sh script.","title":"How to turn on the hybrid precision training mode\uff1f"},{"location":"adv_examples/resnet.html#hybrid-precision-model","text":"We provide a hybrid precision model after training 90 epochs on ImageNet2012 dataset, its Top_1 accuracy: 77.33%. You can download and use it directly: resnet50_v15_fp16","title":"Hybrid precision model"},{"location":"adv_examples/resnet.html#advanced","text":"","title":"Advanced"},{"location":"adv_examples/resnet.html#parameters-alignment","text":"OneFlow's ResNet50 implementation is aligned with Nvidia's Mxnet edition. We've made careful and almost identical alignment from the learning rate, optimizer, image augmentation to finer per-layer network configuration, bias, weight initialization, and more. The detailed parameters alignment please refer to OneFlow-Benchmark repository.","title":"Parameters alignment"},{"location":"adv_examples/resnet.html#preparing-dataset","text":"","title":"Preparing dataset"},{"location":"adv_examples/resnet.html#introduction-of-image-classification-dataset","text":"The public dataset used for image classification are CIFAR, ImageNet, etc. These datasets provide original images in JPEG format. CIFAR Hinton's student Alex Krizhevsky and Ilya Sutskever collated a small dataset to classify pervasive objects. It includes CIFAR-10 and CIFAR-100 ImageNet ImageNet dataset are generally referred to as the dataset used in large-scale visual recognition challenge (ILSVRC) between 2010-2017. The ImageNet data has changed slightly since 2010. The commonly used ImageNet-2012 dataset includes 1000 categories, its training dataset contains 1281167 pictures, ranging from 732 to 1300 per category. The validation dataset contains 50000 pictures, with an average of 50 pictures per category. For the complete process of preparing ImageNet-2012 dataset, please refer to README in the tools directory.","title":"Introduction of image classification dataset"},{"location":"adv_examples/resnet.html#convert-oneflow-model-to-onnx-model","text":"","title":"Convert OneFlow model to ONNX model"},{"location":"adv_examples/resnet.html#introduction_1","text":"ONNX (Open Neural Network Exchange) is a widely used neural network intermediate format. With the ONNX format, the OneFlow model can be used by many serving framework (like OpenVINO, ONNEX Runtime and some mobile framework: ncnn, tnn, TEgine, etc). In this section, we will introduce how to convert the trained ResNet50 v1.5 model to ONNX model and evaluate it.","title":"Introduction"},{"location":"adv_examples/resnet.html#quick-start_1","text":"We provide complete code: resnet_to_onnx.py , it can help you complete the transformation and testing of the model. **Step1: ** Download the pretrain model: resnet50_v1.5_model , unzip it and put it into current directory **Step2: ** Execute python3 resnet_to_onnx.py This code will complete the transformation of the OneFlow model -> ONNX model, and then use the ONNX Runtime to load the transformed model to test the individual images. The test picture is as follows: \u200b \u56fe\u7247\u6765\u6e90\uff1a https://en.wikipedia.org/wiki/Tiger Output\uff1a Convert to onnx success ! >> onnx / model / resnet_v15_of_best_model_val_top1_77318 . onnx data / tiger . jpg Are the results equal ? Yes Class : tiger , Panthera tigris ; score : 0.8112028241157532","title":"Quick Start"},{"location":"adv_examples/resnet.html#how-to-generate-onnx-model","text":"We have introduced how to convert OneFlow's ResNet model to ONNX model and give an example of using the onnx runtime to make predictions in above example. Similarly, you can follow the steps to complete the transformation of your training ResNet model or other models. Step1: Save the model's weight First you should specify the OneFlow model path, and then specify the transformed ONNX model storage path, like the following example. \u9996\u5148\u6307\u5b9a\u5f85\u8f6c\u6362\u7684OneFlow\u6a21\u578b\u8def\u5f84\uff0c\u7136\u540e\u6307\u5b9a\u8f6c\u6362\u540e\u7684ONNX\u6a21\u578b\u5b58\u653e\u8def\u5f84\uff0c\u4f8b\u5982\u793a\u4f8b\u4e2d\uff1a #set up your model path flow_weights_path = 'resnet_v15_of_best_model_val_top1_77318' onnx_model_dir = 'onnx/model' Step2: Create a new job function for inference Then, we create a new job function for inference, which only contains the network structure, except the operator to read the OFRecord, and accepts the form of numpy array input. You can refer to the InferenceNet in resnet_to_onnx.py . Step3: Call flow.onnx.export method In the following code, we call the oneflow_to_onnx() method, this method includes the core model transformation method: flow.onnx.export() . flow.onnx.export will obtain ONNX model from OneFlow network, its first parameter is the job function used to infer. The second parameter is OneFlow model path, the third parameter is the save path of ONNX model. onnx_model = oneflow_to_onnx ( InferenceNet , flow_weights_path , onnx_model_dir , external_data = False )","title":"How to generate ONNX model"},{"location":"adv_examples/resnet.html#evaluate-the-correctness-of-onnx-model","text":"After the ONNX model is generated, we can use ONNX model by ONNX Runtime to verify that the OneFlow model and the ONNX model give the same results with the same inputs. The corresponding code is check_equality in resnet_to_onnx.py .","title":"Evaluate the correctness of ONNX model"},{"location":"adv_examples/wide_deep.html","text":"\u73af\u5883\u548c\u51c6\u5907 \u8f6f\u4ef6\u8981\u6c42 \u6570\u636e\u51c6\u5907 OneFlow-WDL\u811a\u672c \u8fd0\u884cOneFlow-WDL\u811a\u672c \u6d4b\u8bd5\u7ed3\u679c\u53ca\u8bf4\u660e \u591aGPU\u6027\u80fd\u6d4b\u8bd5 batch size=16384\u6bcf\u5361\uff0c\u591a\u5361\u6027\u80fd\u6d4b\u8bd5 \u5355GPU\u5361\u4e0d\u540cbatch size\u6027\u80fd\u6d4b\u8bd5 \u8d85\u5927\u8bcd\u8868\u6d4b\u8bd5 \u6536\u655b\u6027\u6d4b\u8bd51 \u6536\u655b\u6027\u6d4b\u8bd52 HugeCTR \u662f\u82f1\u4f1f\u8fbe\u63d0\u4f9b\u7684\u4e00\u79cd\u9ad8\u6548\u7684GPU\u6846\u67b6\uff0c\u4e13\u4e3a\u70b9\u51fb\u7387\uff08CTR\uff09\u4f30\u8ba1\u8bad\u7ec3\u800c\u8bbe\u8ba1\u3002 OneFlow\u5bf9\u6807HugeCTR\u642d\u5efa\u4e86Wide & Deep \u5b66\u4e60\u7f51\u7edc\uff08WDL)\u3002OneFlow\u5bf9\u6807HugeCTR\u642d\u5efa\u4e86Wide & Deep \u5b66\u4e60\u7f51\u7edc\uff08WDL)\u3002OneFlow-WDL\u7f51\u7edc\u5b9e\u73b0\u4e86\u6a21\u578b\u5e76\u884c\u4e0e\u7a00\u758f\u66f4\u65b0\uff0c\u57288\u536112G TitanV\u7684\u670d\u52a1\u5668\u4e0a\u5b9e\u73b0\u652f\u6301\u8d85\u8fc74\u4ebf\u7684\u8bcd\u8868\u5927\u5c0f\uff0c\u800c\u4e14\u6027\u80fd\u6ca1\u6709\u635f\u5931\u4e0e\u5c0f\u8bcd\u8868\u6027\u80fd\u76f8\u5f53\u3002 \u672c\u6587\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528OneFlow-WDL\u7f51\u7edc\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ee5\u53ca\u4e00\u4e9b\u8bad\u7ec3\u7ed3\u679c\u53ca\u5206\u6790\u3002 \u73af\u5883\u548c\u51c6\u5907 \u00b6 \u8fd0\u884cOneFlow-WDL\u9700\u8981\u6709\u5b89\u88c5\u597dOneFlow\u7684python\u73af\u5883\uff0c\u5e76\u5b89\u88c5\u4e86 scikit-learn \u3002 \u8f6f\u4ef6\u8981\u6c42 \u00b6 python 3.x\uff08\u63a8\u8350\uff09 OneFlow 0.x scikit-learn \u6570\u636e\u51c6\u5907 \u00b6 \u6211\u4eec\u51c6\u5907\u4e86\u4e00\u4e2a\u5c0f\u7684 \u6837\u672c\u6570\u636e\u96c6 \uff0c\u53ef\u4ee5\u4e0b\u8f7d\u8fdb\u884c\u7b80\u5355\u6d4b\u8bd5\u3002 \u6216\u8005\u53c2\u8003 \u300a\u4f7f\u7528Spark\u521b\u5efaWDL\u6570\u636e\u96c6\u300b \u4e2d\u7684\u6b65\u9aa4\uff0c\u4eceCriteoLabs\u5b98\u7f51\u4e0b\u8f7d\u539f\u59cb\u6570\u636e\u96c6\u5e76\u5236\u4f5c\u6210OneFlow\u6240\u9700\u8981\u7684OFRecord\u683c\u5f0f\u7684\u6570\u636e\u96c6\u3002 OneFlow-WDL\u811a\u672c \u00b6 OneFlow-WDL\u811a\u672c\u53ea\u6709\u4e00\u4e2a\u6587\u4ef6 wdl_train_eval.py \uff0c\u8bf7\u4ece \u8fd9\u91cc \u4e0b\u8f7d\u3002 \u8fd0\u884cOneFlow-WDL\u811a\u672c \u00b6 EMBD_SIZE=1603616 DATA_ROOT=/path/to/wdl/ofrecord python3 wdl_train_eval.py \\ --train_data_dir $DATA_ROOT/train \\ --train_data_part_num 256 \\ --train_part_name_suffix_length=5 \\ --eval_data_dir $DATA_ROOT/val \\ --eval_data_part_num 256 \\ --max_iter=300000 \\ --loss_print_every_n_iter=1000 \\ --eval_interval=1000 \\ --batch_size=16384 \\ --wide_vocab_size=$EMBD_SIZE \\ --deep_vocab_size=$EMBD_SIZE \\ --gpu_num 1 \u901a\u5e38\u914d\u7f6e\u597d\u6570\u636e\u96c6\u7684\u4f4d\u7f6e DATA_ROOT \u540e\uff0c\u4e0a\u9762\u7684shell\u811a\u672c\u5c31\u53ef\u4ee5\u88ab\u6267\u884c\u4e86\uff0c\u5982\u679c\u5c4f\u5e55\u4e0a\u80fd\u591f\u8f93\u51fa\u4e0b\u9762\u7c7b\u4f3c\u7684\u7ed3\u679c\uff0c\u5c31\u8868\u793a\u5df2\u7ecf\u6b63\u786e\u8fd0\u884c\u3002 1000 time 2020-07-08 00:28:08.066281 loss 0.503295350909233 1000 eval_loss 0.4846755236387253 eval_auc 0.7616240146992771 2000 time 2020-07-08 00:28:11.613961 loss 0.48661992555856703 2000 eval_loss 0.4816856697201729 eval_auc 0.765256583562705 3000 time 2020-07-08 00:28:15.149135 loss 0.48245503094792364 3000 eval_loss 0.47835959643125536 eval_auc 0.7715609382514008 4000 time 2020-07-08 00:28:18.686327 loss 0.47975033831596375 4000 eval_loss 0.47925308644771575 eval_auc 0.7781267916810946 \u6d4b\u8bd5\u7ed3\u679c\u53ca\u8bf4\u660e \u00b6 \u6211\u4eec\u5728\u4e00\u53f0\u67098\u575712G\u663e\u5b58\u7684TitanV\u7684\u670d\u52a1\u5668\u4e0a\u5bf9OneFlow-WDL\u8fdb\u884c\u4e86\u4e00\u7ec4\u6d4b\u8bd5\uff0c\u5e76\u4f7f\u7528HugeCTR\u63d0\u4f9b\u7684docker\u5bb9\u5668\u505a\u4e86\u540c\u6837\u53c2\u6570\u7684\u6d4b\u8bd5\u3002 \u591aGPU\u6027\u80fd\u6d4b\u8bd5 \u00b6 \u4e3b\u8981\u6d4b\u8bd5\u76ee\u7684\u662f\u5728batch size = 16384\u7684\u60c5\u51b5\u4e0b\uff0c\u6d4b\u91cf\u4e0d\u540cGPU\u6570\u91cf\u5904\u7406\u6bcf\u4e2a\u6279\u6b21\u7684\u5e73\u5747\u65f6\u5ef6\uff08latency\uff09\u3002 \u6d4b\u8bd5\u914d\u7f6e\u4e867\u4e2a1024\u795e\u7ecf\u5355\u5143\u7684\u9690\u85cf\u5c42\u3002 \u6d4b\u8bd5\u914d\u7f6e\u4e867\u4e2a1024\u795e\u7ecf\u5355\u5143\u7684\u9690\u85cf\u5c42\u3002 \u7ed3\u679c\u5982\u4e0b\u56fe\uff1a \u6211\u4eec\u540c\u65f6\u8bb0\u5f55\u4e86\uff0c\u6d4b\u8bd5\u65f6\u5b9e\u9645\u6700\u5927\u5360\u7528\u663e\u5b58\u7684\u5927\u5c0f\uff0c\u7ed3\u679c\u5982\u4e0b\u56fe\uff1a \u7efc\u5408\u4e0a\u9762\u7ed3\u679c\u8868\u660e\uff0c1\u5361\u52308\u5361\uff0cOneFlow-WDL\u5728\u5360\u7528\u8f83\u5c11\u7684\u663e\u5b58\u7684\u60c5\u51b5\u4e0b\uff0c\u901f\u5ea6\u8981\u6bd4HugeCTR\u5feb\u3002 batch size=16384\u6bcf\u5361\uff0c\u591a\u5361\u6027\u80fd\u6d4b\u8bd5 \u00b6 \u4e3b\u8981\u6d4b\u8bd5\u76ee\u7684\u662f\u5728\u4fdd\u8bc1\u6bcfGPU\u5361\u5904\u740616384batch size\u60c5\u51b5\u4e0b\uff0c\u4f7f\u75281\u81f38GPU\u5361\u8fdb\u884c\u8bad\u7ec3\u6bcf\u4e2a\u6279\u6b21\u7684\u5e73\u5747\u65f6\u5ef6\uff08latency\uff09\u3002 \u6d4b\u8bd5\u914d\u7f6e\u4e867\u4e2a1024\u795e\u7ecf\u5355\u5143\u7684\u9690\u85cf\u5c42\u3002 \u6d4b\u8bd5\u914d\u7f6e\u4e867\u4e2a1024\u795e\u7ecf\u5355\u5143\u7684\u9690\u85cf\u5c42\u3002 \u7ed3\u679c\u5982\u4e0b\u56fe\uff1a \u6211\u4eec\u540c\u65f6\u8bb0\u5f55\u4e86\uff0c\u6d4b\u8bd5\u65f6\u5b9e\u9645\u6700\u5927\u5360\u7528\u663e\u5b58\u7684\u5927\u5c0f\uff0c\u7ed3\u679c\u5982\u4e0b\u56fe\uff1a \u7efc\u5408\u4e0a\u9762\u7ed3\u679c\u8868\u660e\uff0c\u968f\u7740\u5361\u6570\u7684\u589e\u52a0\uff0c\u65f6\u5ef6\u589e\u52a0\uff0cOneFlow-WDL\u5728\u5360\u7528\u8f83\u5c11\u7684\u663e\u5b58\u7684\u60c5\u51b5\u4e0b\uff0c\u901f\u5ea6\u8981\u6bd4HugeCTR\u5feb\uff1b\u56e0\u4e3a\u6bcf\u5361\u4fdd\u8bc116384 batch size\uff0cOneFlow\u6bcf\u5361\u5360\u7528\u7684\u5185\u5b58\u5e76\u65e0\u663e\u8457\u53d8\u5316\u3002 \u5355GPU\u5361\u4e0d\u540cbatch size\u6027\u80fd\u6d4b\u8bd5 \u00b6 \u4e3b\u8981\u6d4b\u8bd5\u76ee\u7684\u662f\u5728\u4e00\u4e2aGPU\u5361\u60c5\u51b5\u4e0b\uff0c\u6d4b\u91cf\u4e0d\u540cbatch size\u6bcf\u4e2a\u6279\u6b21\u7684\u5e73\u5747\u65f6\u5ef6\uff08latency\uff09\u3002 \u6d4b\u8bd5\u914d\u7f6e\u4e862\u4e2a1024\u795e\u7ecf\u5355\u5143\u7684\u9690\u85cf\u5c42\u3002 \u6d4b\u8bd5\u914d\u7f6e\u4e862\u4e2a1024\u795e\u7ecf\u5355\u5143\u7684\u9690\u85cf\u5c42\u3002 \u7ed3\u679c\u5982\u4e0b\u56fe\uff1a \u8d85\u5927\u8bcd\u8868\u6d4b\u8bd5 \u00b6 OneFlow-WDL\u4e2d\u914d\u7f6e\u4e86\u4e24\u4e2aEmbedding Table\uff1a - wide_embedding \u5927\u5c0f\u662fvocab_size x 1 - deep_embedding \u5927\u5c0f\u662fvocab_size x 16 HugeCTR\u4e2d\u8bcd\u8868\u5927\u5c0f\uff08vocab_size\uff09\u662f1603616\u3002\u6211\u4eec\u4ece3200000\u5f00\u59cb\u6d4b\u8d77\uff0c\u4e00\u76f4\u5230\u652f\u63014\u4ebf\u7684\u8bcd\u8868\u5927\u5c0f\uff0c\u7ed3\u679c\u5982\u4e0b\u56fe\uff1a\u6211\u4eec\u4ece3200000\u5f00\u59cb\u6d4b\u8d77\uff0c\u4e00\u76f4\u5230\u652f\u63014\u4ebf\u7684\u8bcd\u8868\u5927\u5c0f\uff0c\u7ed3\u679c\u5982\u4e0b\u56fe\uff1a \u4e0a\u9762\u7684\u56fe\u4e2d\uff0c\u84dd\u8272\u67f1\u5b50\u662f\u6279\u6b21\u8bad\u7ec3\u7684\u5e73\u5747\u65f6\u5ef6\uff08latency\uff09\uff0c\u7ea2\u8272\u66f2\u7ebf\u4ee3\u8868GPU\u663e\u5b58\u7684\u5360\u7528\u3002 \u7ed3\u8bba\uff1a\u968f\u7740\u8bcd\u8868\u5927\u5c0f\u7684\u589e\u5927\uff0c\u5185\u5b58\u968f\u4e4b\u589e\u5927\uff0c\u4f46latency\u6ca1\u6709\u660e\u663e\u7684\u53d8\u5316\u3002 \u6536\u655b\u6027\u6d4b\u8bd51 \u00b6 \u6211\u4eec\u9009\u53d6\u4e86batch size=512\u8fdb\u884c\u4e86\u6536\u655b\u6027\u80fd\u7684\u6d4b\u8bd5\u3002 \u4e0b\u9762\u8fd9\u5f20\u56fe\u662f\uff0c\u524d500\u6b65\u7684\u7ed3\u679c\uff0c\u6bcf\u4e00\u6b65\u8bad\u7ec3\u90fd\u5728\u9a8c\u8bc1\u96c6\u4e2d\u9009\u53d620\u6761\u8bb0\u5f55\u8fdb\u884c\u9a8c\u8bc1\uff0c\u56fe\u4e2d\u7684\u66f2\u7ebf\u5206\u522b\u662floss\u548cAUC\uff1a \u7ed3\u8bba\uff1aAUC\u8fc5\u901f\u5c31\u589e\u957f\u5230\u8d85\u8fc7\u4e860.75\u3002 \u6536\u655b\u6027\u6d4b\u8bd52 \u00b6 \u548c\u6536\u655b\u6027\u6d4b\u8bd51\u540c\u6837\u7684\u60c5\u51b5\uff0c\u8fd9\u4e00\u6b21\u662f\u6bcf\u8bad\u7ec31000\u6b65\u6253\u5370\u8bad\u7ec3loss\u7684\u5e73\u5747\u503c\uff0c\u7136\u540e\u9009\u53d620\u6761\u9a8c\u8bc1\u96c6\u6570\u636e\u8fdb\u884c\u9a8c\u8bc1\uff0c\u4e00\u5171\u8bad\u7ec330\u4e07\u6b65\uff0c\u7ed3\u679c\u5982\u4e0b\uff1a \u7ed3\u8bba\u4e0e\u5206\u6790\uff1a 1. \u84dd\u8272\u7684train loss\u66f2\u7ebf\u6709\u660e\u663e\u5411\u4e0b\u7684\u53f0\u9636\uff0c\u56e0\u4e3a\u6574\u4e2a\u8bad\u7ec3\u96c6\u670936674623\u6761\u6570\u636e\uff0cbatch_size=512\u7684\u60c5\u51b5\u4e0b\uff0c\u5927\u698271630\u6b65\u5c31\u8fc7\u4e86\u6574\u4e2a\u6570\u636e\u96c6\uff08\u4e00\u4e2aepoch\uff09\uff0c30\u4e07\u6b65\u5c31\u628a\u8bad\u7ec3\u6570\u636e\u96c6\u7528\u4e864\u6b21\u591a\uff0c\u84dd\u8272\u66f2\u7ebf\u7684\u53f0\u9636\u5370\u8bc1\u4e86\u8fd9\u4e9b\u3002OneFlow\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u652f\u6301\u6570\u636e\u7684\u6253\u4e71\uff0c\u6bcf\u5f53\u6570\u636e\u96c6\u88ab\u5b8c\u6574\u7684\u7528\u5b8c\u4e00\u904d\u4e4b\u540e\uff0c\u6570\u636e\u4f1a\u88ab\u91cd\u65b0\u6253\u4e71\uff0c\u51cf\u5c11\u8fc7\u62df\u5408\u3002OneFlow\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u652f\u6301\u6570\u636e\u7684\u6253\u4e71\uff0c\u6bcf\u5f53\u6570\u636e\u96c6\u88ab\u5b8c\u6574\u7684\u7528\u5b8c\u4e00\u904d\u4e4b\u540e\uff0c\u6570\u636e\u4f1a\u88ab\u91cd\u65b0\u6253\u4e71\uff0c\u51cf\u5c11\u8fc7\u62df\u5408\u3002 2. \u6a59\u8272\u7684\u66f2\u7ebf\u662f\u9a8c\u8bc1\u96c6loss\uff0c\u5728\u524d\u4e24\u4e2aepoch\u7684\u65f6\u5019\u57fa\u672c\u4fdd\u6301\u4e0b\u964d\u7684\u8d8b\u52bf\uff0c\u4ece\u7b2c\u4e09\u4e2aepoch\u5f00\u59cb\uff0closs\u5f00\u59cb\u6709\u4e0a\u5347\u7684\u8d8b\u52bf\uff0c\u8868\u660e\u5df2\u7ecf\u8fc7\u62df\u5408\u4e86\u3002 3. \u7070\u8272\u662f\u9a8c\u8bc1\u96c6\u7684AUC\uff0cAUC\u4e5f\u662f\u5728\u7b2c\u4e8c\u4e2aepoch\u7684\u65f6\u5019\u8fbe\u5230\u4e86\u5cf0\u503c\uff0c\u8d85\u8fc7\u4e860.8\uff0c\u540e\u9762\u51e0\u4e2aepoch\u5c31\u5f00\u59cb\u4e0b\u964d\u3002","title":"Wide deep"},{"location":"adv_examples/wide_deep.html#_1","text":"\u8fd0\u884cOneFlow-WDL\u9700\u8981\u6709\u5b89\u88c5\u597dOneFlow\u7684python\u73af\u5883\uff0c\u5e76\u5b89\u88c5\u4e86 scikit-learn \u3002","title":"\u73af\u5883\u548c\u51c6\u5907"},{"location":"adv_examples/wide_deep.html#_2","text":"python 3.x\uff08\u63a8\u8350\uff09 OneFlow 0.x scikit-learn","title":"\u8f6f\u4ef6\u8981\u6c42"},{"location":"adv_examples/wide_deep.html#_3","text":"\u6211\u4eec\u51c6\u5907\u4e86\u4e00\u4e2a\u5c0f\u7684 \u6837\u672c\u6570\u636e\u96c6 \uff0c\u53ef\u4ee5\u4e0b\u8f7d\u8fdb\u884c\u7b80\u5355\u6d4b\u8bd5\u3002 \u6216\u8005\u53c2\u8003 \u300a\u4f7f\u7528Spark\u521b\u5efaWDL\u6570\u636e\u96c6\u300b \u4e2d\u7684\u6b65\u9aa4\uff0c\u4eceCriteoLabs\u5b98\u7f51\u4e0b\u8f7d\u539f\u59cb\u6570\u636e\u96c6\u5e76\u5236\u4f5c\u6210OneFlow\u6240\u9700\u8981\u7684OFRecord\u683c\u5f0f\u7684\u6570\u636e\u96c6\u3002","title":"\u6570\u636e\u51c6\u5907"},{"location":"adv_examples/wide_deep.html#oneflow-wdl","text":"OneFlow-WDL\u811a\u672c\u53ea\u6709\u4e00\u4e2a\u6587\u4ef6 wdl_train_eval.py \uff0c\u8bf7\u4ece \u8fd9\u91cc \u4e0b\u8f7d\u3002","title":"OneFlow-WDL\u811a\u672c"},{"location":"adv_examples/wide_deep.html#oneflow-wdl_1","text":"EMBD_SIZE=1603616 DATA_ROOT=/path/to/wdl/ofrecord python3 wdl_train_eval.py \\ --train_data_dir $DATA_ROOT/train \\ --train_data_part_num 256 \\ --train_part_name_suffix_length=5 \\ --eval_data_dir $DATA_ROOT/val \\ --eval_data_part_num 256 \\ --max_iter=300000 \\ --loss_print_every_n_iter=1000 \\ --eval_interval=1000 \\ --batch_size=16384 \\ --wide_vocab_size=$EMBD_SIZE \\ --deep_vocab_size=$EMBD_SIZE \\ --gpu_num 1 \u901a\u5e38\u914d\u7f6e\u597d\u6570\u636e\u96c6\u7684\u4f4d\u7f6e DATA_ROOT \u540e\uff0c\u4e0a\u9762\u7684shell\u811a\u672c\u5c31\u53ef\u4ee5\u88ab\u6267\u884c\u4e86\uff0c\u5982\u679c\u5c4f\u5e55\u4e0a\u80fd\u591f\u8f93\u51fa\u4e0b\u9762\u7c7b\u4f3c\u7684\u7ed3\u679c\uff0c\u5c31\u8868\u793a\u5df2\u7ecf\u6b63\u786e\u8fd0\u884c\u3002 1000 time 2020-07-08 00:28:08.066281 loss 0.503295350909233 1000 eval_loss 0.4846755236387253 eval_auc 0.7616240146992771 2000 time 2020-07-08 00:28:11.613961 loss 0.48661992555856703 2000 eval_loss 0.4816856697201729 eval_auc 0.765256583562705 3000 time 2020-07-08 00:28:15.149135 loss 0.48245503094792364 3000 eval_loss 0.47835959643125536 eval_auc 0.7715609382514008 4000 time 2020-07-08 00:28:18.686327 loss 0.47975033831596375 4000 eval_loss 0.47925308644771575 eval_auc 0.7781267916810946","title":"\u8fd0\u884cOneFlow-WDL\u811a\u672c"},{"location":"adv_examples/wide_deep.html#_4","text":"\u6211\u4eec\u5728\u4e00\u53f0\u67098\u575712G\u663e\u5b58\u7684TitanV\u7684\u670d\u52a1\u5668\u4e0a\u5bf9OneFlow-WDL\u8fdb\u884c\u4e86\u4e00\u7ec4\u6d4b\u8bd5\uff0c\u5e76\u4f7f\u7528HugeCTR\u63d0\u4f9b\u7684docker\u5bb9\u5668\u505a\u4e86\u540c\u6837\u53c2\u6570\u7684\u6d4b\u8bd5\u3002","title":"\u6d4b\u8bd5\u7ed3\u679c\u53ca\u8bf4\u660e"},{"location":"adv_examples/wide_deep.html#gpu","text":"\u4e3b\u8981\u6d4b\u8bd5\u76ee\u7684\u662f\u5728batch size = 16384\u7684\u60c5\u51b5\u4e0b\uff0c\u6d4b\u91cf\u4e0d\u540cGPU\u6570\u91cf\u5904\u7406\u6bcf\u4e2a\u6279\u6b21\u7684\u5e73\u5747\u65f6\u5ef6\uff08latency\uff09\u3002 \u6d4b\u8bd5\u914d\u7f6e\u4e867\u4e2a1024\u795e\u7ecf\u5355\u5143\u7684\u9690\u85cf\u5c42\u3002 \u6d4b\u8bd5\u914d\u7f6e\u4e867\u4e2a1024\u795e\u7ecf\u5355\u5143\u7684\u9690\u85cf\u5c42\u3002 \u7ed3\u679c\u5982\u4e0b\u56fe\uff1a \u6211\u4eec\u540c\u65f6\u8bb0\u5f55\u4e86\uff0c\u6d4b\u8bd5\u65f6\u5b9e\u9645\u6700\u5927\u5360\u7528\u663e\u5b58\u7684\u5927\u5c0f\uff0c\u7ed3\u679c\u5982\u4e0b\u56fe\uff1a \u7efc\u5408\u4e0a\u9762\u7ed3\u679c\u8868\u660e\uff0c1\u5361\u52308\u5361\uff0cOneFlow-WDL\u5728\u5360\u7528\u8f83\u5c11\u7684\u663e\u5b58\u7684\u60c5\u51b5\u4e0b\uff0c\u901f\u5ea6\u8981\u6bd4HugeCTR\u5feb\u3002","title":"\u591aGPU\u6027\u80fd\u6d4b\u8bd5"},{"location":"adv_examples/wide_deep.html#batch-size16384","text":"\u4e3b\u8981\u6d4b\u8bd5\u76ee\u7684\u662f\u5728\u4fdd\u8bc1\u6bcfGPU\u5361\u5904\u740616384batch size\u60c5\u51b5\u4e0b\uff0c\u4f7f\u75281\u81f38GPU\u5361\u8fdb\u884c\u8bad\u7ec3\u6bcf\u4e2a\u6279\u6b21\u7684\u5e73\u5747\u65f6\u5ef6\uff08latency\uff09\u3002 \u6d4b\u8bd5\u914d\u7f6e\u4e867\u4e2a1024\u795e\u7ecf\u5355\u5143\u7684\u9690\u85cf\u5c42\u3002 \u6d4b\u8bd5\u914d\u7f6e\u4e867\u4e2a1024\u795e\u7ecf\u5355\u5143\u7684\u9690\u85cf\u5c42\u3002 \u7ed3\u679c\u5982\u4e0b\u56fe\uff1a \u6211\u4eec\u540c\u65f6\u8bb0\u5f55\u4e86\uff0c\u6d4b\u8bd5\u65f6\u5b9e\u9645\u6700\u5927\u5360\u7528\u663e\u5b58\u7684\u5927\u5c0f\uff0c\u7ed3\u679c\u5982\u4e0b\u56fe\uff1a \u7efc\u5408\u4e0a\u9762\u7ed3\u679c\u8868\u660e\uff0c\u968f\u7740\u5361\u6570\u7684\u589e\u52a0\uff0c\u65f6\u5ef6\u589e\u52a0\uff0cOneFlow-WDL\u5728\u5360\u7528\u8f83\u5c11\u7684\u663e\u5b58\u7684\u60c5\u51b5\u4e0b\uff0c\u901f\u5ea6\u8981\u6bd4HugeCTR\u5feb\uff1b\u56e0\u4e3a\u6bcf\u5361\u4fdd\u8bc116384 batch size\uff0cOneFlow\u6bcf\u5361\u5360\u7528\u7684\u5185\u5b58\u5e76\u65e0\u663e\u8457\u53d8\u5316\u3002","title":"batch size=16384\u6bcf\u5361\uff0c\u591a\u5361\u6027\u80fd\u6d4b\u8bd5"},{"location":"adv_examples/wide_deep.html#gpubatch-size","text":"\u4e3b\u8981\u6d4b\u8bd5\u76ee\u7684\u662f\u5728\u4e00\u4e2aGPU\u5361\u60c5\u51b5\u4e0b\uff0c\u6d4b\u91cf\u4e0d\u540cbatch size\u6bcf\u4e2a\u6279\u6b21\u7684\u5e73\u5747\u65f6\u5ef6\uff08latency\uff09\u3002 \u6d4b\u8bd5\u914d\u7f6e\u4e862\u4e2a1024\u795e\u7ecf\u5355\u5143\u7684\u9690\u85cf\u5c42\u3002 \u6d4b\u8bd5\u914d\u7f6e\u4e862\u4e2a1024\u795e\u7ecf\u5355\u5143\u7684\u9690\u85cf\u5c42\u3002 \u7ed3\u679c\u5982\u4e0b\u56fe\uff1a","title":"\u5355GPU\u5361\u4e0d\u540cbatch size\u6027\u80fd\u6d4b\u8bd5"},{"location":"adv_examples/wide_deep.html#_5","text":"OneFlow-WDL\u4e2d\u914d\u7f6e\u4e86\u4e24\u4e2aEmbedding Table\uff1a - wide_embedding \u5927\u5c0f\u662fvocab_size x 1 - deep_embedding \u5927\u5c0f\u662fvocab_size x 16 HugeCTR\u4e2d\u8bcd\u8868\u5927\u5c0f\uff08vocab_size\uff09\u662f1603616\u3002\u6211\u4eec\u4ece3200000\u5f00\u59cb\u6d4b\u8d77\uff0c\u4e00\u76f4\u5230\u652f\u63014\u4ebf\u7684\u8bcd\u8868\u5927\u5c0f\uff0c\u7ed3\u679c\u5982\u4e0b\u56fe\uff1a\u6211\u4eec\u4ece3200000\u5f00\u59cb\u6d4b\u8d77\uff0c\u4e00\u76f4\u5230\u652f\u63014\u4ebf\u7684\u8bcd\u8868\u5927\u5c0f\uff0c\u7ed3\u679c\u5982\u4e0b\u56fe\uff1a \u4e0a\u9762\u7684\u56fe\u4e2d\uff0c\u84dd\u8272\u67f1\u5b50\u662f\u6279\u6b21\u8bad\u7ec3\u7684\u5e73\u5747\u65f6\u5ef6\uff08latency\uff09\uff0c\u7ea2\u8272\u66f2\u7ebf\u4ee3\u8868GPU\u663e\u5b58\u7684\u5360\u7528\u3002 \u7ed3\u8bba\uff1a\u968f\u7740\u8bcd\u8868\u5927\u5c0f\u7684\u589e\u5927\uff0c\u5185\u5b58\u968f\u4e4b\u589e\u5927\uff0c\u4f46latency\u6ca1\u6709\u660e\u663e\u7684\u53d8\u5316\u3002","title":"\u8d85\u5927\u8bcd\u8868\u6d4b\u8bd5"},{"location":"adv_examples/wide_deep.html#1","text":"\u6211\u4eec\u9009\u53d6\u4e86batch size=512\u8fdb\u884c\u4e86\u6536\u655b\u6027\u80fd\u7684\u6d4b\u8bd5\u3002 \u4e0b\u9762\u8fd9\u5f20\u56fe\u662f\uff0c\u524d500\u6b65\u7684\u7ed3\u679c\uff0c\u6bcf\u4e00\u6b65\u8bad\u7ec3\u90fd\u5728\u9a8c\u8bc1\u96c6\u4e2d\u9009\u53d620\u6761\u8bb0\u5f55\u8fdb\u884c\u9a8c\u8bc1\uff0c\u56fe\u4e2d\u7684\u66f2\u7ebf\u5206\u522b\u662floss\u548cAUC\uff1a \u7ed3\u8bba\uff1aAUC\u8fc5\u901f\u5c31\u589e\u957f\u5230\u8d85\u8fc7\u4e860.75\u3002","title":"\u6536\u655b\u6027\u6d4b\u8bd51"},{"location":"adv_examples/wide_deep.html#2","text":"\u548c\u6536\u655b\u6027\u6d4b\u8bd51\u540c\u6837\u7684\u60c5\u51b5\uff0c\u8fd9\u4e00\u6b21\u662f\u6bcf\u8bad\u7ec31000\u6b65\u6253\u5370\u8bad\u7ec3loss\u7684\u5e73\u5747\u503c\uff0c\u7136\u540e\u9009\u53d620\u6761\u9a8c\u8bc1\u96c6\u6570\u636e\u8fdb\u884c\u9a8c\u8bc1\uff0c\u4e00\u5171\u8bad\u7ec330\u4e07\u6b65\uff0c\u7ed3\u679c\u5982\u4e0b\uff1a \u7ed3\u8bba\u4e0e\u5206\u6790\uff1a 1. \u84dd\u8272\u7684train loss\u66f2\u7ebf\u6709\u660e\u663e\u5411\u4e0b\u7684\u53f0\u9636\uff0c\u56e0\u4e3a\u6574\u4e2a\u8bad\u7ec3\u96c6\u670936674623\u6761\u6570\u636e\uff0cbatch_size=512\u7684\u60c5\u51b5\u4e0b\uff0c\u5927\u698271630\u6b65\u5c31\u8fc7\u4e86\u6574\u4e2a\u6570\u636e\u96c6\uff08\u4e00\u4e2aepoch\uff09\uff0c30\u4e07\u6b65\u5c31\u628a\u8bad\u7ec3\u6570\u636e\u96c6\u7528\u4e864\u6b21\u591a\uff0c\u84dd\u8272\u66f2\u7ebf\u7684\u53f0\u9636\u5370\u8bc1\u4e86\u8fd9\u4e9b\u3002OneFlow\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u652f\u6301\u6570\u636e\u7684\u6253\u4e71\uff0c\u6bcf\u5f53\u6570\u636e\u96c6\u88ab\u5b8c\u6574\u7684\u7528\u5b8c\u4e00\u904d\u4e4b\u540e\uff0c\u6570\u636e\u4f1a\u88ab\u91cd\u65b0\u6253\u4e71\uff0c\u51cf\u5c11\u8fc7\u62df\u5408\u3002OneFlow\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u652f\u6301\u6570\u636e\u7684\u6253\u4e71\uff0c\u6bcf\u5f53\u6570\u636e\u96c6\u88ab\u5b8c\u6574\u7684\u7528\u5b8c\u4e00\u904d\u4e4b\u540e\uff0c\u6570\u636e\u4f1a\u88ab\u91cd\u65b0\u6253\u4e71\uff0c\u51cf\u5c11\u8fc7\u62df\u5408\u3002 2. \u6a59\u8272\u7684\u66f2\u7ebf\u662f\u9a8c\u8bc1\u96c6loss\uff0c\u5728\u524d\u4e24\u4e2aepoch\u7684\u65f6\u5019\u57fa\u672c\u4fdd\u6301\u4e0b\u964d\u7684\u8d8b\u52bf\uff0c\u4ece\u7b2c\u4e09\u4e2aepoch\u5f00\u59cb\uff0closs\u5f00\u59cb\u6709\u4e0a\u5347\u7684\u8d8b\u52bf\uff0c\u8868\u660e\u5df2\u7ecf\u8fc7\u62df\u5408\u4e86\u3002 3. \u7070\u8272\u662f\u9a8c\u8bc1\u96c6\u7684AUC\uff0cAUC\u4e5f\u662f\u5728\u7b2c\u4e8c\u4e2aepoch\u7684\u65f6\u5019\u8fbe\u5230\u4e86\u5cf0\u503c\uff0c\u8d85\u8fc7\u4e860.8\uff0c\u540e\u9762\u51e0\u4e2aepoch\u5c31\u5f00\u59cb\u4e0b\u964d\u3002","title":"\u6536\u655b\u6027\u6d4b\u8bd52"},{"location":"adv_examples/yolov3.html","text":"YoloV3 \u00b6 1. Introduction \u00b6 YOLO series of algorithms (v1~v3), is the first single-stage object detection network, YOLO \u2014 You Only Look Once indicates its single-stage feature. Because the network is simple and the single-stage efficiency is fast, it is distinguished from the two-stage target detector represented by Faster-RCNN. Since it was released, it has become popular in the field of the target detection with its fast speed and high accuracy, and has been widely used and praised. While Yolov3 is the classic and comprehensive one(of course, the official also released Yolov4 recently). It takes Darknet-53 with residual network as the backbone, and integrates features such as multi-scale, 3-way output feature map and upsampling, which greatly improves the model accuracy and small target detection capability. In this article, we provide an OneFlow implementation of Yolov3. The difference is that we handle NMS process in C++ and call it by customizing user op. Of course, we also support handling NMS process in Python. 2. Quick Start \u00b6 Before we start, please make sure you have installed OneFlow properly. Git clone this repository git clone https://github.com/Oneflow-Inc/oneflow_yolov3.git 2. Install python dependency library pip install -r requirements.txt 3. Execute this script in project's root directory bash scripts/test.sh Execute this script to compile the operator defined in cpp code into a callable .so file. You will see in the project path. libdarknet.so liboneflow_yolov3.so Pretrain Model \u00b6 We use the pretrain model\u2014 yolov3.weight provided by Yolov3 author, and generate the model in OneFlow format after transformation. Download pretrain model: of_model_yolov3.zip , extract the of_model folder and put it in the root directory. 3. Predict/inference \u00b6 Execute the following script\uff1a sh yolo_predict.sh Or\uff1a sh yolo_predict_python_data_preprocess.sh After executing the script, we will generate the images with bounding box under the data/result . Parameters description - --pretrained_model Pretrain model path --label_path Coco label path --input_dir The path of images folder to be detected --output_dir The output path of the detect structure --image_paths Single/multiple paths of image to be detected. Like\uff1a --image_paths 'data/images/000002.jpg' 'data/images/000004.jpg' The training is also very simple. After preparing dataset, we only need to execute sh yolo_train.sh . The process of preparing dataset is shown in the Preparing Dataset part. 4. Preparing Dataset \u00b6 Yolov3 supports arbitrary object detection dataset. In the below we use COCO2014 as an example to create the training/validation dataset. Other datasets PASCAL VOC or custom datasets, can be created in the same format. Resource file \u00b6 Download COCO2014 training dataset and validation dataset. unzip it and put train2014 and val2014 under the data/COCO/images directory. (If you have downloaded COCO2014 dataset locally, you can create a soft link of images to the parent directory of train2014 and val2014 ) Prepare resource file: labels , 5k.part , trainvalno5k.part wget -c https://pjreddie.com/media/files/coco/5k.part wget -c https://pjreddie.com/media/files/coco/trainvalno5k.part wget -c https://pjreddie.com/media/files/coco/labels.tgz Scripts \u00b6 Execute the script in data/COCO directory: # get label file tar xzf labels.tgz # set up image list paste <(awk \"{print \\\"$PWD\\\"}\" <5k.part) 5k.part | tr -d '\\t' > 5k.txt paste <(awk \"{print \\\"$PWD\\\"}\" <trainvalno5k.part) trainvalno5k.part | tr -d '\\t' > trainvalno5k.txt # copy label txt to image dir find labels/train2014/ -name \"*.txt\" | xargs -i cp {} images/train2014/ find labels/val2014/ -name \"*.txt\" | xargs -i cp {} images/val2014/ This script will automatically unzip labels.tgz file, and generate 5k.txt and trainvalno5k.txt in current directory. Then copy all label.txt files in labels/train2014 and labels/val2014 to the corresponding training dataset and validation dataset folders (Make sure images and label are in the same directory). At this point, the preparation of the whole dataset is completed. 5. Training \u00b6 Modify the parameter in yolo_train.sh script, let --image_path_file=\"data/COCO/trainvalno5k.txt\" and execute: sh yolo_train.sh Then we start training, more detailed parameters are described as follows: --gpu_num_per_node The amount of devices on each machine --batch_size The batch size --base_lr The base learning rate --classes The number of target categories (COCO 80; VOC 20) --model_save_dir The model storage path --dataset_dir The path of training/validation dataset --num_epoch The total epochs --save_frequency Specify the epoch interval for model saving Descriptions \u00b6 At present, if we call yolo_predict.sh . The data preprocessing is dependent on darknet Among them: In predict decoder , we call load_image_color , letterbox_image function. In train decoder , we call load_data_detection function. It mainly involves the following operations, which will be replaced in later versions with OneFlow Decoder Ops image read nhwc -> nchw image / 255 bgr2rgb resize_image fill_image random_distort_image clip image random flip image and box randomize_boxes correct_boxes","title":"Yolov3"},{"location":"adv_examples/yolov3.html#yolov3","text":"","title":"YoloV3"},{"location":"adv_examples/yolov3.html#1-introduction","text":"YOLO series of algorithms (v1~v3), is the first single-stage object detection network, YOLO \u2014 You Only Look Once indicates its single-stage feature. Because the network is simple and the single-stage efficiency is fast, it is distinguished from the two-stage target detector represented by Faster-RCNN. Since it was released, it has become popular in the field of the target detection with its fast speed and high accuracy, and has been widely used and praised. While Yolov3 is the classic and comprehensive one(of course, the official also released Yolov4 recently). It takes Darknet-53 with residual network as the backbone, and integrates features such as multi-scale, 3-way output feature map and upsampling, which greatly improves the model accuracy and small target detection capability. In this article, we provide an OneFlow implementation of Yolov3. The difference is that we handle NMS process in C++ and call it by customizing user op. Of course, we also support handling NMS process in Python.","title":"1. Introduction"},{"location":"adv_examples/yolov3.html#2-quick-start","text":"Before we start, please make sure you have installed OneFlow properly. Git clone this repository git clone https://github.com/Oneflow-Inc/oneflow_yolov3.git 2. Install python dependency library pip install -r requirements.txt 3. Execute this script in project's root directory bash scripts/test.sh Execute this script to compile the operator defined in cpp code into a callable .so file. You will see in the project path. libdarknet.so liboneflow_yolov3.so","title":"2. Quick Start"},{"location":"adv_examples/yolov3.html#pretrain-model","text":"We use the pretrain model\u2014 yolov3.weight provided by Yolov3 author, and generate the model in OneFlow format after transformation. Download pretrain model: of_model_yolov3.zip , extract the of_model folder and put it in the root directory.","title":"Pretrain Model"},{"location":"adv_examples/yolov3.html#3-predictinference","text":"Execute the following script\uff1a sh yolo_predict.sh Or\uff1a sh yolo_predict_python_data_preprocess.sh After executing the script, we will generate the images with bounding box under the data/result . Parameters description - --pretrained_model Pretrain model path --label_path Coco label path --input_dir The path of images folder to be detected --output_dir The output path of the detect structure --image_paths Single/multiple paths of image to be detected. Like\uff1a --image_paths 'data/images/000002.jpg' 'data/images/000004.jpg' The training is also very simple. After preparing dataset, we only need to execute sh yolo_train.sh . The process of preparing dataset is shown in the Preparing Dataset part.","title":"3. Predict/inference"},{"location":"adv_examples/yolov3.html#4-preparing-dataset","text":"Yolov3 supports arbitrary object detection dataset. In the below we use COCO2014 as an example to create the training/validation dataset. Other datasets PASCAL VOC or custom datasets, can be created in the same format.","title":"4. Preparing Dataset"},{"location":"adv_examples/yolov3.html#resource-file","text":"Download COCO2014 training dataset and validation dataset. unzip it and put train2014 and val2014 under the data/COCO/images directory. (If you have downloaded COCO2014 dataset locally, you can create a soft link of images to the parent directory of train2014 and val2014 ) Prepare resource file: labels , 5k.part , trainvalno5k.part wget -c https://pjreddie.com/media/files/coco/5k.part wget -c https://pjreddie.com/media/files/coco/trainvalno5k.part wget -c https://pjreddie.com/media/files/coco/labels.tgz","title":"Resource file"},{"location":"adv_examples/yolov3.html#scripts","text":"Execute the script in data/COCO directory: # get label file tar xzf labels.tgz # set up image list paste <(awk \"{print \\\"$PWD\\\"}\" <5k.part) 5k.part | tr -d '\\t' > 5k.txt paste <(awk \"{print \\\"$PWD\\\"}\" <trainvalno5k.part) trainvalno5k.part | tr -d '\\t' > trainvalno5k.txt # copy label txt to image dir find labels/train2014/ -name \"*.txt\" | xargs -i cp {} images/train2014/ find labels/val2014/ -name \"*.txt\" | xargs -i cp {} images/val2014/ This script will automatically unzip labels.tgz file, and generate 5k.txt and trainvalno5k.txt in current directory. Then copy all label.txt files in labels/train2014 and labels/val2014 to the corresponding training dataset and validation dataset folders (Make sure images and label are in the same directory). At this point, the preparation of the whole dataset is completed.","title":"Scripts"},{"location":"adv_examples/yolov3.html#5-training","text":"Modify the parameter in yolo_train.sh script, let --image_path_file=\"data/COCO/trainvalno5k.txt\" and execute: sh yolo_train.sh Then we start training, more detailed parameters are described as follows: --gpu_num_per_node The amount of devices on each machine --batch_size The batch size --base_lr The base learning rate --classes The number of target categories (COCO 80; VOC 20) --model_save_dir The model storage path --dataset_dir The path of training/validation dataset --num_epoch The total epochs --save_frequency Specify the epoch interval for model saving","title":"5. Training"},{"location":"adv_examples/yolov3.html#descriptions","text":"At present, if we call yolo_predict.sh . The data preprocessing is dependent on darknet Among them: In predict decoder , we call load_image_color , letterbox_image function. In train decoder , we call load_data_detection function. It mainly involves the following operations, which will be replaced in later versions with OneFlow Decoder Ops image read nhwc -> nchw image / 255 bgr2rgb resize_image fill_image random_distort_image clip image random flip image and box randomize_boxes correct_boxes","title":"Descriptions"},{"location":"basics_topics/async_get.html","text":"Get Results from Job Function \u00b6 In this article, we will talk about getting the return value of a job function in OneFlow. It covers: How to get the return value from a job function synchronously. How to get the return value from a job function asynchronously. In OneFlow, a function decorated by @flow.global_function is called \"Job Function\". Job function can be implmented for training, evaluation and prediction. By specifing the return type of job function, we can get results from job function both synchronously and asynchronously. Difference Between Synchronous and Asynchronous \u00b6 Synchronization \u00b6 During synchronous training, the training of the next step cannot be started until the work of the previous step is completed. Asynchronization \u00b6 In asynchronous training, it is equivalent to turning on multi-threading mode. A step does not have to wait until previous step completes. For instance data preprocessing and loading task could be runned in advance. Through the comparison above, it can be seen that the use of asynchronous execution job function in OneFlow can effectively utilize computer resources, especially in the case of loading huge data, enabling asynchronous execution can effectively shorten the data loading and preprocessing time, and accelerate the model training. Next, we will explain how to get the results synchronously and asynchronously from job function, and how to write callback functions in asynchronous jobs. At the end of the article, we will provide a complete example. The main points are\uff1a Getting results synchronously or asynchronously is determined by the return value type of job function The data type of return value is selected in oneflow.typing When we call a job function, the form of getting results synchronously / asynchronously is slightly different Get Result Synchronously \u00b6 When we define a job function, if the annotation of the return type of the job function is oneflow.typing.Numpy , the job function will be called synchronously. For example, when we define a job function like this: @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : with flow . scope . placement ( \"gpu\" , \"0:0\" ): logits = lenet ( images , train = True ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.1 ]) flow . optimizer . SGD ( lr_scheduler , momentum = 0 ) . minimize ( loss ) return loss Through Python annotations, OneFlow knows the type of the job function's return is oneflow.typing.Numpy , which corresponds to ndarray in Numpy . Then when we call the job function, it will simply return the ndarray object: loss = train_job ( images , labels ) if i % 20 == 0 : print ( loss . mean ()) From the example above, it should be noted that: When we define the job function, the return value of job function (loss) is a placeholder for graph construction. When we specify the return value type as oneflow.typing.Numpy . OneFlow will know that when the job function is called, the real data type returned is Numpy.ndarray object. By calling the job function train_job(images, labels) , we can get the result from job function directly, and the retnred value is a ndarray object corresponding to oneflow.typing.Numpy . Data Types in oneflow.typing \u00b6 The oneflow.typing contains all the data types that can be returned by the job function. The oneflow.typing.Numpy shown above is only one of them. The commonly used types and their corresponding meanings are listed as follows: oneflow.typing.Numpy : corresponding to a numpy.ndarray flow.typing.ListNumpy : corresponding to a list container. Each element in it is a numpy.ndarray object. It is related to the view of OneFlow for distributed training. We will see details in The consistent and mirrored view in distributed training. oneflow.typing.ListListNumpy \uff1acorresponds to a list container where each element is a TensorList object and some interfaces to OneFlow need to process or return multiple TensorList . More information refer to Term & Concept in OneFlow and related API documentation oneflow.typing.Callback : corresponding to a callback function. It is used to call job function asynchronously. We will introduce it below. In addition, OneFlow also allows job functions to pass out data in dictionary form. For examples of ListNumpy , ListNumpy , ListListNumpy and how to pass out data in dictionary form please refer to OneFlow's Test Case . Get Result Asynchronously \u00b6 Generally speaking, the efficiency of asynchronous training is higher than that of synchronous training. The following describes how to call job function asynchronously and process the results. The basic steps include: Prepare a callback function: It is necessary to specify the parameters accepted by the callback function by annotations and implementing the logic for return value of the job function inside the callback function. Implementing a Job Function: Specify flow.typing.Callback as the return type of the job function by annotation. As we will see in the following example that we can specify the parameter type of the callback function with Callback . Call the job function: Register the callback function prepared in step 1 above. The first three steps are completed by users of OneFlow. When the program runs, the registered callback function is called by OneFlow and the return values of the job function are passed as parameters to the callback function. Prepare a Callback Function \u00b6 Suppose the prototype of the callback function is: def cb_func ( result : T ): #... Among them, the result needs to be annotated to specify its type T , that is, numpy, listnumpy, etc. mentioned above, or their composite type. We will have corresponding examples below. The result of callback function is actually the return value of job function. Therefore, it must be consistent with the annotation of the return value of the job function For example, when we define a job function below: @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Callback [ tp . Numpy ]: # mlp initializer = flow . truncated_normal ( 0.1 ) reshape = flow . reshape ( images , [ images . shape [ 0 ], - 1 ]) hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"hidden\" , ) logits = flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , name = \"output\" ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.1 ]) flow . optimizer . SGD ( lr_scheduler , momentum = 0 ) . minimize ( loss ) return loss Annotation -> tp.Callback[tp.Numpy] means that this job function returns a oneflow.typing.Numpy type, and need to be called asynchronously. Thus, the callback function we defined should accept a numpy parameter: def cb_print_loss ( result : tp . Numpy ): global g_i if g_i % 20 == 0 : print ( result . mean ()) g_i += 1 Let's take a look at another example. If the job function is defined as below: @flow . global_function ( type = \"predict\" ) def eval_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Callback [ Tuple [ tp . Numpy , tp . Numpy ]]: with flow . scope . placement ( \"cpu\" , \"0:0\" ): logits = mlp ( images ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) return ( labels , logits ) Annotation -> tp.Callback[Tuple[tp.Numpy, tp.Numpy]] means that this job function returns a tuple and each element is tp.Numpy . The job function needs to be called asynchronously. Thus, the parameter annotation of the corresponding callback function should be: g_total = 0 g_correct = 0 def acc ( arguments : Tuple [ tp . Numpy , tp . Numpy ]): global g_total global g_correct labels = arguments [ 0 ] logits = arguments [ 1 ] predictions = np . argmax ( logits , 1 ) right_count = np . sum ( predictions == labels ) g_total += labels . shape [ 0 ] g_correct += right_count The arguments corresponds to the return type of the above job function. Registration of Callback Function \u00b6 When we call the job function asynchronously, the job function will return a 'callback' object, and we register the prepared callback function by passing it to that object. OneFlow will automatically call the registered callback function when it gets the training results. callbacker = train_job ( images , labels ) callbacker ( cb_print_loss ) But the above code is redundant, we recommend: train_job ( images , labels )( cb_print_loss ) Code \u00b6 Get single result synchronously \u00b6 We use LeNet as an example here to show how to get the return value loss synchronously and print the loss every 20 iterations. Code example: synchronize_single_job.py Run: wget https://docs.oneflow.org/code/basics_topics/synchronize_single_job.py python3 synchronize_single_job.py There will be outputs like: File mnist.npz already exist, path: ./mnist.npz 7.3258467 2.1435719 1.1712438 0.7531896 ... ... model saved \u00b6 Get Mutiple Results Synchronously \u00b6 In this case, the job function returns a tuple . We get the results labels and logits in tuple synchronously. Also, we evaluate the trained model in the above example, then output the accuracy rate. Code: synchronize_batch_job.py The trained model can be downloaded from lenet_models_1.zip Run: wget https://oneflow-public.oss-cn-beijing.aliyuncs.com/online_document/docs/quick_start/lenet_models_1.zip unzip lenet_models_1.zip wget https://docs.oneflow.org/code/basics_topics/synchronize_batch_job.py python3 synchronize_batch_job.py There will be outputs like: accuracy: 99.3% \u00b6 Get Single Result Asynchronously \u00b6 In this case, we take MLP as example to get the single return result loss from job function asynchronously, and the loss is printed every 20 rounds. Code: async_single_job.py Run: wget https://docs.oneflow.org/code/basics_topics/async_single_job.py python3 async_single_job.py There will be outputs like: File mnist.npz already exist, path: ./mnist.npz 3.0865736 0.8949808 0.47858357 0.3486296 ... Get Multiple Results Asynchronously \u00b6 In the following example, we will show how to get multiple return results of the job function asynchronously, evaluate the trained model in the above example, and get the accuracy. Code: async_batch_job.py The trained model can be downloaded from mlp_models_1.zip ]( https://oneflow-public.oss-cn-beijing.aliyuncs.com/online_document/docs/basics_topics/mlp_models_1.zip ) Run: wget https://oneflow-public.oss-cn-beijing.aliyuncs.com/online_document/docs/basics_topics/mlp_models_1.zip unzip mlp_models_1.zip wget https://docs.oneflow.org/code/basics_topics/async_batch_job.py python3 async_batch_job.py There would be outputs like: File mnist.npz already exist, path: ./mnist.npz accuracy: 97.6%","title":"Get results from job function"},{"location":"basics_topics/async_get.html#get-results-from-job-function","text":"In this article, we will talk about getting the return value of a job function in OneFlow. It covers: How to get the return value from a job function synchronously. How to get the return value from a job function asynchronously. In OneFlow, a function decorated by @flow.global_function is called \"Job Function\". Job function can be implmented for training, evaluation and prediction. By specifing the return type of job function, we can get results from job function both synchronously and asynchronously.","title":"Get Results from Job Function"},{"location":"basics_topics/async_get.html#difference-between-synchronous-and-asynchronous","text":"","title":"Difference Between Synchronous and Asynchronous"},{"location":"basics_topics/async_get.html#synchronization","text":"During synchronous training, the training of the next step cannot be started until the work of the previous step is completed.","title":"Synchronization"},{"location":"basics_topics/async_get.html#asynchronization","text":"In asynchronous training, it is equivalent to turning on multi-threading mode. A step does not have to wait until previous step completes. For instance data preprocessing and loading task could be runned in advance. Through the comparison above, it can be seen that the use of asynchronous execution job function in OneFlow can effectively utilize computer resources, especially in the case of loading huge data, enabling asynchronous execution can effectively shorten the data loading and preprocessing time, and accelerate the model training. Next, we will explain how to get the results synchronously and asynchronously from job function, and how to write callback functions in asynchronous jobs. At the end of the article, we will provide a complete example. The main points are\uff1a Getting results synchronously or asynchronously is determined by the return value type of job function The data type of return value is selected in oneflow.typing When we call a job function, the form of getting results synchronously / asynchronously is slightly different","title":"Asynchronization"},{"location":"basics_topics/async_get.html#get-result-synchronously","text":"When we define a job function, if the annotation of the return type of the job function is oneflow.typing.Numpy , the job function will be called synchronously. For example, when we define a job function like this: @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : with flow . scope . placement ( \"gpu\" , \"0:0\" ): logits = lenet ( images , train = True ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.1 ]) flow . optimizer . SGD ( lr_scheduler , momentum = 0 ) . minimize ( loss ) return loss Through Python annotations, OneFlow knows the type of the job function's return is oneflow.typing.Numpy , which corresponds to ndarray in Numpy . Then when we call the job function, it will simply return the ndarray object: loss = train_job ( images , labels ) if i % 20 == 0 : print ( loss . mean ()) From the example above, it should be noted that: When we define the job function, the return value of job function (loss) is a placeholder for graph construction. When we specify the return value type as oneflow.typing.Numpy . OneFlow will know that when the job function is called, the real data type returned is Numpy.ndarray object. By calling the job function train_job(images, labels) , we can get the result from job function directly, and the retnred value is a ndarray object corresponding to oneflow.typing.Numpy .","title":"Get Result Synchronously"},{"location":"basics_topics/async_get.html#data-types-in-oneflowtyping","text":"The oneflow.typing contains all the data types that can be returned by the job function. The oneflow.typing.Numpy shown above is only one of them. The commonly used types and their corresponding meanings are listed as follows: oneflow.typing.Numpy : corresponding to a numpy.ndarray flow.typing.ListNumpy : corresponding to a list container. Each element in it is a numpy.ndarray object. It is related to the view of OneFlow for distributed training. We will see details in The consistent and mirrored view in distributed training. oneflow.typing.ListListNumpy \uff1acorresponds to a list container where each element is a TensorList object and some interfaces to OneFlow need to process or return multiple TensorList . More information refer to Term & Concept in OneFlow and related API documentation oneflow.typing.Callback : corresponding to a callback function. It is used to call job function asynchronously. We will introduce it below. In addition, OneFlow also allows job functions to pass out data in dictionary form. For examples of ListNumpy , ListNumpy , ListListNumpy and how to pass out data in dictionary form please refer to OneFlow's Test Case .","title":"Data Types in oneflow.typing"},{"location":"basics_topics/async_get.html#get-result-asynchronously","text":"Generally speaking, the efficiency of asynchronous training is higher than that of synchronous training. The following describes how to call job function asynchronously and process the results. The basic steps include: Prepare a callback function: It is necessary to specify the parameters accepted by the callback function by annotations and implementing the logic for return value of the job function inside the callback function. Implementing a Job Function: Specify flow.typing.Callback as the return type of the job function by annotation. As we will see in the following example that we can specify the parameter type of the callback function with Callback . Call the job function: Register the callback function prepared in step 1 above. The first three steps are completed by users of OneFlow. When the program runs, the registered callback function is called by OneFlow and the return values of the job function are passed as parameters to the callback function.","title":"Get Result Asynchronously"},{"location":"basics_topics/async_get.html#prepare-a-callback-function","text":"Suppose the prototype of the callback function is: def cb_func ( result : T ): #... Among them, the result needs to be annotated to specify its type T , that is, numpy, listnumpy, etc. mentioned above, or their composite type. We will have corresponding examples below. The result of callback function is actually the return value of job function. Therefore, it must be consistent with the annotation of the return value of the job function For example, when we define a job function below: @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Callback [ tp . Numpy ]: # mlp initializer = flow . truncated_normal ( 0.1 ) reshape = flow . reshape ( images , [ images . shape [ 0 ], - 1 ]) hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"hidden\" , ) logits = flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , name = \"output\" ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.1 ]) flow . optimizer . SGD ( lr_scheduler , momentum = 0 ) . minimize ( loss ) return loss Annotation -> tp.Callback[tp.Numpy] means that this job function returns a oneflow.typing.Numpy type, and need to be called asynchronously. Thus, the callback function we defined should accept a numpy parameter: def cb_print_loss ( result : tp . Numpy ): global g_i if g_i % 20 == 0 : print ( result . mean ()) g_i += 1 Let's take a look at another example. If the job function is defined as below: @flow . global_function ( type = \"predict\" ) def eval_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Callback [ Tuple [ tp . Numpy , tp . Numpy ]]: with flow . scope . placement ( \"cpu\" , \"0:0\" ): logits = mlp ( images ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) return ( labels , logits ) Annotation -> tp.Callback[Tuple[tp.Numpy, tp.Numpy]] means that this job function returns a tuple and each element is tp.Numpy . The job function needs to be called asynchronously. Thus, the parameter annotation of the corresponding callback function should be: g_total = 0 g_correct = 0 def acc ( arguments : Tuple [ tp . Numpy , tp . Numpy ]): global g_total global g_correct labels = arguments [ 0 ] logits = arguments [ 1 ] predictions = np . argmax ( logits , 1 ) right_count = np . sum ( predictions == labels ) g_total += labels . shape [ 0 ] g_correct += right_count The arguments corresponds to the return type of the above job function.","title":"Prepare a Callback Function"},{"location":"basics_topics/async_get.html#registration-of-callback-function","text":"When we call the job function asynchronously, the job function will return a 'callback' object, and we register the prepared callback function by passing it to that object. OneFlow will automatically call the registered callback function when it gets the training results. callbacker = train_job ( images , labels ) callbacker ( cb_print_loss ) But the above code is redundant, we recommend: train_job ( images , labels )( cb_print_loss )","title":"Registration of Callback Function"},{"location":"basics_topics/async_get.html#code","text":"","title":"Code"},{"location":"basics_topics/async_get.html#get-single-result-synchronously","text":"We use LeNet as an example here to show how to get the return value loss synchronously and print the loss every 20 iterations. Code example: synchronize_single_job.py Run: wget https://docs.oneflow.org/code/basics_topics/synchronize_single_job.py python3 synchronize_single_job.py There will be outputs like: File mnist.npz already exist, path: ./mnist.npz 7.3258467 2.1435719 1.1712438 0.7531896 ... ... model saved","title":"Get single result synchronously"},{"location":"basics_topics/async_get.html#_1","text":"","title":""},{"location":"basics_topics/async_get.html#get-mutiple-results-synchronously","text":"In this case, the job function returns a tuple . We get the results labels and logits in tuple synchronously. Also, we evaluate the trained model in the above example, then output the accuracy rate. Code: synchronize_batch_job.py The trained model can be downloaded from lenet_models_1.zip Run: wget https://oneflow-public.oss-cn-beijing.aliyuncs.com/online_document/docs/quick_start/lenet_models_1.zip unzip lenet_models_1.zip wget https://docs.oneflow.org/code/basics_topics/synchronize_batch_job.py python3 synchronize_batch_job.py There will be outputs like: accuracy: 99.3%","title":"Get Mutiple Results Synchronously"},{"location":"basics_topics/async_get.html#_2","text":"","title":""},{"location":"basics_topics/async_get.html#get-single-result-asynchronously","text":"In this case, we take MLP as example to get the single return result loss from job function asynchronously, and the loss is printed every 20 rounds. Code: async_single_job.py Run: wget https://docs.oneflow.org/code/basics_topics/async_single_job.py python3 async_single_job.py There will be outputs like: File mnist.npz already exist, path: ./mnist.npz 3.0865736 0.8949808 0.47858357 0.3486296 ...","title":"Get Single Result Asynchronously"},{"location":"basics_topics/async_get.html#get-multiple-results-asynchronously","text":"In the following example, we will show how to get multiple return results of the job function asynchronously, evaluate the trained model in the above example, and get the accuracy. Code: async_batch_job.py The trained model can be downloaded from mlp_models_1.zip ]( https://oneflow-public.oss-cn-beijing.aliyuncs.com/online_document/docs/basics_topics/mlp_models_1.zip ) Run: wget https://oneflow-public.oss-cn-beijing.aliyuncs.com/online_document/docs/basics_topics/mlp_models_1.zip unzip mlp_models_1.zip wget https://docs.oneflow.org/code/basics_topics/async_batch_job.py python3 async_batch_job.py There would be outputs like: File mnist.npz already exist, path: ./mnist.npz accuracy: 97.6%","title":"Get Multiple Results Asynchronously"},{"location":"basics_topics/build_nn_with_op_and_layer.html","text":"Build a Neural Network \u00b6 In the article Recognition of MNIST Handwritten Digits , we have used \"operator\" in oneflow.nn and \"layer\" in oneflow.layers to build a LeNet neural network. Now we will use this simple neural network to introduce the core element for network construction in OneFlow: operator and layer. LeNet is constructed by convolution layer, pooling layer and fully connected layer. There are two types of elements in the above diagram, one is the computing units represented by boxes which including Op and Layer such as conv2d , dense , max_pool2d and etc. The other is the data represented by arrows. It corresponds to the following code: def lenet ( data , train = False ): initializer = flow . truncated_normal ( 0.1 ) conv1 = flow . layers . conv2d ( data , 32 , 5 , padding = \"SAME\" , activation = flow . nn . relu , name = \"conv1\" , kernel_initializer = initializer , ) pool1 = flow . nn . max_pool2d ( conv1 , ksize = 2 , strides = 2 , padding = \"SAME\" , name = \"pool1\" , data_format = \"NCHW\" ) conv2 = flow . layers . conv2d ( pool1 , 64 , 5 , padding = \"SAME\" , activation = flow . nn . relu , name = \"conv2\" , kernel_initializer = initializer , ) pool2 = flow . nn . max_pool2d ( conv2 , ksize = 2 , strides = 2 , padding = \"SAME\" , name = \"pool2\" , data_format = \"NCHW\" ) reshape = flow . reshape ( pool2 , [ pool2 . shape [ 0 ], - 1 ]) hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"dense1\" , ) if train : hidden = flow . nn . dropout ( hidden , rate = 0.5 , name = \"dropout\" ) return flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , name = \"dense2\" ) When the job function is running, data's shape is 100x1\u00d728\u00d728 . Data is firstly used as input in conv2d to participate in the convolution calculation and the result from conv1 is passed to max_pool2d as input. Operator and Layer \u00b6 Operator is a common concept. It is the basic calculation unit in OneFlow. reshape and nn.max_pool2d used in LeNet code are two kinds of operators. In contrast, layers.conv2d and layers.dense are not operator. They are layers which constructed by specific operators. The existence of layers makes it easier to build neural networks. For more details please refer to oneflow.layers API By reading oneflow.layers source code , you can learn the details of building a layer of calculations from basic operators. Data block in neural network \u00b6 OneFlow's default mode is a static graph mechanism and the network is actually built and run separately. As a result, when defining the network, there is no real data in each variable, which means they are just placeholders. The computation of the real data occurs during the call of the job function. When building the network by defining job function, we only describe the attributes and shapes(such as shape , dtype ) of the nodes in network. There is no data in the node, we call the node as PlaceHolder , OneFlow can compile and infer according to these placeholders to get the computation graph. The placeholders are usually called Blob in OneFlow. There is a corresponding base class BlobDef in OneFlow. When we build network, we can print the information of Blob . For example, we can print data shape and dtype as follow. print ( conv1 . shape , conv1 . dtype ) Operator Overloading \u00b6 The BlobDef class implements operator overloading which means BlobDef supports math operators such as addition, subtraction, multiplication and division and so on. Like '+' in the following code: output = output + fc2_biases which is same as: output = flow.broadcast_add(output, fc2_biases) Summary \u00b6 Neural network builds by OneFlow require the operators or layers provided by OneFlow as computing units. The placeholder Blob serves as input and output for the operators and layers. Also operator reloading helps simplify some of the statements. The operators provided by OneFlow can be found in the API documentation: oneflow.nn \u3001 oneflow.math \u3001 oneflow.layers","title":"Build a Neural Network"},{"location":"basics_topics/build_nn_with_op_and_layer.html#build-a-neural-network","text":"In the article Recognition of MNIST Handwritten Digits , we have used \"operator\" in oneflow.nn and \"layer\" in oneflow.layers to build a LeNet neural network. Now we will use this simple neural network to introduce the core element for network construction in OneFlow: operator and layer. LeNet is constructed by convolution layer, pooling layer and fully connected layer. There are two types of elements in the above diagram, one is the computing units represented by boxes which including Op and Layer such as conv2d , dense , max_pool2d and etc. The other is the data represented by arrows. It corresponds to the following code: def lenet ( data , train = False ): initializer = flow . truncated_normal ( 0.1 ) conv1 = flow . layers . conv2d ( data , 32 , 5 , padding = \"SAME\" , activation = flow . nn . relu , name = \"conv1\" , kernel_initializer = initializer , ) pool1 = flow . nn . max_pool2d ( conv1 , ksize = 2 , strides = 2 , padding = \"SAME\" , name = \"pool1\" , data_format = \"NCHW\" ) conv2 = flow . layers . conv2d ( pool1 , 64 , 5 , padding = \"SAME\" , activation = flow . nn . relu , name = \"conv2\" , kernel_initializer = initializer , ) pool2 = flow . nn . max_pool2d ( conv2 , ksize = 2 , strides = 2 , padding = \"SAME\" , name = \"pool2\" , data_format = \"NCHW\" ) reshape = flow . reshape ( pool2 , [ pool2 . shape [ 0 ], - 1 ]) hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"dense1\" , ) if train : hidden = flow . nn . dropout ( hidden , rate = 0.5 , name = \"dropout\" ) return flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , name = \"dense2\" ) When the job function is running, data's shape is 100x1\u00d728\u00d728 . Data is firstly used as input in conv2d to participate in the convolution calculation and the result from conv1 is passed to max_pool2d as input.","title":"Build a Neural Network"},{"location":"basics_topics/build_nn_with_op_and_layer.html#operator-and-layer","text":"Operator is a common concept. It is the basic calculation unit in OneFlow. reshape and nn.max_pool2d used in LeNet code are two kinds of operators. In contrast, layers.conv2d and layers.dense are not operator. They are layers which constructed by specific operators. The existence of layers makes it easier to build neural networks. For more details please refer to oneflow.layers API By reading oneflow.layers source code , you can learn the details of building a layer of calculations from basic operators.","title":"Operator and Layer"},{"location":"basics_topics/build_nn_with_op_and_layer.html#data-block-in-neural-network","text":"OneFlow's default mode is a static graph mechanism and the network is actually built and run separately. As a result, when defining the network, there is no real data in each variable, which means they are just placeholders. The computation of the real data occurs during the call of the job function. When building the network by defining job function, we only describe the attributes and shapes(such as shape , dtype ) of the nodes in network. There is no data in the node, we call the node as PlaceHolder , OneFlow can compile and infer according to these placeholders to get the computation graph. The placeholders are usually called Blob in OneFlow. There is a corresponding base class BlobDef in OneFlow. When we build network, we can print the information of Blob . For example, we can print data shape and dtype as follow. print ( conv1 . shape , conv1 . dtype )","title":"Data block in neural network"},{"location":"basics_topics/build_nn_with_op_and_layer.html#operator-overloading","text":"The BlobDef class implements operator overloading which means BlobDef supports math operators such as addition, subtraction, multiplication and division and so on. Like '+' in the following code: output = output + fc2_biases which is same as: output = flow.broadcast_add(output, fc2_biases)","title":"Operator Overloading"},{"location":"basics_topics/build_nn_with_op_and_layer.html#summary","text":"Neural network builds by OneFlow require the operators or layers provided by OneFlow as computing units. The placeholder Blob serves as input and output for the operators and layers. Also operator reloading helps simplify some of the statements. The operators provided by OneFlow can be found in the API documentation: oneflow.nn \u3001 oneflow.math \u3001 oneflow.layers","title":"Summary"},{"location":"basics_topics/concept_explanation.html","text":"Term & Concept in OneFlow \u00b6 In this article, we will explain some common terms and concepts in OneFlow. The main content is divided for algorithm engineers and framework developers: Algorithm Development Framework Development In algorithms development part, we will explain some common terms and concepts used in the process of deep learning algorithms development. And in framework development part, we will focus on the inner design concepts of OneFlow and some relevant element concepts. Algorithms Development \u00b6 1.Placeholder \u00b6 Placeholder is data placeholder . This concept is used to store the shape of input or output data. There is no actual data in Placeholder. For example: import oneflow.typing as tp def test_job ( images : tp . Numpy . Placeholder (( 32 , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( 32 ,), dtype = flow . int32 ), ) -> Tuple [ tp . Numpy , tp . Numpy ]: # do something with images or labels return ( images , labels ) The code above shows that the images shape is (32, 1, 28, 28) and images data type is flow.float32 , the labels shape is (32,) and labels data type is flow.int32 . 2.Tensor and Blob \u00b6 Tensor is a common concept in other framework. In PyTorch, Tensor contains the data, data type, grad, storing device and other attributes. Tensor can be used to create and describe the computation graph in forward and backward process. OneFlow Tensor is basically the same, but there are some difference. In order to provide sufficient support for distributed system and parallelism, the Tensor in OneFlow is more complex and have more types and attributes (Such as logical, physical, devices and attributes of distribution). The Tensor at logical level could be divided to different devices. In order to simplify description, OneFlow hides the different types of Tensor, all the things are defined by a higher level concept named Blob. In OneFlow, Blob has a base class BlobDef . You can print the attributes of Blob when building network. As in the following code, we can print conv1 's shape and dtype : print ( conv1 . shape , conv1 . dtype ) Blob can be Placeholder at compile time, but can also contains values at running time. 3.Job Function \u00b6 In OneFlow, we call the training, evaluating, predicting and inference tasks as job function. Job function connects logic of user and computing resources that managed by OneFlow. In OneFlow, we can use decorator @oneflow.global_function to convert a function to a job function. By this decorator, we can not only define the type of job function(such as: type=\"train\" ), but also bind a FunctionConfig object to set the configuration of job function which can help OneFlow to manage memory and device resources. 4.Layer and Operator \u00b6 Layer \u00b6 The layer concept in OneFlow is basically the same as the layer in TensorFlow, PyTorch and other popular deep learning framework. It is used to describe a layer in neural network such as convolution layer, batch normalization layer, fully connected layer and normalization layer. Layer can simplify the process of building neural network. For example you can use just few lines of code to build LeNet: def lenet ( data , train = False ): initializer = flow . truncated_normal ( 0.1 ) conv1 = flow . layers . conv2d ( data , 32 , 5 , padding = \"SAME\" , activation = flow . nn . relu , name = \"conv1\" , kernel_initializer = initializer , ) pool1 = flow . nn . max_pool2d ( conv1 , ksize = 2 , strides = 2 , padding = \"SAME\" , name = \"pool1\" , data_format = \"NCHW\" ) conv2 = flow . layers . conv2d ( pool1 , 64 , 5 , padding = \"SAME\" , activation = flow . nn . relu , name = \"conv2\" , kernel_initializer = initializer , ) pool2 = flow . nn . max_pool2d ( conv2 , ksize = 2 , strides = 2 , padding = \"SAME\" , name = \"pool2\" , data_format = \"NCHW\" ) reshape = flow . reshape ( pool2 , [ pool2 . shape [ 0 ], - 1 ]) hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"dense1\" , ) if train : hidden = flow . nn . dropout ( hidden , rate = 0.5 , name = \"dropout\" ) return flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , name = \"dense2\" ) Layer is composed by operators. For example: layers.conv2d is composed by conv2d and variable . Operator \u00b6 Operator is the basic calculation unit in OneFlow. The calculation in above example is completed by operators. flow.nn.max_pool2d and flow.reshape are two kinds of operators. 5.Mirrored View and Consistent View \u00b6 OneFlow use two types of view: Mirrored View and Consistent View . They are used to describe the distribution of data and model under distributed system. Different view is corresponding to different parallelism view. Mirrored View comes from mirrors view of MPI. It is used to describe the mirrored model to multiple devices when using data parallelism. Consistent View regards multi hosts and devices as one in distributed environment. From this view, OneFlow will hide the detailed parallelism strategy and try to find the optimal parallelism plan(data/model/hybrid parallelism). Basically: When we set the mirrored view ( flow.scope.mirrored_view ), it means we can only use data parallelism . For example, when we set one host and four devices in job function, the model will be broadcasted to all devices, the data will be divided into four parts and send to each device. When set consistent view ( flow.scope.consistent_view ), OneFlow can choose data parallelism, model parallelism or hybrid parallelism by its compiler. Framework developing \u00b6 1.Boxing \u00b6 The module responsible for splitting or merging data blob according the parallelism strategy. We called it Boxing . Such as: When the op of upstream and downstream has different parallelism feature (such as different parallelism number), OneFlow will use Boxing to automatic process the data conversion and transmission. 2.SBP \u00b6 All the forward and backward operations in neural network can be calculated by matrix. In block matrix calculation, matrix needs split and broadcast operations at different axises. OneFlow operator has an attribute to describe this, we call it SBP. Of course, the SBP in OneFlow is not only matrix calculation. It also corresponding to divided data into different devices, broadcast and some other operations. SBP is abbreviated for Split, Broadcast, Partial sum. Split \u00b6 In parallelism operations, tensor is divided into many sub tensors. Different operators allow tensor to be divided on different axis. Boxing will automatically handle the splitting of tensor on different axis. Broadcast \u00b6 In parallelism operator calculation, the tensor will be broadcasted to many devices. This makes the tensor be same on each device. Partial Sum \u00b6 If an operator has distributive property, different part of tensor can be simply added. 3.TensorBuffer and TensorList \u00b6 Base on static map mechanism, OneFlow can infer the tensor shape of each operator and distribute the memory in advance when compiling. It can achieve zero copies of memory when running programs. But in some specific scenarios, OneFlow needs handle growing data. For example, the shape of the image loaded by DataLoader is unknown when compiling. In order to handle the growing data, OneFlow have two type of data format which is TensorBuffer and TensorList . TensorBuffer \u00b6 TensorBuffer is a flexible data format. When using TensorBuffer. We need to specify the dimension of the instance. OneFlow will generate a corresponding TensorBuffer object for each instance. TensorBuffer will indirectly references memory data and the memory section is dynamic and discontinuous . TensorList \u00b6 Similar with TensorBuffer, TensorList also can store the growing data. The main difference is that the data of TensorList is continuous in memory.","title":"Term & Concept Explanation"},{"location":"basics_topics/concept_explanation.html#term-concept-in-oneflow","text":"In this article, we will explain some common terms and concepts in OneFlow. The main content is divided for algorithm engineers and framework developers: Algorithm Development Framework Development In algorithms development part, we will explain some common terms and concepts used in the process of deep learning algorithms development. And in framework development part, we will focus on the inner design concepts of OneFlow and some relevant element concepts.","title":"Term &amp; Concept in OneFlow"},{"location":"basics_topics/concept_explanation.html#algorithms-development","text":"","title":"Algorithms Development"},{"location":"basics_topics/concept_explanation.html#1placeholder","text":"Placeholder is data placeholder . This concept is used to store the shape of input or output data. There is no actual data in Placeholder. For example: import oneflow.typing as tp def test_job ( images : tp . Numpy . Placeholder (( 32 , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( 32 ,), dtype = flow . int32 ), ) -> Tuple [ tp . Numpy , tp . Numpy ]: # do something with images or labels return ( images , labels ) The code above shows that the images shape is (32, 1, 28, 28) and images data type is flow.float32 , the labels shape is (32,) and labels data type is flow.int32 .","title":"1.Placeholder"},{"location":"basics_topics/concept_explanation.html#2tensor-and-blob","text":"Tensor is a common concept in other framework. In PyTorch, Tensor contains the data, data type, grad, storing device and other attributes. Tensor can be used to create and describe the computation graph in forward and backward process. OneFlow Tensor is basically the same, but there are some difference. In order to provide sufficient support for distributed system and parallelism, the Tensor in OneFlow is more complex and have more types and attributes (Such as logical, physical, devices and attributes of distribution). The Tensor at logical level could be divided to different devices. In order to simplify description, OneFlow hides the different types of Tensor, all the things are defined by a higher level concept named Blob. In OneFlow, Blob has a base class BlobDef . You can print the attributes of Blob when building network. As in the following code, we can print conv1 's shape and dtype : print ( conv1 . shape , conv1 . dtype ) Blob can be Placeholder at compile time, but can also contains values at running time.","title":"2.Tensor and Blob"},{"location":"basics_topics/concept_explanation.html#3job-function","text":"In OneFlow, we call the training, evaluating, predicting and inference tasks as job function. Job function connects logic of user and computing resources that managed by OneFlow. In OneFlow, we can use decorator @oneflow.global_function to convert a function to a job function. By this decorator, we can not only define the type of job function(such as: type=\"train\" ), but also bind a FunctionConfig object to set the configuration of job function which can help OneFlow to manage memory and device resources.","title":"3.Job Function"},{"location":"basics_topics/concept_explanation.html#4layer-and-operator","text":"","title":"4.Layer and Operator"},{"location":"basics_topics/concept_explanation.html#layer","text":"The layer concept in OneFlow is basically the same as the layer in TensorFlow, PyTorch and other popular deep learning framework. It is used to describe a layer in neural network such as convolution layer, batch normalization layer, fully connected layer and normalization layer. Layer can simplify the process of building neural network. For example you can use just few lines of code to build LeNet: def lenet ( data , train = False ): initializer = flow . truncated_normal ( 0.1 ) conv1 = flow . layers . conv2d ( data , 32 , 5 , padding = \"SAME\" , activation = flow . nn . relu , name = \"conv1\" , kernel_initializer = initializer , ) pool1 = flow . nn . max_pool2d ( conv1 , ksize = 2 , strides = 2 , padding = \"SAME\" , name = \"pool1\" , data_format = \"NCHW\" ) conv2 = flow . layers . conv2d ( pool1 , 64 , 5 , padding = \"SAME\" , activation = flow . nn . relu , name = \"conv2\" , kernel_initializer = initializer , ) pool2 = flow . nn . max_pool2d ( conv2 , ksize = 2 , strides = 2 , padding = \"SAME\" , name = \"pool2\" , data_format = \"NCHW\" ) reshape = flow . reshape ( pool2 , [ pool2 . shape [ 0 ], - 1 ]) hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"dense1\" , ) if train : hidden = flow . nn . dropout ( hidden , rate = 0.5 , name = \"dropout\" ) return flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , name = \"dense2\" ) Layer is composed by operators. For example: layers.conv2d is composed by conv2d and variable .","title":"Layer"},{"location":"basics_topics/concept_explanation.html#operator","text":"Operator is the basic calculation unit in OneFlow. The calculation in above example is completed by operators. flow.nn.max_pool2d and flow.reshape are two kinds of operators.","title":"Operator"},{"location":"basics_topics/concept_explanation.html#5mirrored-view-and-consistent-view","text":"OneFlow use two types of view: Mirrored View and Consistent View . They are used to describe the distribution of data and model under distributed system. Different view is corresponding to different parallelism view. Mirrored View comes from mirrors view of MPI. It is used to describe the mirrored model to multiple devices when using data parallelism. Consistent View regards multi hosts and devices as one in distributed environment. From this view, OneFlow will hide the detailed parallelism strategy and try to find the optimal parallelism plan(data/model/hybrid parallelism). Basically: When we set the mirrored view ( flow.scope.mirrored_view ), it means we can only use data parallelism . For example, when we set one host and four devices in job function, the model will be broadcasted to all devices, the data will be divided into four parts and send to each device. When set consistent view ( flow.scope.consistent_view ), OneFlow can choose data parallelism, model parallelism or hybrid parallelism by its compiler.","title":"5.Mirrored View and Consistent View"},{"location":"basics_topics/concept_explanation.html#framework-developing","text":"","title":"Framework developing"},{"location":"basics_topics/concept_explanation.html#1boxing","text":"The module responsible for splitting or merging data blob according the parallelism strategy. We called it Boxing . Such as: When the op of upstream and downstream has different parallelism feature (such as different parallelism number), OneFlow will use Boxing to automatic process the data conversion and transmission.","title":"1.Boxing"},{"location":"basics_topics/concept_explanation.html#2sbp","text":"All the forward and backward operations in neural network can be calculated by matrix. In block matrix calculation, matrix needs split and broadcast operations at different axises. OneFlow operator has an attribute to describe this, we call it SBP. Of course, the SBP in OneFlow is not only matrix calculation. It also corresponding to divided data into different devices, broadcast and some other operations. SBP is abbreviated for Split, Broadcast, Partial sum.","title":"2.SBP"},{"location":"basics_topics/concept_explanation.html#split","text":"In parallelism operations, tensor is divided into many sub tensors. Different operators allow tensor to be divided on different axis. Boxing will automatically handle the splitting of tensor on different axis.","title":"Split"},{"location":"basics_topics/concept_explanation.html#broadcast","text":"In parallelism operator calculation, the tensor will be broadcasted to many devices. This makes the tensor be same on each device.","title":"Broadcast"},{"location":"basics_topics/concept_explanation.html#partial-sum","text":"If an operator has distributive property, different part of tensor can be simply added.","title":"Partial Sum"},{"location":"basics_topics/concept_explanation.html#3tensorbuffer-and-tensorlist","text":"Base on static map mechanism, OneFlow can infer the tensor shape of each operator and distribute the memory in advance when compiling. It can achieve zero copies of memory when running programs. But in some specific scenarios, OneFlow needs handle growing data. For example, the shape of the image loaded by DataLoader is unknown when compiling. In order to handle the growing data, OneFlow have two type of data format which is TensorBuffer and TensorList .","title":"3.TensorBuffer and TensorList"},{"location":"basics_topics/concept_explanation.html#tensorbuffer","text":"TensorBuffer is a flexible data format. When using TensorBuffer. We need to specify the dimension of the instance. OneFlow will generate a corresponding TensorBuffer object for each instance. TensorBuffer will indirectly references memory data and the memory section is dynamic and discontinuous .","title":"TensorBuffer"},{"location":"basics_topics/concept_explanation.html#tensorlist","text":"Similar with TensorBuffer, TensorList also can store the growing data. The main difference is that the data of TensorList is continuous in memory.","title":"TensorList"},{"location":"basics_topics/data_input.html","text":"Data Input \u00b6 Machine learning is driven by data. Data loading and preprocessing require both efficiency and scalability. OneFlow supports two methods to load data: One way to do this is to pass a Numpy ndarray object as a parameter to the job function directly. Another approach is to use DataLoader of OneFlow and its related operators. It can load and pre-process datasets of a particular format from the file system. Working directly with Numpy data is easy and convenient but only for small amounts of data. Because when the amount of data is too large, there may be barrier in preparing the Numpy data. Therefore, this approach is more suitable for the initial stages of the project to quickly validate and improve the algorithm. The DataLoader of OneFlow use techniques such as multi-threading and data pipelining which make data loading, data pre-processing more efficient.However, you need to prepare dataset which already supported by Oneflow or develop you own DataLoader for the datatype which not supported by Oneflow. Thus we recommend use that in mature projects. Use Numpy as Data Input \u00b6 Example \u00b6 We can directly use Numpy ndarray as data input during training or predicting with OneFlow: # feed_numpy.py import numpy as np import oneflow as flow import oneflow.typing as tp from typing import Tuple @flow . global_function ( type = \"predict\" ) def test_job ( images : tp . Numpy . Placeholder (( 32 , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( 32 ,), dtype = flow . int32 ), ) -> Tuple [ tp . Numpy , tp . Numpy ]: # do something with images or labels return ( images , labels ) if __name__ == \"__main__\" : images_in = np . random . uniform ( - 10 , 10 , ( 32 , 1 , 28 , 28 )) . astype ( np . float32 ) labels_in = np . random . randint ( - 10 , 10 , ( 32 ,)) . astype ( np . int32 ) images , labels = test_job ( images_in , labels_in ) print ( images . shape , labels . shape ) You can download code from feed_numpy.py and run it by: python3 feed_numpy.py Following output are expected: ( 32 , 1 , 28 , 28 ) ( 32 , ) Code Explanation \u00b6 In the above code, we defined a job function test_job() with images and labels as inputs and annotate (note that the formal parameter is followed by \u201c:\u201d , not \u201c=\u201d) to specifies the shape and data type of the data. Thus, the example generates Numpy data randomly ( images_in and labels_in ) according to the shape and data type requirements of the job function. images_in = np . random . uniform ( - 10 , 10 , ( 32 , 1 , 28 , 28 )) . astype ( np . float32 ) labels_in = np . random . randint ( - 10 , 10 , ( 32 , )) . astype ( np . int32 ) Then directly pass the Numpy data images_in and labels_in as parameters when the job function is called. images , labels = test_job ( images_in , labels_in ) The oneflow.typing.Numpy.Placeholder is the placeholder of Numpy ndarray . There are also various placeholders in OneFlow that can represent more complex forms of Numpy data. More details please refer to The Definition and Call of Job Function . Using DataLoader and Related Operators \u00b6 Under the oneflow.data module, there are DataLoader operators for loading datasets and associated data preprocessing operators.DataLoader is usually named as data.xxx_reader , such as the existing data.ofrecord_reader and data.coco_reader which support OneFlow's native OFRecord format and COCO dataset. In addition, there are other data preprocessing operators that are used to process the data after DataLoader has been loaded. The following code uses data.OFRecordImageDecoderRandomCrop for random image cropping and data.OFRecordRawDecoder for image decoding. You can refer to the API documentation for more details. Examples \u00b6 The following example reads the OFRecord data format file and dealing with images from the ImageNet dataset. The complete code can be downloaded here: of_data_pipeline.py . This script requires an OFRecord dataset and you can make your own one according to [this article] (. /extended_topics/how_to_make_of_dataset.md). Or you can download the part-00000 that we have prepared for you which contains 64 images. Then replace path/to/ImageNet/ofrecord in the script with the directory where the part-00000 file is located and run the script. The following example is running a script with our pre-prepared dataset: wget https://oneflow-public.oss-cn-beijing.aliyuncs.com/online_document/docs/basics_topics/part-00000 sed -i \"s:path/to/ImageNet/ofrecord:./:\" of_data_pipeline.py python3 of_data_pipeline.py The following output are expected: (64, 3, 224, 224) (64,) Code Explanation \u00b6 There are generally two stages in using OneFlow DataLoader: Load Data and Preprocessing Data . flow.data.ofrecord_reader in the script is responsible for loading data from the file system into memory. ofrecord = flow . data . ofrecord_reader ( \"path/to/ImageNet/ofrecord\" , batch_size = batch_size , data_part_num = 1 , part_name_suffix_length = 5 , random_shuffle = True , shuffle_after_epoch = True , ) To specify the directory where the OFRecord file is located and some other parameters please refer to data.ofrecord_reader . If the return value of the DataLoader is a basic data type. Then it can be used directly as an input to the downstream operator. Otherwise the data preprocessing operator needs to be called further for preprocessing. For example, in the script: image = flow . data . OFRecordImageDecoderRandomCrop ( ofrecord , \"encoded\" , color_space = color_space ) label = flow . data . OFRecordRawDecoder ( ofrecord , \"class/label\" , shape = (), dtype = flow . int32 ) rsz = flow . image . Resize ( image , resize_x = 224 , resize_y = 224 , color_space = color_space ) rng = flow . random . CoinFlip ( batch_size = batch_size ) normal = flow . image . CropMirrorNormalize ( rsz , mirror_blob = rng , color_space = color_space , mean = [ 123.68 , 116.779 , 103.939 ], std = [ 58.393 , 57.12 , 57.375 ], output_dtype = flow . float , ) OFRecordImageDecoderRandomCrop is responsible for randomly cropping the image, OFRecordRawDecoder is responsible for decoding the label directly from the ofrecord object. image.Resize resizes the cropped image to 224x224 and CropMirrorNormalize normalizes the image. More Formats Support by DataLoader \u00b6 OneFlow provides a number of DataLoaders and preprocessing operators, refer to oneflow.data for details. These operators will be enriched and optimized in the future, but users can also refer to this article to customize the DataLoader to meet specific needs.","title":"Data Input"},{"location":"basics_topics/data_input.html#data-input","text":"Machine learning is driven by data. Data loading and preprocessing require both efficiency and scalability. OneFlow supports two methods to load data: One way to do this is to pass a Numpy ndarray object as a parameter to the job function directly. Another approach is to use DataLoader of OneFlow and its related operators. It can load and pre-process datasets of a particular format from the file system. Working directly with Numpy data is easy and convenient but only for small amounts of data. Because when the amount of data is too large, there may be barrier in preparing the Numpy data. Therefore, this approach is more suitable for the initial stages of the project to quickly validate and improve the algorithm. The DataLoader of OneFlow use techniques such as multi-threading and data pipelining which make data loading, data pre-processing more efficient.However, you need to prepare dataset which already supported by Oneflow or develop you own DataLoader for the datatype which not supported by Oneflow. Thus we recommend use that in mature projects.","title":"Data Input"},{"location":"basics_topics/data_input.html#use-numpy-as-data-input","text":"","title":"Use Numpy as Data Input"},{"location":"basics_topics/data_input.html#example","text":"We can directly use Numpy ndarray as data input during training or predicting with OneFlow: # feed_numpy.py import numpy as np import oneflow as flow import oneflow.typing as tp from typing import Tuple @flow . global_function ( type = \"predict\" ) def test_job ( images : tp . Numpy . Placeholder (( 32 , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( 32 ,), dtype = flow . int32 ), ) -> Tuple [ tp . Numpy , tp . Numpy ]: # do something with images or labels return ( images , labels ) if __name__ == \"__main__\" : images_in = np . random . uniform ( - 10 , 10 , ( 32 , 1 , 28 , 28 )) . astype ( np . float32 ) labels_in = np . random . randint ( - 10 , 10 , ( 32 ,)) . astype ( np . int32 ) images , labels = test_job ( images_in , labels_in ) print ( images . shape , labels . shape ) You can download code from feed_numpy.py and run it by: python3 feed_numpy.py Following output are expected: ( 32 , 1 , 28 , 28 ) ( 32 , )","title":"Example"},{"location":"basics_topics/data_input.html#code-explanation","text":"In the above code, we defined a job function test_job() with images and labels as inputs and annotate (note that the formal parameter is followed by \u201c:\u201d , not \u201c=\u201d) to specifies the shape and data type of the data. Thus, the example generates Numpy data randomly ( images_in and labels_in ) according to the shape and data type requirements of the job function. images_in = np . random . uniform ( - 10 , 10 , ( 32 , 1 , 28 , 28 )) . astype ( np . float32 ) labels_in = np . random . randint ( - 10 , 10 , ( 32 , )) . astype ( np . int32 ) Then directly pass the Numpy data images_in and labels_in as parameters when the job function is called. images , labels = test_job ( images_in , labels_in ) The oneflow.typing.Numpy.Placeholder is the placeholder of Numpy ndarray . There are also various placeholders in OneFlow that can represent more complex forms of Numpy data. More details please refer to The Definition and Call of Job Function .","title":"Code Explanation"},{"location":"basics_topics/data_input.html#using-dataloader-and-related-operators","text":"Under the oneflow.data module, there are DataLoader operators for loading datasets and associated data preprocessing operators.DataLoader is usually named as data.xxx_reader , such as the existing data.ofrecord_reader and data.coco_reader which support OneFlow's native OFRecord format and COCO dataset. In addition, there are other data preprocessing operators that are used to process the data after DataLoader has been loaded. The following code uses data.OFRecordImageDecoderRandomCrop for random image cropping and data.OFRecordRawDecoder for image decoding. You can refer to the API documentation for more details.","title":"Using DataLoader and Related Operators"},{"location":"basics_topics/data_input.html#examples","text":"The following example reads the OFRecord data format file and dealing with images from the ImageNet dataset. The complete code can be downloaded here: of_data_pipeline.py . This script requires an OFRecord dataset and you can make your own one according to [this article] (. /extended_topics/how_to_make_of_dataset.md). Or you can download the part-00000 that we have prepared for you which contains 64 images. Then replace path/to/ImageNet/ofrecord in the script with the directory where the part-00000 file is located and run the script. The following example is running a script with our pre-prepared dataset: wget https://oneflow-public.oss-cn-beijing.aliyuncs.com/online_document/docs/basics_topics/part-00000 sed -i \"s:path/to/ImageNet/ofrecord:./:\" of_data_pipeline.py python3 of_data_pipeline.py The following output are expected: (64, 3, 224, 224) (64,)","title":"Examples"},{"location":"basics_topics/data_input.html#code-explanation_1","text":"There are generally two stages in using OneFlow DataLoader: Load Data and Preprocessing Data . flow.data.ofrecord_reader in the script is responsible for loading data from the file system into memory. ofrecord = flow . data . ofrecord_reader ( \"path/to/ImageNet/ofrecord\" , batch_size = batch_size , data_part_num = 1 , part_name_suffix_length = 5 , random_shuffle = True , shuffle_after_epoch = True , ) To specify the directory where the OFRecord file is located and some other parameters please refer to data.ofrecord_reader . If the return value of the DataLoader is a basic data type. Then it can be used directly as an input to the downstream operator. Otherwise the data preprocessing operator needs to be called further for preprocessing. For example, in the script: image = flow . data . OFRecordImageDecoderRandomCrop ( ofrecord , \"encoded\" , color_space = color_space ) label = flow . data . OFRecordRawDecoder ( ofrecord , \"class/label\" , shape = (), dtype = flow . int32 ) rsz = flow . image . Resize ( image , resize_x = 224 , resize_y = 224 , color_space = color_space ) rng = flow . random . CoinFlip ( batch_size = batch_size ) normal = flow . image . CropMirrorNormalize ( rsz , mirror_blob = rng , color_space = color_space , mean = [ 123.68 , 116.779 , 103.939 ], std = [ 58.393 , 57.12 , 57.375 ], output_dtype = flow . float , ) OFRecordImageDecoderRandomCrop is responsible for randomly cropping the image, OFRecordRawDecoder is responsible for decoding the label directly from the ofrecord object. image.Resize resizes the cropped image to 224x224 and CropMirrorNormalize normalizes the image.","title":"Code Explanation"},{"location":"basics_topics/data_input.html#more-formats-support-by-dataloader","text":"OneFlow provides a number of DataLoaders and preprocessing operators, refer to oneflow.data for details. These operators will be enriched and optimized in the future, but users can also refer to this article to customize the DataLoader to meet specific needs.","title":"More Formats Support by DataLoader"},{"location":"basics_topics/distributed_train.html","text":"Distributed Training \u00b6 In deep learning, more and more scenarios require distributed training. Since distributed systems face problems such as distributed task scheduling and complex resource parallelism in multiple cards machines. Thus distributed training usually has a certain technical threshold for users. In OneFlow, through top-level design and engineering innovation. It is easiest use distribution system . Users can easily use OneFlow for distributed training without making any special changes to the network structure or job logic. This is the most important feature that make OneFlow different from other frameworks. In this article, we will introduce: How to switch a program platform from a single machine to a distributed system. The concept and mission of node in OneFlow. The Distribution Advantage of OneFlow. \u00b6 OneFlow use decentralized streaming architecture. Not like master and worker architecture, it can optimize the communication efficiency of network to the maximum extent. Support consistent view , the whole network only needs one logic input and output. A mirrored view compatible with other frameworks is provided. Users who are familiar with the distributed training of other frameworks can learn to use it quickly. Only a few lines of configuration code are needed to switch a program platform from a single machine to a distributed system. Configuration of the Distributed Training \u00b6 By the distributed training interface of OneFlow, you only need a few configuration to specify the distributed computing nodes IP and the number of devices for performing distributed training network. In another word, it makes a single machine program and a distributed machine program almost the same in terms of complexity of coding. User just need to focus on job logic and structures of model without worrying about distribution execution. Everything related to distribution is handled by OneFlow. Here is an example to change a program run on a single machine to be run on a distributed system with few configurations. Single Machine Program \u00b6 Here is the framework of single machine training program. Because the code of each function will be presented in the distributed program below, it is not listed in detail here. import numpy as np import oneflow as flow import oneflow.typing as tp BATCH_SIZE = 100 def mlp ( data ): #build network... @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : #achieve job function ... #Optimization method and parameters configuration training if __name__ == '__main__' : #call job function and start training... loss = train_job ( images , labels ) #... Configuration of Ports and Device \u00b6 In oneflow.config , we provide interfaces related to distributed program. We mainly use two of them: oneflow.config.gpu_device_num : set the number of device. This will be applied to all machines. oneflow.config.ctrl_port : set the port number of communications. All the machines will use the same port. In the following demo, we set all machines to use one device and use the port 9988 for communication. User can change the configuration according to their actual situation. #device number flow . config . gpu_device_num ( 1 ) #Port number flow . env . ctrl_port ( 9988 ) To be mentioned that, if we only have one single machine with multiple GPU devices in it, we can still use flow.config.gpu_device_num to change a program from running on a single machine to run on a distributed system. In the code below, we will use two GPU devices in one machine to do the distributed training: flow . config . gpu_device_num ( 2 ) Node Configuration \u00b6 Then we need to config the connection between the machines in network. In OneFlow, the distributed machine called node . The network information of each node is stored as a dict . The key \"addr\" is corresponding with IP of this node. All nodes are stored in a list , which will be informed to Oneflow by flow.env.machine . OneFlow will automatically generate the connection between nodes. nodes = [{ \"addr\" : \"192.168.1.12\" }, { \"addr\" : \"192.168.1.11\" }] flow . env . machine ( nodes ) In the code above, we have two nodes in our distributed system. Their IP is \"192.168.1.12\" and \"192.168.1.11\". It should be noted that the node 0 in list (in the above code is 192.168.1.12) is called master node . After the whole distributed training system starts, it will create the graph while the other nodes are waiting. When construction of graph is finished, all nodes will receive a notice specifying which nodes that they need to contact. Then they will work together in a decentralized way. During the training process, master node will deal with the standard output and store the model. The other nodes are only responsible for calculation. We can wrap the configuration code for distributed training as a function, which is easy to be called: def config_distributed (): print ( \"distributed config\" ) #number of device used in each node flow . config . gpu_device_num ( 1 ) #communication channel flow . env . ctrl_port ( 9988 ) #node configuration nodes = [{ \"addr\" : \"192.168.1.12\" }, { \"addr\" : \"192.168.1.11\" }] flow . env . machine ( n Complete Code of Distributed Training \u00b6 After adding the configurations code, the program becomes a distributed training one. Just follow the same step as we do in a single machine program. Compared with single machine training program , the distributed training program only needs to call one more function named config_distributed . Distribution script: distributed_train.py Running on both 192.168.1.12 and 192.168.1.11 : wget https://docs.oneflow.org/code/basics_topics/distributed_train.py python3 distributed_train.py The result of the program will be displayed on 192.168.1.12 . FAQ \u00b6 After running this distribution code, the program waits for a long time and does not display the calculation results\u3002 Please check the ssh configuration to ensure that the two machines can be interconnected with each other ssh-free. Make sure that both machines are using the same version of OneFlow and are running the exact same script program. Make sure the port used for training is unoccupied or replace the port with oneflow.config.ctrl_port . If a proxy is set in an environment variable, make sure the proxy works or disable it. Run training in docker, program waits for a long time and does not show calculation results. In default mode of docker, the machine is isolated from the ports in the container. Then use --net=host (host mode) or use the -p option for port mapping when starting the container. For details information please refer to the docker manual. The communications library was not installed correctly Make sure the version of the communication library (nccl) is the same on each machine during distributed training. Using virtual network cards If there are virtual network cards, you may not be able to communicate with nccl. In this case, you need to specify the communication network cards by export NCCL_SOCKET_IFNAME=device_name . More details please refer to nccl official documentation .","title":"Distributed training"},{"location":"basics_topics/distributed_train.html#distributed-training","text":"In deep learning, more and more scenarios require distributed training. Since distributed systems face problems such as distributed task scheduling and complex resource parallelism in multiple cards machines. Thus distributed training usually has a certain technical threshold for users. In OneFlow, through top-level design and engineering innovation. It is easiest use distribution system . Users can easily use OneFlow for distributed training without making any special changes to the network structure or job logic. This is the most important feature that make OneFlow different from other frameworks. In this article, we will introduce: How to switch a program platform from a single machine to a distributed system. The concept and mission of node in OneFlow.","title":"Distributed Training"},{"location":"basics_topics/distributed_train.html#the-distribution-advantage-of-oneflow","text":"OneFlow use decentralized streaming architecture. Not like master and worker architecture, it can optimize the communication efficiency of network to the maximum extent. Support consistent view , the whole network only needs one logic input and output. A mirrored view compatible with other frameworks is provided. Users who are familiar with the distributed training of other frameworks can learn to use it quickly. Only a few lines of configuration code are needed to switch a program platform from a single machine to a distributed system.","title":"The Distribution Advantage of OneFlow."},{"location":"basics_topics/distributed_train.html#configuration-of-the-distributed-training","text":"By the distributed training interface of OneFlow, you only need a few configuration to specify the distributed computing nodes IP and the number of devices for performing distributed training network. In another word, it makes a single machine program and a distributed machine program almost the same in terms of complexity of coding. User just need to focus on job logic and structures of model without worrying about distribution execution. Everything related to distribution is handled by OneFlow. Here is an example to change a program run on a single machine to be run on a distributed system with few configurations.","title":"Configuration of the Distributed Training"},{"location":"basics_topics/distributed_train.html#single-machine-program","text":"Here is the framework of single machine training program. Because the code of each function will be presented in the distributed program below, it is not listed in detail here. import numpy as np import oneflow as flow import oneflow.typing as tp BATCH_SIZE = 100 def mlp ( data ): #build network... @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : #achieve job function ... #Optimization method and parameters configuration training if __name__ == '__main__' : #call job function and start training... loss = train_job ( images , labels ) #...","title":"Single Machine Program"},{"location":"basics_topics/distributed_train.html#configuration-of-ports-and-device","text":"In oneflow.config , we provide interfaces related to distributed program. We mainly use two of them: oneflow.config.gpu_device_num : set the number of device. This will be applied to all machines. oneflow.config.ctrl_port : set the port number of communications. All the machines will use the same port. In the following demo, we set all machines to use one device and use the port 9988 for communication. User can change the configuration according to their actual situation. #device number flow . config . gpu_device_num ( 1 ) #Port number flow . env . ctrl_port ( 9988 ) To be mentioned that, if we only have one single machine with multiple GPU devices in it, we can still use flow.config.gpu_device_num to change a program from running on a single machine to run on a distributed system. In the code below, we will use two GPU devices in one machine to do the distributed training: flow . config . gpu_device_num ( 2 )","title":"Configuration of Ports and Device"},{"location":"basics_topics/distributed_train.html#node-configuration","text":"Then we need to config the connection between the machines in network. In OneFlow, the distributed machine called node . The network information of each node is stored as a dict . The key \"addr\" is corresponding with IP of this node. All nodes are stored in a list , which will be informed to Oneflow by flow.env.machine . OneFlow will automatically generate the connection between nodes. nodes = [{ \"addr\" : \"192.168.1.12\" }, { \"addr\" : \"192.168.1.11\" }] flow . env . machine ( nodes ) In the code above, we have two nodes in our distributed system. Their IP is \"192.168.1.12\" and \"192.168.1.11\". It should be noted that the node 0 in list (in the above code is 192.168.1.12) is called master node . After the whole distributed training system starts, it will create the graph while the other nodes are waiting. When construction of graph is finished, all nodes will receive a notice specifying which nodes that they need to contact. Then they will work together in a decentralized way. During the training process, master node will deal with the standard output and store the model. The other nodes are only responsible for calculation. We can wrap the configuration code for distributed training as a function, which is easy to be called: def config_distributed (): print ( \"distributed config\" ) #number of device used in each node flow . config . gpu_device_num ( 1 ) #communication channel flow . env . ctrl_port ( 9988 ) #node configuration nodes = [{ \"addr\" : \"192.168.1.12\" }, { \"addr\" : \"192.168.1.11\" }] flow . env . machine ( n","title":"Node Configuration"},{"location":"basics_topics/distributed_train.html#complete-code-of-distributed-training","text":"After adding the configurations code, the program becomes a distributed training one. Just follow the same step as we do in a single machine program. Compared with single machine training program , the distributed training program only needs to call one more function named config_distributed . Distribution script: distributed_train.py Running on both 192.168.1.12 and 192.168.1.11 : wget https://docs.oneflow.org/code/basics_topics/distributed_train.py python3 distributed_train.py The result of the program will be displayed on 192.168.1.12 .","title":"Complete Code of Distributed Training"},{"location":"basics_topics/distributed_train.html#faq","text":"After running this distribution code, the program waits for a long time and does not display the calculation results\u3002 Please check the ssh configuration to ensure that the two machines can be interconnected with each other ssh-free. Make sure that both machines are using the same version of OneFlow and are running the exact same script program. Make sure the port used for training is unoccupied or replace the port with oneflow.config.ctrl_port . If a proxy is set in an environment variable, make sure the proxy works or disable it. Run training in docker, program waits for a long time and does not show calculation results. In default mode of docker, the machine is isolated from the ports in the container. Then use --net=host (host mode) or use the -p option for port mapping when starting the container. For details information please refer to the docker manual. The communications library was not installed correctly Make sure the version of the communication library (nccl) is the same on each machine during distributed training. Using virtual network cards If there are virtual network cards, you may not be able to communicate with nccl. In this case, you need to specify the communication network cards by export NCCL_SOCKET_IFNAME=device_name . More details please refer to nccl official documentation .","title":"FAQ"},{"location":"basics_topics/essentials_of_oneflow.html","text":"OneFlow System Design \u00b6 In this article, we will cover these topics: Motivation OneFlow feature 1: Runtime based on actor system OneFlow feature 2: Compile-time based on a formal description of parallelism Summary Motivation \u00b6 OneFlow is born for performance and horizontal scalability, especially for multi-nodes and multi-devices scenarios. We expect that users can leverage the power of multiple machines and multiple devices in a way as easy as using a single machine with single device, and enjoy the efficiency of linear speedup. Why does OneFlow focus on the performance and user experience in distributed scenarios? With the development of deep learning, the model becomes increasingly large, and the computing power required to train deep learning models will become higher and higher. The computing power and the memory of a single device are far from meeting the needs of deep learning model training, and multiple machines and multiple devices are required for parallelism speedup. If the deep learning framework can make multiple interconnected devices work well together and achieve linear speedup, even if the performance of each device is just so so, it can also meet the computing power needs of any scale. This is the so-called horizontal scalability or scaling out. We do believe this is the solution to the increasing need of computing power\u3002 However, the existing frameworks usually focus on the user experience of a single device, and only handle the multi-machine and multi-devices scenarios that works for data parallelism. That is, mirroring the computation graph on a single device to multiple machines and multiple devices, synchronizing model with Allreduce. For models with a huge amount of parameters such as BERT/GPT-3, users often find it not friendly to use, hard to deploy and not efficient to train models on multiple machines and multiple devices when using existing deep learning frameworks. It is also time-consuming for users to learn how to do distributed training. They also need to care about the synchronization of models between multiple machines and multiple devices. In order to solve the above problems in distributed deep learning, both industry and academia not only improve the deep learning framework itself, but also develop a variety of third-party plugins, such as NCCL, Horovod, BytePS, HugeCTR, Mesh-tensorflow, Gpipe, etc. However, it still can\u2019t meet users' unlimited pursuit to performance. The core motivation of OneFlow is to make multi-machine and multi-devices distributed training efficiently, and at the same time, to make the distributed training experience as simple as using a single device. Let's introduce the two core ideas of OneFlow, and explain how OneFlow views deep learning training in distributed scenarios. Runtime based on actor system \u00b6 Key features: Decentralized scheduling Pipelining Data movement as a first-class citizen Overlapping data movement and computation Overlapping control and data logic OneFlow consists of two stages: Compile-time and Runtime. In the Compile-time, user-defined neural networks and the requested resource are compiled into a static graph execution plan, which is composed of the description of the basic execution unit Actor ; During the runtime , each machine actually creates many Actor instances located to its own machine based on the Actor description in the Plan, and then started the Actor operating system. In the training procedure, the basic unit of OneFlow execution is Actor, which corresponds to a node of the static execution graph. The data produced and consumed between Actors are stored in the form of Registers , and the Actors cooperate through message passing. Decentralized scheduling \u00b6 OneFlow implements decentralized scheduling through the Actor mechanism. In the entire static graph is composed of actors, there is no central scheduler. Each actor only cares about the producer of the data it needs (upstream Actor) and the consumer of the data it produces (downstream Actor). In this way, in the ultra-large-scale distributed training scenario, completely decentralized scheduling can avoid the single-point performance bottleneck with centralized scheduling. Each Actor has an internal state machine, which updates its status according to the messages sent and received by the Actor. It should be noted that Register is a storage block, which stores the data produced by the Actor, and the message is a lightweight data containing the memory address of the Register storage block. It is message instead of Register that is passed between Actors, in this way, OneFlow runtime achieves zero-copy. When an Actor receives a new message and decides whether the Register it needs to consume is ready, and it has free Register to write the produced data. If yes, the Actor executes (Act) once and produces some new data. After action, the Actor sends a message to the consumer Actors who need to consume the produced Register, indicating that \"you can read the data I produced\"; At the same time, the Actor also needs to return the Register it consumes to its producer, indicating that \"I have used up your data and you can recycle it.\" The state machine inside the Actor is shown in Figure 1. Figure 1 Actor state machine inside After the Actor starts, it will switch its two states according to the messages sent and received with other actors: waiting state and execution state . The messages received by an Actor are generally divided into several types: The upstream producer Actor sends a message saying that you can read the data I produce; the downstream consumer Actor sends a message saying that I have used up the data you produced. When this data are used up by all consumers, it can be recycled as a free block and wait for the Actor to produce a new data in next time. Whenever receiving a message, an Actor will try to decides whether its action conditions are met with. There are generally two action conditions: Whether all the data to be read are available; Whether there are free blocks that can be used for production. When the action state is satisfied, the actor starts to launch its internal Kernel to consume incoming data and produce some new data. After action, the Actor will send messages to upstream and downstream: Send a message to the downstream consumer Actor saying that I just produced a piece of data, you can read them; Send a message to the upstream producer Actor saying that I just used up the data you sent me before. Actors only need to care about upstream and downstream messages to decide whether they can act or not. All Actors form a completely decentralized distributed collaborative network through their own internal state machines and messages exchanging mechanism. Pipelining \u00b6 In above, we introduced the internal finite state machine of Actors. Message passing and data movement between Actors are implemented by Register . Whether an Actor can act only relates to two conditions: Whether the Registers consumed by itself are readable; Whether the Registers produced by itself have free blocks to write. For a Register, if we allocate multiple free blocks for it, two adjacent Actors can work simultaneously. In this way, the overlapping of adjacent actors implements pipelining. In an ideal case, the initiation interval of the entire static execution graph is the execution time of the bottleneck actor's each action, the execution time of all the other actors will be hidden through the pipelining. Let's take an example to explain how the pipelining of the Actor system works. Figure 2 is an execution sequence diagram of a computation graph composed of 3 Actors (a, b, c). The green Regst square represents the Register block being occupied, and the white Regst square represents the free block of the same Register. At Time0, Actor a produces a Regst_a_0, and Actor b and Actor c are in waiting state because they have no readable Register. Here we assume that the execution time of each Actor is the same. At Time1, Actor a sends a message to Actor b saying that you can read the Regst_a_0 that I produced. Actor b receives the message and checks whether there is a free block available in the Register b owned by itself, and finds that there is an available Regst_b_0 , so Actor b executes at Time1, reading Regst_a_0 and writing Regst_b_0; at the same time, Actor a will also check whether it has a free block to write, and finds that it has a free block to write, so Actor a will also begin executing at Time1, writing Regst_a_1. (It should be noted here that Regst_a_0 and Regst_a_1 logically belong to the same Register, but they are spatially divided into different free blocks. In deep learning training task, Regst_a_0 and Regst_a_1 store data belonging to different batches produced by a same producer.) So Actor a and Actor b work in parallel. Actor c is still waiting because there is no data to read. At Time2, Actor b has produced Regst_b_0, so it sends a message to the downstream consumer Actor c that you can read the Regst_b_0 I produced, and at the same time sends a message to the upstream producer Actor a that I have consumed your Regst_a_0 . At the same time, Actor a sends a newly produced Regst_a_1 to Actor b . Actor b checks that it still has Regst_b_1 being free, so Actor b starts to read Regst_a_1 and writes Regst_b_1; Actor c receives Regst_b_0 and finds that it has Regst_c_0 being free, so Actor c starts execution, reading Regst_b_0 and writing Regst_c_0; Actor a receives Regst_a_0 that Actor b has used up and returned the ownership, and checks that all consumers of Regst_a_0 are used up, so Regst_a_0 is recycled and marked as a free block, and Actor a can continue to execute and write Regst_a_2. Figure 2 Actor producer-consumer relationship and execution sequence diagram In the above example, at Time2, Actors a , b , and c are all working simultaneously. In typical deep learning training job, Regst_b_0 and Regst_c_0 at Time2 store the data of Batch 0, and Regst_a_1 and Regst_b_1 store the data of Batch 1. Regst_a_2 stores data of Batch 2. By the design of a Register with multiple free blocks, the Actor naturally supports pipelining. Here we raise a further in-depth problem: in OneFlow, the execution of the entire data flow is like a network, and the data flow throught the network and completes the computation. How to slow down the producer's production if it is too fast for the consumer to consume, and how to avoid the case if the producer's production is too slow, and consumers get hungry. This problem involves planning for computing, memory, and transmission bandwidth, so that the bottleneck of the system is as wide as possible. It relates to flow control and resource allocation (For example, how many memory block quotas are allocated to the Register of each Actor). This is a critical problem which has been solved by the OneFlow system. Data movement as a first-class citizen \u00b6 In a distributed environment with multiple machines and multiple devices, the data movement between machines and devices is often the bottleneck affecting the horizontal scalability of the system. Only if the movement cost can be overlapped by the computation, can distributed deep learning training achieve the ideal linear speedup. Compared with other frameworks, OneFlow regards data movement as important as computation, thus proposing the idea of \"data movement is the first-class citizen\" . Most attention of the conventional frameworks is paid to computation in compile-time. The existing frameworks treat the data movement occuring implicitly behind the scenes. Therefore, the arrangement of overlapping computation and movement is ignored while performing the static analysis of the computation graph. OneFlow explicitly expresses the data movement in the computation graph and treat data movement and data computation equally important in static analysis to maximize the overlapping between data movement and computation. In runtime, data movement operations are also carried out by Actors. In addition to actors used for computation on devices, there are also Actors responsible for data movement between host memory and device memory, network Actors for network communication between machines, Actors responsible for data splitting, merging, and replication, Actors responsible for fetching and reading data from disk, and Actors responsible for loading and saving the model, etc. Many other frameworks make data loading, synchronization of model gradients, networks, model loading updates, etc. into a separate module, but in OneFlow, all such complicated functions are implemented in a static execution graph composed of Actors. The design of OneFlow is simple, elegant and efficient. Figure 3 Data movement from one device to another Figure 3 shows that, in the runtime of OneFlow, how the data are moved from the producer to the consumer on another machine if without GPU-direct. Exploit parallelism as much as possible \u00b6 In the design of OneFlow, parallelism is used as much as possible to achieve optimal distributed performance. For example, when considering the distributed training model of gradient synchronization, the transmission bandwidth between device memory and host memory is higher than the network transmission bandwidth between machines. OneFlow will perform two-level scatter and gather operations (local and between each machine) to increase locality and improve overall performance. Give another example, when OneFlow is running, the control part of user program (usually is Python) is executed in parallel with the execution graph. When necessary, OneFlow use mutually exclusive section ensure the correctness of the concurrent execution. Whether the data loader reads data from disk or is fed data from python, OneFlow ensures that it uses parallelism whenever possible, so that the computing device will not be idle due to waiting for data. If existing frameworks want to overlap data movement and computation as much as possible, they usually use multiple nested callback functions. When there are too many nesting levels, the so-called Callback Hell becomes troublesome, and the correctness and readability of code may decrease. However, in OneFlow, the above concurrency is implemented with the simple and clear Actor mechanism, which avoids the Callback Hell problem. In addition, in the multi-machine network communication, the network communication library in OneFlow not only supports the low level epoll implementation, but also naturally supports high-performance communication protocol such as RDMA. However, in most other deep learning frameworks, they use RPC for data movement in the multi-machine network communication. Compile-time based on a formal description of parallelism \u00b6 OneFlow may be the most user-friendly deep learning framework that supports data parallelism, model parallelism, and pipelining parallelism in distributed scenarios. Users only need to create a network model as if it\u2019s on a single device, and tell OneFlow which resource (machines and devices) is available. OneFlow will automatically generate an almost optimal execulation plan for the job, enabling the runtime system use these machines and devices in an efficient way. This stems from a unique design of OneFlow: Consistent View. For multi-machines and multi-devices, OneFlow will abstract it into a single super large device , which we call a logical device. The device memory of this logical device is the sum of the actual device memories of multiple physical devices, and the computing power of this logical device is also the sum of the actual computing power of multiple physical devices. The user only needs to define how the deep learning model is constructed in this logical super device, and doesn\u2019t need to worry about how OneFlow maps from the model to the physical devices. Here are two concepts: \"logical\" and \"physical\". \"Logical\" means that OneFlow abstracts the distributed computation and data into a single super-device, and \"physical\" means that the computation and data are actually deployed on various machines and devices. The deep learning model is a computation graph composed of Ops, and each Op produces and consumes some data in the form of tensor. In a multi-machine and multi-devices environment, a logical Op is mapped to multiple physical Ops. The computation actually performed by each physical Op is a part of the logical Op computation, and a logical Tensor also is mapped to multiple physical Tensors, and each physical Tensor is a part of the logical Tensor. In distributed training defined by other frameworks, each device is viewed as a \"world\", and the data or parameters are synchronized between multiple devices according to the exposed interface; In OneFlow, the involved multiple machines and multiple devices are together viewed as a \"world\". In the following, we introduce a set of Placement+SBP method for overall management of the world. Placement \u00b6 While creating the computation graph, each computation Op can be assigned an attribute called Placement, indicating on which machines and devices the logical Op will be deployed. In general data parallelism, all Ops are deployed on all devices. However, OneFlow also supports user-specified Op Placement. For example, if the network is too large for a single device to accommodate at all, OneFlow allows the first part of the network to be on one device and the second part on the other device. The devices work together like in a \"relay game\", which enables pipelining parallelism. Figure 4 shows an example of a possible Placement. The user defines a network consisting of 3 Ops: Op_0 -> Op_1 -> Op_2. In this example, the Placement of Op_0 and Op_1 is Device 0, and the Placement of Op_2 is Device 1. This is an example of pipelining parallelism. Oneflow will automatically insert the Copy Op needed for data transfer between Op_1 and Op_2. Figure 4 a placement for pipelining parallelism SBP \u00b6 SBP is a unique concept of OneFlow. It is a combination of the initials of three words: Split, Broadcast, PartialSum (taking PartialSum as an example, in fact, it can also be a reduce operation such as PartialMin, PartialMax). The full name of SBP is SbpParallel, which represents a mapping relationship between the logic Tensor and the physical Tensor. Split means that the physical Tensor is obtained by splitting the logical Tensor along a certain dimension. An axis parameter is used to indicate the dimension of the split. If multiple physical Tensors are concatenated along the dimension of Split, the logical Tensor can be restored. Broadcast indicates that each physical Tensor is exactly a copy of the logical Tensor. PartialSum indicates that although the physical Tensor has the same shape as the logical Tensor, the value in the physical Tensor is a part of the value in the corresponding position in the logical Tensor, if you add multiple physical Tensors at the same positions, you can restore the logical Tensor. Figure 5 shows a simple example of SbpParallel. Figure 5 Examples of SbpParallel SbpSignature is a collection of SbpParallels, each of which is an attribute of a specific Op. It depicts how a logical Op is mapped to multiple physical Ops on each device, and how these physical Ops treat the logical and physical mapping of their Input and Output Tensors. An Op may have multiple legal SbpSignatures. A simple legal signature is that the SbpParallel values of both input and output are Broadcast, which means that each physical Op needs the entire logical Tensor. Once the logical computation graph is constructed by the user, OneFlow generates a distributed physical execution graph by the Compiler. Among the feasible Placements of Ops and the list of legal SbpSignature of each Op, the Compile is able to find an optimal SbpSignature (such as with he minimum transmission cost) for each Op, so that the Compiler can generate the most efficient execution plan. Regarding to the list of legal SbpSignatures of an Op, we will give an example of an Op of matrix multiplication (matmul). Definition: Y = matmul(A,B) , A , B , Y are all Tensor , which means Y = AB . Then there are at least two legal SbpSignatures: 1) Y: Split(0) , A: Split(0) , B: Broadcast 2) Y: Split(1) , A: Broadcast , B: Split(1) The diagram of the two legal signatures on the two devices is shown in Figure 6. Assume that the shapes of the logical input and output Tensor of MatMul is: A(64, 10) \u00d7 B(10, 50) -> Y(64, 50) Figure 6 Two leagal SbpSignatures of MatMul , and the Op is distributed on two devices. Under the first SbpSignature, A on device 0 is the first half of logical A, A on device 1 is the second half of logical A (division along the 0 th dimension), and B on both devices is exactly the same as the logical B. The output Y from the two devices is the first half and the second half of the logical Y respectively. The second SbpSignature can also be analyzed in the same way. It should be noted that when A is data and B is model, the first SbpSignature is actually data parallelism , and the second SbpSignature is model parallelism . If there\u2019re two adjacent MatMul ops, the former uses the first SbpSignature and the latter uses the second SbpSignature, the entire network will form the so-called hybrid parallelism . Figure 7 is an example of hybrid parallelism. It defines Y0 = MatMul_0(A0, B0), Y1 = MatMul_1(Y0, B1), a computation graph composed of two ops, where A0, Y0, Y1 are data Tensor, B0 , B1 is the model Tensor. Figure 7 Hybrid parallelism In Figure 7, Y0 produced by MatMul_0 is consumed by MatMul_1, but the two ops view the SBP of the same Tensor differently. MatMul_0 considers Y0 to be a Split (axis=0) segment, but MatMul_1 needs a Broadcast Y0 input. To achieve the mathematical consistency, OneFlow will automatically insert a \"universal\" Boxing Op to do the necessary data splitting, concatenating, handling and summing operations, so that all Ops can efficiently get the data they want in a distributed environment. In data parallelism, if the Tensor in a training forward model is Broadcast, the corresponding gradient computation in the backward direction is PartialSum. When the Optimizer needs all the gradients to update the model, it will trigger the Boxing mechanism to perform efficient gradient synchronization. The most user-friendly distributed framework \u00b6 OneFlow\u2019s Placement + SBP + Boxing mechanisms allow Op and Tensor in user-defined computation graphs to be distributed on various machines and devices in any way. No matter it is data parallelism, model parallelism or pipelining parallelism, for OneFlow, it is just a combination of a specific SbpSignature under a specific Placement, which can be easily configured by the user, or handed over to OneFlow for automatic processing. In addition, before Microsoft launched the ZeRO-2 framework, OneFlow already supported similar features. In the multiple machines and multiple devices scenarios, each model Tensor is only saved on one of the devices, reducing the memory usage in gradient computations. Summary \u00b6 In summary, during the compile time, OneFlow introduces a mathematically rigorous formal system to describe all legal parallel modes, and enable the compiler to automatically search for the optimal parallel mode conveniently. At the runtime, the Actor system supports parallel and concurrent execution in an flexible and efficient way. The core of OneFlow runtime system has the advantages of simplicity, efficiency and high scalability. Based on such mechanisms, OneFlow makes the distributed training extremely efficient, and makes it as easy as training on a single device.","title":"OneFlow System Design"},{"location":"basics_topics/essentials_of_oneflow.html#oneflow-system-design","text":"In this article, we will cover these topics: Motivation OneFlow feature 1: Runtime based on actor system OneFlow feature 2: Compile-time based on a formal description of parallelism Summary","title":"OneFlow System Design"},{"location":"basics_topics/essentials_of_oneflow.html#motivation","text":"OneFlow is born for performance and horizontal scalability, especially for multi-nodes and multi-devices scenarios. We expect that users can leverage the power of multiple machines and multiple devices in a way as easy as using a single machine with single device, and enjoy the efficiency of linear speedup. Why does OneFlow focus on the performance and user experience in distributed scenarios? With the development of deep learning, the model becomes increasingly large, and the computing power required to train deep learning models will become higher and higher. The computing power and the memory of a single device are far from meeting the needs of deep learning model training, and multiple machines and multiple devices are required for parallelism speedup. If the deep learning framework can make multiple interconnected devices work well together and achieve linear speedup, even if the performance of each device is just so so, it can also meet the computing power needs of any scale. This is the so-called horizontal scalability or scaling out. We do believe this is the solution to the increasing need of computing power\u3002 However, the existing frameworks usually focus on the user experience of a single device, and only handle the multi-machine and multi-devices scenarios that works for data parallelism. That is, mirroring the computation graph on a single device to multiple machines and multiple devices, synchronizing model with Allreduce. For models with a huge amount of parameters such as BERT/GPT-3, users often find it not friendly to use, hard to deploy and not efficient to train models on multiple machines and multiple devices when using existing deep learning frameworks. It is also time-consuming for users to learn how to do distributed training. They also need to care about the synchronization of models between multiple machines and multiple devices. In order to solve the above problems in distributed deep learning, both industry and academia not only improve the deep learning framework itself, but also develop a variety of third-party plugins, such as NCCL, Horovod, BytePS, HugeCTR, Mesh-tensorflow, Gpipe, etc. However, it still can\u2019t meet users' unlimited pursuit to performance. The core motivation of OneFlow is to make multi-machine and multi-devices distributed training efficiently, and at the same time, to make the distributed training experience as simple as using a single device. Let's introduce the two core ideas of OneFlow, and explain how OneFlow views deep learning training in distributed scenarios.","title":"Motivation"},{"location":"basics_topics/essentials_of_oneflow.html#runtime-based-on-actor-system","text":"Key features: Decentralized scheduling Pipelining Data movement as a first-class citizen Overlapping data movement and computation Overlapping control and data logic OneFlow consists of two stages: Compile-time and Runtime. In the Compile-time, user-defined neural networks and the requested resource are compiled into a static graph execution plan, which is composed of the description of the basic execution unit Actor ; During the runtime , each machine actually creates many Actor instances located to its own machine based on the Actor description in the Plan, and then started the Actor operating system. In the training procedure, the basic unit of OneFlow execution is Actor, which corresponds to a node of the static execution graph. The data produced and consumed between Actors are stored in the form of Registers , and the Actors cooperate through message passing.","title":"Runtime based on actor system"},{"location":"basics_topics/essentials_of_oneflow.html#decentralized-scheduling","text":"OneFlow implements decentralized scheduling through the Actor mechanism. In the entire static graph is composed of actors, there is no central scheduler. Each actor only cares about the producer of the data it needs (upstream Actor) and the consumer of the data it produces (downstream Actor). In this way, in the ultra-large-scale distributed training scenario, completely decentralized scheduling can avoid the single-point performance bottleneck with centralized scheduling. Each Actor has an internal state machine, which updates its status according to the messages sent and received by the Actor. It should be noted that Register is a storage block, which stores the data produced by the Actor, and the message is a lightweight data containing the memory address of the Register storage block. It is message instead of Register that is passed between Actors, in this way, OneFlow runtime achieves zero-copy. When an Actor receives a new message and decides whether the Register it needs to consume is ready, and it has free Register to write the produced data. If yes, the Actor executes (Act) once and produces some new data. After action, the Actor sends a message to the consumer Actors who need to consume the produced Register, indicating that \"you can read the data I produced\"; At the same time, the Actor also needs to return the Register it consumes to its producer, indicating that \"I have used up your data and you can recycle it.\" The state machine inside the Actor is shown in Figure 1. Figure 1 Actor state machine inside After the Actor starts, it will switch its two states according to the messages sent and received with other actors: waiting state and execution state . The messages received by an Actor are generally divided into several types: The upstream producer Actor sends a message saying that you can read the data I produce; the downstream consumer Actor sends a message saying that I have used up the data you produced. When this data are used up by all consumers, it can be recycled as a free block and wait for the Actor to produce a new data in next time. Whenever receiving a message, an Actor will try to decides whether its action conditions are met with. There are generally two action conditions: Whether all the data to be read are available; Whether there are free blocks that can be used for production. When the action state is satisfied, the actor starts to launch its internal Kernel to consume incoming data and produce some new data. After action, the Actor will send messages to upstream and downstream: Send a message to the downstream consumer Actor saying that I just produced a piece of data, you can read them; Send a message to the upstream producer Actor saying that I just used up the data you sent me before. Actors only need to care about upstream and downstream messages to decide whether they can act or not. All Actors form a completely decentralized distributed collaborative network through their own internal state machines and messages exchanging mechanism.","title":"Decentralized scheduling"},{"location":"basics_topics/essentials_of_oneflow.html#pipelining","text":"In above, we introduced the internal finite state machine of Actors. Message passing and data movement between Actors are implemented by Register . Whether an Actor can act only relates to two conditions: Whether the Registers consumed by itself are readable; Whether the Registers produced by itself have free blocks to write. For a Register, if we allocate multiple free blocks for it, two adjacent Actors can work simultaneously. In this way, the overlapping of adjacent actors implements pipelining. In an ideal case, the initiation interval of the entire static execution graph is the execution time of the bottleneck actor's each action, the execution time of all the other actors will be hidden through the pipelining. Let's take an example to explain how the pipelining of the Actor system works. Figure 2 is an execution sequence diagram of a computation graph composed of 3 Actors (a, b, c). The green Regst square represents the Register block being occupied, and the white Regst square represents the free block of the same Register. At Time0, Actor a produces a Regst_a_0, and Actor b and Actor c are in waiting state because they have no readable Register. Here we assume that the execution time of each Actor is the same. At Time1, Actor a sends a message to Actor b saying that you can read the Regst_a_0 that I produced. Actor b receives the message and checks whether there is a free block available in the Register b owned by itself, and finds that there is an available Regst_b_0 , so Actor b executes at Time1, reading Regst_a_0 and writing Regst_b_0; at the same time, Actor a will also check whether it has a free block to write, and finds that it has a free block to write, so Actor a will also begin executing at Time1, writing Regst_a_1. (It should be noted here that Regst_a_0 and Regst_a_1 logically belong to the same Register, but they are spatially divided into different free blocks. In deep learning training task, Regst_a_0 and Regst_a_1 store data belonging to different batches produced by a same producer.) So Actor a and Actor b work in parallel. Actor c is still waiting because there is no data to read. At Time2, Actor b has produced Regst_b_0, so it sends a message to the downstream consumer Actor c that you can read the Regst_b_0 I produced, and at the same time sends a message to the upstream producer Actor a that I have consumed your Regst_a_0 . At the same time, Actor a sends a newly produced Regst_a_1 to Actor b . Actor b checks that it still has Regst_b_1 being free, so Actor b starts to read Regst_a_1 and writes Regst_b_1; Actor c receives Regst_b_0 and finds that it has Regst_c_0 being free, so Actor c starts execution, reading Regst_b_0 and writing Regst_c_0; Actor a receives Regst_a_0 that Actor b has used up and returned the ownership, and checks that all consumers of Regst_a_0 are used up, so Regst_a_0 is recycled and marked as a free block, and Actor a can continue to execute and write Regst_a_2. Figure 2 Actor producer-consumer relationship and execution sequence diagram In the above example, at Time2, Actors a , b , and c are all working simultaneously. In typical deep learning training job, Regst_b_0 and Regst_c_0 at Time2 store the data of Batch 0, and Regst_a_1 and Regst_b_1 store the data of Batch 1. Regst_a_2 stores data of Batch 2. By the design of a Register with multiple free blocks, the Actor naturally supports pipelining. Here we raise a further in-depth problem: in OneFlow, the execution of the entire data flow is like a network, and the data flow throught the network and completes the computation. How to slow down the producer's production if it is too fast for the consumer to consume, and how to avoid the case if the producer's production is too slow, and consumers get hungry. This problem involves planning for computing, memory, and transmission bandwidth, so that the bottleneck of the system is as wide as possible. It relates to flow control and resource allocation (For example, how many memory block quotas are allocated to the Register of each Actor). This is a critical problem which has been solved by the OneFlow system.","title":"Pipelining"},{"location":"basics_topics/essentials_of_oneflow.html#data-movement-as-a-first-class-citizen","text":"In a distributed environment with multiple machines and multiple devices, the data movement between machines and devices is often the bottleneck affecting the horizontal scalability of the system. Only if the movement cost can be overlapped by the computation, can distributed deep learning training achieve the ideal linear speedup. Compared with other frameworks, OneFlow regards data movement as important as computation, thus proposing the idea of \"data movement is the first-class citizen\" . Most attention of the conventional frameworks is paid to computation in compile-time. The existing frameworks treat the data movement occuring implicitly behind the scenes. Therefore, the arrangement of overlapping computation and movement is ignored while performing the static analysis of the computation graph. OneFlow explicitly expresses the data movement in the computation graph and treat data movement and data computation equally important in static analysis to maximize the overlapping between data movement and computation. In runtime, data movement operations are also carried out by Actors. In addition to actors used for computation on devices, there are also Actors responsible for data movement between host memory and device memory, network Actors for network communication between machines, Actors responsible for data splitting, merging, and replication, Actors responsible for fetching and reading data from disk, and Actors responsible for loading and saving the model, etc. Many other frameworks make data loading, synchronization of model gradients, networks, model loading updates, etc. into a separate module, but in OneFlow, all such complicated functions are implemented in a static execution graph composed of Actors. The design of OneFlow is simple, elegant and efficient. Figure 3 Data movement from one device to another Figure 3 shows that, in the runtime of OneFlow, how the data are moved from the producer to the consumer on another machine if without GPU-direct.","title":"Data movement as a first-class citizen"},{"location":"basics_topics/essentials_of_oneflow.html#exploit-parallelism-as-much-as-possible","text":"In the design of OneFlow, parallelism is used as much as possible to achieve optimal distributed performance. For example, when considering the distributed training model of gradient synchronization, the transmission bandwidth between device memory and host memory is higher than the network transmission bandwidth between machines. OneFlow will perform two-level scatter and gather operations (local and between each machine) to increase locality and improve overall performance. Give another example, when OneFlow is running, the control part of user program (usually is Python) is executed in parallel with the execution graph. When necessary, OneFlow use mutually exclusive section ensure the correctness of the concurrent execution. Whether the data loader reads data from disk or is fed data from python, OneFlow ensures that it uses parallelism whenever possible, so that the computing device will not be idle due to waiting for data. If existing frameworks want to overlap data movement and computation as much as possible, they usually use multiple nested callback functions. When there are too many nesting levels, the so-called Callback Hell becomes troublesome, and the correctness and readability of code may decrease. However, in OneFlow, the above concurrency is implemented with the simple and clear Actor mechanism, which avoids the Callback Hell problem. In addition, in the multi-machine network communication, the network communication library in OneFlow not only supports the low level epoll implementation, but also naturally supports high-performance communication protocol such as RDMA. However, in most other deep learning frameworks, they use RPC for data movement in the multi-machine network communication.","title":"Exploit parallelism as much as possible"},{"location":"basics_topics/essentials_of_oneflow.html#compile-time-based-on-a-formal-description-of-parallelism","text":"OneFlow may be the most user-friendly deep learning framework that supports data parallelism, model parallelism, and pipelining parallelism in distributed scenarios. Users only need to create a network model as if it\u2019s on a single device, and tell OneFlow which resource (machines and devices) is available. OneFlow will automatically generate an almost optimal execulation plan for the job, enabling the runtime system use these machines and devices in an efficient way. This stems from a unique design of OneFlow: Consistent View. For multi-machines and multi-devices, OneFlow will abstract it into a single super large device , which we call a logical device. The device memory of this logical device is the sum of the actual device memories of multiple physical devices, and the computing power of this logical device is also the sum of the actual computing power of multiple physical devices. The user only needs to define how the deep learning model is constructed in this logical super device, and doesn\u2019t need to worry about how OneFlow maps from the model to the physical devices. Here are two concepts: \"logical\" and \"physical\". \"Logical\" means that OneFlow abstracts the distributed computation and data into a single super-device, and \"physical\" means that the computation and data are actually deployed on various machines and devices. The deep learning model is a computation graph composed of Ops, and each Op produces and consumes some data in the form of tensor. In a multi-machine and multi-devices environment, a logical Op is mapped to multiple physical Ops. The computation actually performed by each physical Op is a part of the logical Op computation, and a logical Tensor also is mapped to multiple physical Tensors, and each physical Tensor is a part of the logical Tensor. In distributed training defined by other frameworks, each device is viewed as a \"world\", and the data or parameters are synchronized between multiple devices according to the exposed interface; In OneFlow, the involved multiple machines and multiple devices are together viewed as a \"world\". In the following, we introduce a set of Placement+SBP method for overall management of the world.","title":"Compile-time based on a formal description of parallelism"},{"location":"basics_topics/essentials_of_oneflow.html#placement","text":"While creating the computation graph, each computation Op can be assigned an attribute called Placement, indicating on which machines and devices the logical Op will be deployed. In general data parallelism, all Ops are deployed on all devices. However, OneFlow also supports user-specified Op Placement. For example, if the network is too large for a single device to accommodate at all, OneFlow allows the first part of the network to be on one device and the second part on the other device. The devices work together like in a \"relay game\", which enables pipelining parallelism. Figure 4 shows an example of a possible Placement. The user defines a network consisting of 3 Ops: Op_0 -> Op_1 -> Op_2. In this example, the Placement of Op_0 and Op_1 is Device 0, and the Placement of Op_2 is Device 1. This is an example of pipelining parallelism. Oneflow will automatically insert the Copy Op needed for data transfer between Op_1 and Op_2. Figure 4 a placement for pipelining parallelism","title":"Placement"},{"location":"basics_topics/essentials_of_oneflow.html#sbp","text":"SBP is a unique concept of OneFlow. It is a combination of the initials of three words: Split, Broadcast, PartialSum (taking PartialSum as an example, in fact, it can also be a reduce operation such as PartialMin, PartialMax). The full name of SBP is SbpParallel, which represents a mapping relationship between the logic Tensor and the physical Tensor. Split means that the physical Tensor is obtained by splitting the logical Tensor along a certain dimension. An axis parameter is used to indicate the dimension of the split. If multiple physical Tensors are concatenated along the dimension of Split, the logical Tensor can be restored. Broadcast indicates that each physical Tensor is exactly a copy of the logical Tensor. PartialSum indicates that although the physical Tensor has the same shape as the logical Tensor, the value in the physical Tensor is a part of the value in the corresponding position in the logical Tensor, if you add multiple physical Tensors at the same positions, you can restore the logical Tensor. Figure 5 shows a simple example of SbpParallel. Figure 5 Examples of SbpParallel SbpSignature is a collection of SbpParallels, each of which is an attribute of a specific Op. It depicts how a logical Op is mapped to multiple physical Ops on each device, and how these physical Ops treat the logical and physical mapping of their Input and Output Tensors. An Op may have multiple legal SbpSignatures. A simple legal signature is that the SbpParallel values of both input and output are Broadcast, which means that each physical Op needs the entire logical Tensor. Once the logical computation graph is constructed by the user, OneFlow generates a distributed physical execution graph by the Compiler. Among the feasible Placements of Ops and the list of legal SbpSignature of each Op, the Compile is able to find an optimal SbpSignature (such as with he minimum transmission cost) for each Op, so that the Compiler can generate the most efficient execution plan. Regarding to the list of legal SbpSignatures of an Op, we will give an example of an Op of matrix multiplication (matmul). Definition: Y = matmul(A,B) , A , B , Y are all Tensor , which means Y = AB . Then there are at least two legal SbpSignatures: 1) Y: Split(0) , A: Split(0) , B: Broadcast 2) Y: Split(1) , A: Broadcast , B: Split(1) The diagram of the two legal signatures on the two devices is shown in Figure 6. Assume that the shapes of the logical input and output Tensor of MatMul is: A(64, 10) \u00d7 B(10, 50) -> Y(64, 50) Figure 6 Two leagal SbpSignatures of MatMul , and the Op is distributed on two devices. Under the first SbpSignature, A on device 0 is the first half of logical A, A on device 1 is the second half of logical A (division along the 0 th dimension), and B on both devices is exactly the same as the logical B. The output Y from the two devices is the first half and the second half of the logical Y respectively. The second SbpSignature can also be analyzed in the same way. It should be noted that when A is data and B is model, the first SbpSignature is actually data parallelism , and the second SbpSignature is model parallelism . If there\u2019re two adjacent MatMul ops, the former uses the first SbpSignature and the latter uses the second SbpSignature, the entire network will form the so-called hybrid parallelism . Figure 7 is an example of hybrid parallelism. It defines Y0 = MatMul_0(A0, B0), Y1 = MatMul_1(Y0, B1), a computation graph composed of two ops, where A0, Y0, Y1 are data Tensor, B0 , B1 is the model Tensor. Figure 7 Hybrid parallelism In Figure 7, Y0 produced by MatMul_0 is consumed by MatMul_1, but the two ops view the SBP of the same Tensor differently. MatMul_0 considers Y0 to be a Split (axis=0) segment, but MatMul_1 needs a Broadcast Y0 input. To achieve the mathematical consistency, OneFlow will automatically insert a \"universal\" Boxing Op to do the necessary data splitting, concatenating, handling and summing operations, so that all Ops can efficiently get the data they want in a distributed environment. In data parallelism, if the Tensor in a training forward model is Broadcast, the corresponding gradient computation in the backward direction is PartialSum. When the Optimizer needs all the gradients to update the model, it will trigger the Boxing mechanism to perform efficient gradient synchronization.","title":"SBP"},{"location":"basics_topics/essentials_of_oneflow.html#the-most-user-friendly-distributed-framework","text":"OneFlow\u2019s Placement + SBP + Boxing mechanisms allow Op and Tensor in user-defined computation graphs to be distributed on various machines and devices in any way. No matter it is data parallelism, model parallelism or pipelining parallelism, for OneFlow, it is just a combination of a specific SbpSignature under a specific Placement, which can be easily configured by the user, or handed over to OneFlow for automatic processing. In addition, before Microsoft launched the ZeRO-2 framework, OneFlow already supported similar features. In the multiple machines and multiple devices scenarios, each model Tensor is only saved on one of the devices, reducing the memory usage in gradient computations.","title":"The most user-friendly distributed framework"},{"location":"basics_topics/essentials_of_oneflow.html#summary","text":"In summary, during the compile time, OneFlow introduces a mathematically rigorous formal system to describe all legal parallel modes, and enable the compiler to automatically search for the optimal parallel mode conveniently. At the runtime, the Actor system supports parallel and concurrent execution in an flexible and efficient way. The core of OneFlow runtime system has the advantages of simplicity, efficiency and high scalability. Based on such mechanisms, OneFlow makes the distributed training extremely efficient, and makes it as easy as training on a single device.","title":"Summary"},{"location":"basics_topics/model_load_save.html","text":"Loading and Saving of Model \u00b6 For loading and saving for model, the common scences is: Save the model that has been trained for a while to facilitate the next training. Save trained model for reproduction(Such as Model Serving). Strictly speaking, we save the untrained model as checkpoint or snapshot . It is different from model saving of a completed model. However in Oneflow, despite the model has been trained or not, we can use the same interface to save model. Thus, like the model \u3001 checkpoint \u3001 snapshot we see in other framework is no difference in OneFlow. In OneFlow, there are interfaces for model saving and loading under the flow.checkpoint . In this article, we will introduce: How to create model parameters How to save and load model Storage structure of OneFlow model How to finetune and extend model Use get_variable to Create/Obtain Model Parameters Object \u00b6 We can use oneflow.get_variable to create or obtain an object and this object can be used to interact with information in global job functions. When we call the interfaces of oneflow.get_all_variables and oneflow.load_variables , we can get or update the value of the object created by get_variable . Because of this feature, the object created by get_variable is used to store model parameters. In fact, there are many high level interface in OneFlow (like oneflow.layers.conv2d ) use get_variable internally to create model parameters internally. Process \u00b6 The get_variable requires a specified name as the identity of the created object. If the name value already existed in the program, then get_variable will get the existed object and return. If the name value doesn't exist in the program, get_variable will create a blob object internally and return. Use get_variable Create Object \u00b6 The signature of oneflow.get_variable is: def get_variable ( name , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , model_name = None , random_seed = None , distribute = distribute_util . broadcast (), ) The following example use get_variable to create parameters and build the network with oneflow.layers.conv2d : #... weight = flow . get_variable ( weight_name if weight_name else name_prefix + \"-weight\" , shape = weight_shape , dtype = inputs . dtype , initializer = kernel_initializer if kernel_initializer is not None else flow . constant_initializer ( 0 ), regularizer = kernel_regularizer , trainable = trainable , model_name = \"weight\" , ) output = flow . nn . conv2d ( inputs , weight , strides , padding , data_format , dilation_rate , groups = groups , name = name ) #... Initializer Setting \u00b6 In the previous sections, when we call get_variable , we specify the method of initializing the parameters by initializer . In OneFlow, we provide many initializers which can be found in oneflow . Under the static graph mechanism, we set the initializer first, and parameter initialization will be done by the OneFlow framework automatically. The initializers currently supported by OneFlow are listed below. Click it to see the details of algorithm: constant_initializer zeros_initializer ones_initializer random_uniform_initializer random_normal_initializer truncated_normal_initializer glorot_uniform_initializer glorot_normal_initializer variance_scaling_initializer kaiming_initializer xavier_normal_initializer xavier_uniform_initializer The Python Interface of OneFlow Models \u00b6 We can use the following interfaces to get or update the value of the variable object created by oneflow.get_variable in job function. oneflow.get_all_variables : Get the variable of all job functions. oneflow.load_variables : Update the variable in job function. oneflow.get_all_variables returns a dictionary whose key is the name specified when creating the variable and the value corresponding to the key is a tensor which has numpy() method to convert itself to a numpy array. For example, creating an object named myblob in job function: @flow . global_function () def job () -> tp . Numpy : ... myblob = flow . get_variable ( \"myblob\" , shape = ( 3 , 3 ), initializer = flow . random_normal_initializer () ) ... If we want to print the value of myblob , we can call: ... for epoch in range ( 20 ): ... job () all_variables = flow . get_all_variables () print ( all_variables [ \"myblob\" ] . numpy ()) ... The flow.get_all_variables gets the dictionary and all_variables[\"myblob\"].numpy() gets the myblob object then converts it to a numpy array. By contrary, we can use oneflow.load_variables to update the values of variable. The signature of oneflow.load_variables is as follows: def load_variables ( value_dict , ignore_mismatch = True ) Before call load_variables , we have to prepare a dictionary whose key is the name specified when creating variable and value is a numpy array. After passing the dictionary to load_variables , load_variables will find the variable object in the job function based on the key and update the value. For example: @flow . global_function ( type = \"predict\" ) def job () -> tp . Numpy : myblob = flow . get_variable ( \"myblob\" , shape = ( 3 , 3 ), initializer = flow . random_normal_initializer () ) return myblob myvardict = { \"myblob\" : np . ones (( 3 , 3 )) . astype ( np . float32 )} flow . load_variables ( myvardict ) print ( flow . get_all_variables ()[ \"myblob\" ] . numpy ()) Although we have chosen the random_normal_initializer initializer, flow.load_variables(myvardict) updates the value of myblob . The final output will be: [[1. 1. 1.] [1. 1. 1.] [1. 1. 1.]] Model Saving and Loading \u00b6 We can save or load the model by methods: oneflow.checkpoint.save : Save the model to the specified path. oneflow.checkpoint.get : Load a model from the specified path. The signature of save is as follows which saves the model to the path specified by path . def save ( path , var_dict = None ) If the optional parameter var_dict is not None , save will save the object specified in var_dict to the specified path . The signature of get is as follows which loads the previously saved model specified by the path . def get ( path ) It will return a dictionary that can be updated into the model using the load_variables . flow . load_variables ( flow . checkpoint . get ( save_dir )) Attention\uff1a The path specified by the save should either be empty or not existed. Otherwise save will report an error (to prevent overwriting the existed saved model) OneFlow models are stored in a specified path in a certain structure. See the storage structure of OneFlow models below for more details. Although there is no limit to the frequency of save in OneFlow. But excessive saving frequency will increase the load on resources such as disk and bandwidth. The Structure of OneFlow Saved Model \u00b6 OneFlow model are the parameters of network. For now there are no meta graph information in OneFlow model. The path to save model have many sub-directories. Each of them is corresponding to the name of job function in model. For example, we define the model in the first place: def lenet ( data , train = False ): initializer = flow . truncated_normal ( 0.1 ) conv1 = flow . layers . conv2d ( data , 32 , 5 , padding = \"SAME\" , activation = flow . nn . relu , name = \"conv1\" , kernel_initializer = initializer , ) pool1 = flow . nn . max_pool2d ( conv1 , ksize = 2 , strides = 2 , padding = \"SAME\" , name = \"pool1\" , data_format = \"NCHW\" ) conv2 = flow . layers . conv2d ( pool1 , 64 , 5 , padding = \"SAME\" , activation = flow . nn . relu , name = \"conv2\" , kernel_initializer = initializer , ) pool2 = flow . nn . max_pool2d ( conv2 , ksize = 2 , strides = 2 , padding = \"SAME\" , name = \"pool2\" , data_format = \"NCHW\" ) reshape = flow . reshape ( pool2 , [ pool2 . shape [ 0 ], - 1 ]) hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"dense1\" , ) if train : hidden = flow . nn . dropout ( hidden , rate = 0.5 , name = \"dropout\" ) return flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , name = \"dense2\" ) Assume that in the process of training, we call the following code to save model: flow . checkpoint . save ( './lenet_models_name' ) Then lenet_models_name and the subdirectories are as follows: lenet_models_name/ \u251c\u2500\u2500 conv1-bias \u2502 \u251c\u2500\u2500 meta \u2502 \u2514\u2500\u2500 out \u251c\u2500\u2500 conv1-weight \u2502 \u251c\u2500\u2500 meta \u2502 \u2514\u2500\u2500 out \u251c\u2500\u2500 conv2-bias \u2502 \u251c\u2500\u2500 meta \u2502 \u2514\u2500\u2500 out \u251c\u2500\u2500 conv2-weight \u2502 \u251c\u2500\u2500 meta \u2502 \u2514\u2500\u2500 out \u251c\u2500\u2500 dense1-bias \u2502 \u251c\u2500\u2500 meta \u2502 \u2514\u2500\u2500 out \u251c\u2500\u2500 dense1-weight \u2502 \u251c\u2500\u2500 meta \u2502 \u2514\u2500\u2500 out \u251c\u2500\u2500 dense2-bias \u2502 \u251c\u2500\u2500 meta \u2502 \u2514\u2500\u2500 out \u251c\u2500\u2500 dense2-weight \u2502 \u251c\u2500\u2500 meta \u2502 \u2514\u2500\u2500 out \u251c\u2500\u2500 snapshot_done \u2514\u2500\u2500 System-Train-TrainStep-train_job \u251c\u2500\u2500 meta \u2514\u2500\u2500 out We can see: In the network in job function, each variable is corresponding to a sub-directory. In each of the subdirectories, there are out and meta files where out stores the values of the network parameters in binary form and meta stores the network structure information in text form. Snapshot_done is an empty folder. If it exists, it means that the network training has been finished. Snapshots of the training steps is stored in System-Train-TrainStep-train_job . Model Finetune and Transfer Learning \u00b6 In model finetune and transfer learning, we always need\uff1a Load some of the parameters from original model Initialize the other part of parameters in model We can use oneflow.load_variables to complete the process above. Here is a simple example to illustrate the concept. First we need define a model and save it to ./mlp_models_1 after training: @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : with flow . scope . placement ( \"cpu\" , \"0:0\" ): initializer = flow . truncated_normal ( 0.1 ) reshape = flow . reshape ( images , [ images . shape [ 0 ], - 1 ]) hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"dense1\" , ) dense2 = flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , name = \"dense2\" ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , dense2 ) lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.1 ]) flow . optimizer . SGD ( lr_scheduler , momentum = 0 ) . minimize ( loss ) return loss Then we expand the network and add one more layer ( dense3 ) in above model: @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : with flow . scope . placement ( \"cpu\" , \"0:0\" ): #... original structure dense3 = flow . layers . dense ( dense2 , 10 , kernel_initializer = initializer , name = \"dense3\" ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , dense3 ) #... Finally, load parameters from original model and start training: if __name__ == \"__main__\" : check_point = flow . train . CheckPoint () check_point . load ( \"./mlp_models_1\" ) ( train_images , train_labels ), ( test_images , test_labels ) = flow . data . load_mnist ( BATCH_SIZE , BATCH_SIZE ) for i , ( images , labels ) in enumerate ( zip ( train_images , train_labels )): loss = train_job ( images , labels ) if i % 20 == 0 : print ( loss . mean ()) check_point . save ( \"./mlp_ext_models_1\" ) The parameters of new dense3 layer do not exist in the original model. They are automatically initialized to their values by OneFlow. Codes \u00b6 The following code is from mlp_mnist_origin.py . As the backbone network. Trained model is stored in ./mlp_models_1 . Run: wget https://docs.oneflow.org/code/basics_topics/mlp_mnist_origin.py python3 mlp_mnist_origin.py When the training is complete, you will get the mlp_models_1 in the current working directory. The following code is from mlp_mnist_finetune.py . After finetuning (add one more layer dense3 in backbone network), we load ./mlp_models_1 and train it. Run: wget https://docs.oneflow.org/code/basics_topics/mlp_mnist_finetune.py python3 mlp_mnist_finetune.py The finetuned models are saved in . /mlp_ext_models_1 .","title":"Loading and saving of model"},{"location":"basics_topics/model_load_save.html#loading-and-saving-of-model","text":"For loading and saving for model, the common scences is: Save the model that has been trained for a while to facilitate the next training. Save trained model for reproduction(Such as Model Serving). Strictly speaking, we save the untrained model as checkpoint or snapshot . It is different from model saving of a completed model. However in Oneflow, despite the model has been trained or not, we can use the same interface to save model. Thus, like the model \u3001 checkpoint \u3001 snapshot we see in other framework is no difference in OneFlow. In OneFlow, there are interfaces for model saving and loading under the flow.checkpoint . In this article, we will introduce: How to create model parameters How to save and load model Storage structure of OneFlow model How to finetune and extend model","title":"Loading and Saving of Model"},{"location":"basics_topics/model_load_save.html#use-get_variable-to-createobtain-model-parameters-object","text":"We can use oneflow.get_variable to create or obtain an object and this object can be used to interact with information in global job functions. When we call the interfaces of oneflow.get_all_variables and oneflow.load_variables , we can get or update the value of the object created by get_variable . Because of this feature, the object created by get_variable is used to store model parameters. In fact, there are many high level interface in OneFlow (like oneflow.layers.conv2d ) use get_variable internally to create model parameters internally.","title":"Use get_variable to Create/Obtain Model Parameters Object"},{"location":"basics_topics/model_load_save.html#process","text":"The get_variable requires a specified name as the identity of the created object. If the name value already existed in the program, then get_variable will get the existed object and return. If the name value doesn't exist in the program, get_variable will create a blob object internally and return.","title":"Process"},{"location":"basics_topics/model_load_save.html#use-get_variable-create-object","text":"The signature of oneflow.get_variable is: def get_variable ( name , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , model_name = None , random_seed = None , distribute = distribute_util . broadcast (), ) The following example use get_variable to create parameters and build the network with oneflow.layers.conv2d : #... weight = flow . get_variable ( weight_name if weight_name else name_prefix + \"-weight\" , shape = weight_shape , dtype = inputs . dtype , initializer = kernel_initializer if kernel_initializer is not None else flow . constant_initializer ( 0 ), regularizer = kernel_regularizer , trainable = trainable , model_name = \"weight\" , ) output = flow . nn . conv2d ( inputs , weight , strides , padding , data_format , dilation_rate , groups = groups , name = name ) #...","title":"Use get_variable Create Object"},{"location":"basics_topics/model_load_save.html#initializer-setting","text":"In the previous sections, when we call get_variable , we specify the method of initializing the parameters by initializer . In OneFlow, we provide many initializers which can be found in oneflow . Under the static graph mechanism, we set the initializer first, and parameter initialization will be done by the OneFlow framework automatically. The initializers currently supported by OneFlow are listed below. Click it to see the details of algorithm: constant_initializer zeros_initializer ones_initializer random_uniform_initializer random_normal_initializer truncated_normal_initializer glorot_uniform_initializer glorot_normal_initializer variance_scaling_initializer kaiming_initializer xavier_normal_initializer xavier_uniform_initializer","title":"Initializer Setting"},{"location":"basics_topics/model_load_save.html#the-python-interface-of-oneflow-models","text":"We can use the following interfaces to get or update the value of the variable object created by oneflow.get_variable in job function. oneflow.get_all_variables : Get the variable of all job functions. oneflow.load_variables : Update the variable in job function. oneflow.get_all_variables returns a dictionary whose key is the name specified when creating the variable and the value corresponding to the key is a tensor which has numpy() method to convert itself to a numpy array. For example, creating an object named myblob in job function: @flow . global_function () def job () -> tp . Numpy : ... myblob = flow . get_variable ( \"myblob\" , shape = ( 3 , 3 ), initializer = flow . random_normal_initializer () ) ... If we want to print the value of myblob , we can call: ... for epoch in range ( 20 ): ... job () all_variables = flow . get_all_variables () print ( all_variables [ \"myblob\" ] . numpy ()) ... The flow.get_all_variables gets the dictionary and all_variables[\"myblob\"].numpy() gets the myblob object then converts it to a numpy array. By contrary, we can use oneflow.load_variables to update the values of variable. The signature of oneflow.load_variables is as follows: def load_variables ( value_dict , ignore_mismatch = True ) Before call load_variables , we have to prepare a dictionary whose key is the name specified when creating variable and value is a numpy array. After passing the dictionary to load_variables , load_variables will find the variable object in the job function based on the key and update the value. For example: @flow . global_function ( type = \"predict\" ) def job () -> tp . Numpy : myblob = flow . get_variable ( \"myblob\" , shape = ( 3 , 3 ), initializer = flow . random_normal_initializer () ) return myblob myvardict = { \"myblob\" : np . ones (( 3 , 3 )) . astype ( np . float32 )} flow . load_variables ( myvardict ) print ( flow . get_all_variables ()[ \"myblob\" ] . numpy ()) Although we have chosen the random_normal_initializer initializer, flow.load_variables(myvardict) updates the value of myblob . The final output will be: [[1. 1. 1.] [1. 1. 1.] [1. 1. 1.]]","title":"The Python Interface of OneFlow Models"},{"location":"basics_topics/model_load_save.html#model-saving-and-loading","text":"We can save or load the model by methods: oneflow.checkpoint.save : Save the model to the specified path. oneflow.checkpoint.get : Load a model from the specified path. The signature of save is as follows which saves the model to the path specified by path . def save ( path , var_dict = None ) If the optional parameter var_dict is not None , save will save the object specified in var_dict to the specified path . The signature of get is as follows which loads the previously saved model specified by the path . def get ( path ) It will return a dictionary that can be updated into the model using the load_variables . flow . load_variables ( flow . checkpoint . get ( save_dir )) Attention\uff1a The path specified by the save should either be empty or not existed. Otherwise save will report an error (to prevent overwriting the existed saved model) OneFlow models are stored in a specified path in a certain structure. See the storage structure of OneFlow models below for more details. Although there is no limit to the frequency of save in OneFlow. But excessive saving frequency will increase the load on resources such as disk and bandwidth.","title":"Model Saving and Loading"},{"location":"basics_topics/model_load_save.html#the-structure-of-oneflow-saved-model","text":"OneFlow model are the parameters of network. For now there are no meta graph information in OneFlow model. The path to save model have many sub-directories. Each of them is corresponding to the name of job function in model. For example, we define the model in the first place: def lenet ( data , train = False ): initializer = flow . truncated_normal ( 0.1 ) conv1 = flow . layers . conv2d ( data , 32 , 5 , padding = \"SAME\" , activation = flow . nn . relu , name = \"conv1\" , kernel_initializer = initializer , ) pool1 = flow . nn . max_pool2d ( conv1 , ksize = 2 , strides = 2 , padding = \"SAME\" , name = \"pool1\" , data_format = \"NCHW\" ) conv2 = flow . layers . conv2d ( pool1 , 64 , 5 , padding = \"SAME\" , activation = flow . nn . relu , name = \"conv2\" , kernel_initializer = initializer , ) pool2 = flow . nn . max_pool2d ( conv2 , ksize = 2 , strides = 2 , padding = \"SAME\" , name = \"pool2\" , data_format = \"NCHW\" ) reshape = flow . reshape ( pool2 , [ pool2 . shape [ 0 ], - 1 ]) hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"dense1\" , ) if train : hidden = flow . nn . dropout ( hidden , rate = 0.5 , name = \"dropout\" ) return flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , name = \"dense2\" ) Assume that in the process of training, we call the following code to save model: flow . checkpoint . save ( './lenet_models_name' ) Then lenet_models_name and the subdirectories are as follows: lenet_models_name/ \u251c\u2500\u2500 conv1-bias \u2502 \u251c\u2500\u2500 meta \u2502 \u2514\u2500\u2500 out \u251c\u2500\u2500 conv1-weight \u2502 \u251c\u2500\u2500 meta \u2502 \u2514\u2500\u2500 out \u251c\u2500\u2500 conv2-bias \u2502 \u251c\u2500\u2500 meta \u2502 \u2514\u2500\u2500 out \u251c\u2500\u2500 conv2-weight \u2502 \u251c\u2500\u2500 meta \u2502 \u2514\u2500\u2500 out \u251c\u2500\u2500 dense1-bias \u2502 \u251c\u2500\u2500 meta \u2502 \u2514\u2500\u2500 out \u251c\u2500\u2500 dense1-weight \u2502 \u251c\u2500\u2500 meta \u2502 \u2514\u2500\u2500 out \u251c\u2500\u2500 dense2-bias \u2502 \u251c\u2500\u2500 meta \u2502 \u2514\u2500\u2500 out \u251c\u2500\u2500 dense2-weight \u2502 \u251c\u2500\u2500 meta \u2502 \u2514\u2500\u2500 out \u251c\u2500\u2500 snapshot_done \u2514\u2500\u2500 System-Train-TrainStep-train_job \u251c\u2500\u2500 meta \u2514\u2500\u2500 out We can see: In the network in job function, each variable is corresponding to a sub-directory. In each of the subdirectories, there are out and meta files where out stores the values of the network parameters in binary form and meta stores the network structure information in text form. Snapshot_done is an empty folder. If it exists, it means that the network training has been finished. Snapshots of the training steps is stored in System-Train-TrainStep-train_job .","title":"The Structure of OneFlow Saved Model"},{"location":"basics_topics/model_load_save.html#model-finetune-and-transfer-learning","text":"In model finetune and transfer learning, we always need\uff1a Load some of the parameters from original model Initialize the other part of parameters in model We can use oneflow.load_variables to complete the process above. Here is a simple example to illustrate the concept. First we need define a model and save it to ./mlp_models_1 after training: @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : with flow . scope . placement ( \"cpu\" , \"0:0\" ): initializer = flow . truncated_normal ( 0.1 ) reshape = flow . reshape ( images , [ images . shape [ 0 ], - 1 ]) hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"dense1\" , ) dense2 = flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , name = \"dense2\" ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , dense2 ) lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.1 ]) flow . optimizer . SGD ( lr_scheduler , momentum = 0 ) . minimize ( loss ) return loss Then we expand the network and add one more layer ( dense3 ) in above model: @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : with flow . scope . placement ( \"cpu\" , \"0:0\" ): #... original structure dense3 = flow . layers . dense ( dense2 , 10 , kernel_initializer = initializer , name = \"dense3\" ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , dense3 ) #... Finally, load parameters from original model and start training: if __name__ == \"__main__\" : check_point = flow . train . CheckPoint () check_point . load ( \"./mlp_models_1\" ) ( train_images , train_labels ), ( test_images , test_labels ) = flow . data . load_mnist ( BATCH_SIZE , BATCH_SIZE ) for i , ( images , labels ) in enumerate ( zip ( train_images , train_labels )): loss = train_job ( images , labels ) if i % 20 == 0 : print ( loss . mean ()) check_point . save ( \"./mlp_ext_models_1\" ) The parameters of new dense3 layer do not exist in the original model. They are automatically initialized to their values by OneFlow.","title":"Model Finetune and Transfer Learning"},{"location":"basics_topics/model_load_save.html#codes","text":"The following code is from mlp_mnist_origin.py . As the backbone network. Trained model is stored in ./mlp_models_1 . Run: wget https://docs.oneflow.org/code/basics_topics/mlp_mnist_origin.py python3 mlp_mnist_origin.py When the training is complete, you will get the mlp_models_1 in the current working directory. The following code is from mlp_mnist_finetune.py . After finetuning (add one more layer dense3 in backbone network), we load ./mlp_models_1 and train it. Run: wget https://docs.oneflow.org/code/basics_topics/mlp_mnist_finetune.py python3 mlp_mnist_finetune.py The finetuned models are saved in . /mlp_ext_models_1 .","title":"Codes"},{"location":"basics_topics/optimizer_in_function_config.html","text":"Configuration of Optimization Algorithms and Hyperparameters \u00b6 After a neural network model has been set up, it usually requires training before using for prediction or inference. The training process means to optimize parameters of the nerwork which are usually updated with the back propagation algorithm and a specified optimizer. In this article, we will introduce how to setup optimizers and hyperparameters in OneFlow to users. Key point summary of this article: Configuration examples of job functions for training and prediction. The use of optimizer and learning strategies. Common errors due to misconfiguration and corresponding solutions. Users can directly use the training and inferencing configurations described in Example of configutraion section without knowing the design concept of OneFlow. For more detials please refer to optimizer api Job Function Configuration \u00b6 In [Recognizing MNIST Handwritten Digits] (... /quick_start/lenet_mnist.md#global_function), we have learned about the concept of the oneflow.global_function decorator and the job function. The configuration of this article base on that. The job function can be configured by passing the function_config parameter to the decorator. If you are not familiar with oneflow.global_function , please refer to Recognizing MNIST Handwritten Digits and Job Function Definitions and Calls . Example of Configurations \u00b6 Configuration for prediction/inference \u00b6 Here we define a job function to evaluate the model: eval_job We set up the configurations of eval_job() in get_eval_config fucntion and pass it to @flow.global_function . At the same time, we set the type parameter of the @flow.global_function to \"predict\" for evaluation task. This way, OneFlow does not propagate backwards in this job function. def get_eval_config (): config = flow . function_config () config . default_data_type ( flow . float ) return config @flow . global_function ( type = \"predict\" , get_eval_config ()) def eval_job () -> tp . Numpy : # build neural network here Configuration for training \u00b6 If you specify the type parameter of @flow.global_function to be train , you can get a job function for training. In the following code, train_job is the job function used for training and it is configured with the default function_config (so there is no parameter passed to function_config ). The reason you need to specify the following settings like optimizer, learning rate and other hyperparameters in the job function is because OneFlow will back propagate for train functions. @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : with flow . scope . placement ( \"gpu\" , \"0:0\" ): logits = lenet ( images , train = True ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.1 ]) flow . optimizer . SGD ( lr_scheduler , momentum = 0 ) . minimize ( loss ) return loss In above code: PiecewiseConstantScheduler` sets the learning rate (0.1) and the learning strategy (PiecewiseConstantScheduler, a segment scaling strategy). There are other learning strategies built inside OneFlow. Such as: CosineScheduler \u3001 CustomScheduler \u3001 InverseTimeScheduler and etc. In flow.optimizer.SGD(lr_scheduler, momentum=0).minimize(loss) , set the optimizer to SGD and specify the optimization target as loss . OneFlow contains multiple optimizers such as: SGD \u3001 Adam \u3001 AdamW \u3001 LazyAdam \u3001 LARS \u3001 RMSProp . More information please refer to API documentation. FAQ \u00b6 Error Check failed: job().job_conf().train_conf().has_model_update_conf() If the type of the job function is \"train\" , but optimizer and optimization target are not configured. OneFlow will report an error during back propagation because OneFlow does not know how to update the parameters. Solution: Configure optimizer for the job function and specify the optimization target. Error Check failed: NeedBackwardOp If the type of the job function is \"predict\" but optimizer is incorrectly configured. Then optimizer cannot get the reversed data because OneFlow does not generate a reversed map for the predict job function. Solution: Remove the optimizer statement from the predict function.","title":"Optimization Algorithm and Parameter Configuration"},{"location":"basics_topics/optimizer_in_function_config.html#configuration-of-optimization-algorithms-and-hyperparameters","text":"After a neural network model has been set up, it usually requires training before using for prediction or inference. The training process means to optimize parameters of the nerwork which are usually updated with the back propagation algorithm and a specified optimizer. In this article, we will introduce how to setup optimizers and hyperparameters in OneFlow to users. Key point summary of this article: Configuration examples of job functions for training and prediction. The use of optimizer and learning strategies. Common errors due to misconfiguration and corresponding solutions. Users can directly use the training and inferencing configurations described in Example of configutraion section without knowing the design concept of OneFlow. For more detials please refer to optimizer api","title":"Configuration of Optimization Algorithms and Hyperparameters"},{"location":"basics_topics/optimizer_in_function_config.html#job-function-configuration","text":"In [Recognizing MNIST Handwritten Digits] (... /quick_start/lenet_mnist.md#global_function), we have learned about the concept of the oneflow.global_function decorator and the job function. The configuration of this article base on that. The job function can be configured by passing the function_config parameter to the decorator. If you are not familiar with oneflow.global_function , please refer to Recognizing MNIST Handwritten Digits and Job Function Definitions and Calls .","title":"Job Function Configuration"},{"location":"basics_topics/optimizer_in_function_config.html#example-of-configurations","text":"","title":"Example of Configurations"},{"location":"basics_topics/optimizer_in_function_config.html#configuration-for-predictioninference","text":"Here we define a job function to evaluate the model: eval_job We set up the configurations of eval_job() in get_eval_config fucntion and pass it to @flow.global_function . At the same time, we set the type parameter of the @flow.global_function to \"predict\" for evaluation task. This way, OneFlow does not propagate backwards in this job function. def get_eval_config (): config = flow . function_config () config . default_data_type ( flow . float ) return config @flow . global_function ( type = \"predict\" , get_eval_config ()) def eval_job () -> tp . Numpy : # build neural network here","title":"Configuration for prediction/inference"},{"location":"basics_topics/optimizer_in_function_config.html#configuration-for-training","text":"If you specify the type parameter of @flow.global_function to be train , you can get a job function for training. In the following code, train_job is the job function used for training and it is configured with the default function_config (so there is no parameter passed to function_config ). The reason you need to specify the following settings like optimizer, learning rate and other hyperparameters in the job function is because OneFlow will back propagate for train functions. @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : with flow . scope . placement ( \"gpu\" , \"0:0\" ): logits = lenet ( images , train = True ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.1 ]) flow . optimizer . SGD ( lr_scheduler , momentum = 0 ) . minimize ( loss ) return loss In above code: PiecewiseConstantScheduler` sets the learning rate (0.1) and the learning strategy (PiecewiseConstantScheduler, a segment scaling strategy). There are other learning strategies built inside OneFlow. Such as: CosineScheduler \u3001 CustomScheduler \u3001 InverseTimeScheduler and etc. In flow.optimizer.SGD(lr_scheduler, momentum=0).minimize(loss) , set the optimizer to SGD and specify the optimization target as loss . OneFlow contains multiple optimizers such as: SGD \u3001 Adam \u3001 AdamW \u3001 LazyAdam \u3001 LARS \u3001 RMSProp . More information please refer to API documentation.","title":"Configuration for training"},{"location":"basics_topics/optimizer_in_function_config.html#faq","text":"Error Check failed: job().job_conf().train_conf().has_model_update_conf() If the type of the job function is \"train\" , but optimizer and optimization target are not configured. OneFlow will report an error during back propagation because OneFlow does not know how to update the parameters. Solution: Configure optimizer for the job function and specify the optimization target. Error Check failed: NeedBackwardOp If the type of the job function is \"predict\" but optimizer is incorrectly configured. Then optimizer cannot get the reversed data because OneFlow does not generate a reversed map for the predict job function. Solution: Remove the optimizer statement from the predict function.","title":"FAQ"},{"location":"contribute/intro.html","text":"As an open source infrastructure framework with prominent competitiveness and solid foundation, OneFlow is still in rapid development and improvement. OneFlow can't grow without the friends who love open source. Every line of code you contribute, every issue you submit, every bug you report, even every question you ask, is precious to OneFlow. Grow up with OneFlow \u00b6 Contributing code, adding API docstring, fixing bugs and submiting issues, reviewing code ... OneFlow looks forward to your help in every way. You can participate in OneFlow's open source project by choosing one or more of the following, depending on your situation. Contribute code Contribute test cases Improve documentation Review pull requests from others Recommend OneFlow to people who need it Participate in OneFlow's group discussion \u2026\u2026 All the things you can think of to make OneFlow better Contribute code \u00b6 Our main open source code repository are\uff1a oneflow : It contains codes of OneFlow framework. OneFlow-Benchmark : It provides OneFlow deep learning benchmark examples for CV, CTR and NLP, and more models are on the way and will be provided here when ready. You can also find or create more OneFlow open source projects here . Contribute test cases \u00b6 OneFlow has reproduced and tested a lot of popular models, some related scripts and documents are placed in OneFlow-Benchmark repository. But there are more models that are not covered by OneFlow, we are looking forward to your help to bring them to OneFlow. We also strongly encourage using OneFlow in your own original research and sharing it in OneFlow-Benchmark repository. Improve documentation \u00b6 The API documents of OneFlow are generated by sphinx to extract the annotation and docstring from source code. The API documents project is placed in the docs folder of the oneflow repository, we can generate it by the following commands. cd oneflow/docs && make dev We are looking forward to your help in adding docstring and function annotation to source code for OneFlow Python API . Review \u00b6 We are committed to building OneFlow into a basic software product full of industrial beauty with the mentality of creating artworks. This process is also inseparable with code review from open source community. We are looking forward to your submission of more issues, pull requests and reviews in each repository. We are also looking forward to your participation.","title":"Contribute to OneFlow"},{"location":"contribute/intro.html#grow-up-with-oneflow","text":"Contributing code, adding API docstring, fixing bugs and submiting issues, reviewing code ... OneFlow looks forward to your help in every way. You can participate in OneFlow's open source project by choosing one or more of the following, depending on your situation. Contribute code Contribute test cases Improve documentation Review pull requests from others Recommend OneFlow to people who need it Participate in OneFlow's group discussion \u2026\u2026 All the things you can think of to make OneFlow better","title":"Grow up with OneFlow"},{"location":"contribute/intro.html#contribute-code","text":"Our main open source code repository are\uff1a oneflow : It contains codes of OneFlow framework. OneFlow-Benchmark : It provides OneFlow deep learning benchmark examples for CV, CTR and NLP, and more models are on the way and will be provided here when ready. You can also find or create more OneFlow open source projects here .","title":"Contribute code"},{"location":"contribute/intro.html#contribute-test-cases","text":"OneFlow has reproduced and tested a lot of popular models, some related scripts and documents are placed in OneFlow-Benchmark repository. But there are more models that are not covered by OneFlow, we are looking forward to your help to bring them to OneFlow. We also strongly encourage using OneFlow in your own original research and sharing it in OneFlow-Benchmark repository.","title":"Contribute test cases"},{"location":"contribute/intro.html#improve-documentation","text":"The API documents of OneFlow are generated by sphinx to extract the annotation and docstring from source code. The API documents project is placed in the docs folder of the oneflow repository, we can generate it by the following commands. cd oneflow/docs && make dev We are looking forward to your help in adding docstring and function annotation to source code for OneFlow Python API .","title":"Improve documentation"},{"location":"contribute/intro.html#review","text":"We are committed to building OneFlow into a basic software product full of industrial beauty with the mentality of creating artworks. This process is also inseparable with code review from open source community. We are looking forward to your submission of more issues, pull requests and reviews in each repository. We are also looking forward to your participation.","title":"Review"},{"location":"extended_topics/consistent_mirrored.html","text":"In distributed training, OneFlow provides two aspects for determining the relationship between data and model. There are consistent view and mirrored view. In this article, we will introduce: The difference and applicable scenario of data parallelism and model parallelism. The characteristics of mirrored view in distributed training. The characteristics of consistent view in distributed training. Data Parallelism and Model Parallelism. \u00b6 In order to better understand consistent and mirrored in OneFlow, we need to understand the difference between data parallelism and model parallelism in distributed training. In order to show the difference more visually, let's look at a simple Op: Matrix multiplication. We assume that during training, there is an input matrix I and the output matrix O is obtained by multiplying matrix I with another matrix W. As shown in the figure, the size of I is (N, C1), the size of W is (C1, C2) and the size of O is (N, C2). In machine learning, we can describe the matrixes as following: Matrix I is the input object, each row is a sample and each column represents the features of the sample. Matrix W represents the parameters of the model. Matrix O is the prediction result. If N in the matrix I is very large, we have large-scale samples. If C2 in matrix W is very large, it means we have a very complex model. If the scale and complexity reach a point, the single machine with a single device will not able to handle the training job. We might consider the distributed training. In a distributed training system, we can choose data parallelism and model parallelism . In order to better understand data parallelism and model parallelism, we use the following figure as the demo of matrix multiplication: The first matrix in grey on the left-hand side of the equation is the input sample. Each row is a sample. The second matrix in blue on the left-hand side of the equation is the parameter(model). In this section, we will see how the operators above are split in different ways in data parallelism and model parallelism. Data Parallelism Diagram \u00b6 In data parallelism , the sample data are divided into small parts. The divided data will send to each training node and calculate with the complete models . Finally, we combine the information in each node. As shown in the figure below: Model Parallelism Diagram \u00b6 In model parallelism , the model will be divided. Complete data will be sent to each node and calculate with the divided model . Finally, we combine the model in each node. As shown in the figure below: In conclusion: In data parallelism, each node uses the same model to train and the data is divided. In model parallelism, each node receives the same data and the model is divided. We will introduce two parallelism strategies in OneFlow ( mirrored and consistent ) and learn how to choose different parallelism methods in different strategies. Two Types of Placeholder \u00b6 In use OneFlow build neural network and The Definition and Call of Job Function , we have already introduced the concept of Placeholder and Blob . Actually, in the view of parallelism, the Placeholder of OneFlow can be divided into two types: Use oneflow.typing.Numpy.Placeholder and oneflow.typing.ListNumpy.Placeholder to construct the placeholder, which is corresponding to Consistent and Mirrored . We will explain them in detail in the examples below. Using Mirrored View in OneFlow \u00b6 Other frameworks like TensorFlow or Pytorch support mirrored view . The mirrored view of OneFlow is similar to them. In mirrored view, the model are copied in each GPU, the graph building on each node is the same, thus we can only use data parallelism . In OneFlow, the default strategy is consistent view, so you should use default_logical_view of flow.function_config() to define: func_config = flow . function_config () func_config . default_logical_view ( flow . scope . mirrored_view ()) In mirrored_view , we can only use data parallelism . When we call the job function, we need to divide the data evenly according to the amount of the devices and put the data after dividing it into a list . Every element in the list is the data to send to each device . The return value type of job function is oneflow.typing.ListNumpy . Every element in the list is corresponding to the results of each device. Concat all elements in the list can make a complete batch. Code \u00b6 In the following code, we use mirrored_view with two devices to train. Complete Code: mirrored_strategy.py We will explain the key part of the code in detail in the following \"code explanation\" section. Code explanation \u00b6 In the above code: Use flow.config.gpu_device_num to set device amount as 2. flow . config . gpu_device_num ( 2 ) oneflow.typing.ListNumpy.Placeholder defined the sample amount which is divided. And the relationship between BATCH_SIZE_PER_GPU and BATCH_SIZE is BATCH_SIZE=BATCH_SIZE_PER_GPU\u00d7GPU_NUM . def train_job ( images : tp . ListNumpy . Placeholder (( BATCH_SIZE_PER_GPU , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . ListNumpy . Placeholder (( BATCH_SIZE_PER_GPU ,), dtype = flow . int32 ), ) -> tp . ListNumpy : The data after dividing need to be stored in the list and passed to training functions. The number of elements in list need to be same as the amount of devices in the training process . The i-th element in list will be sent to the i-th device: images1 = images [: BATCH_SIZE_PER_GPU ] images2 = images [ BATCH_SIZE_PER_GPU :] labels1 = labels [: BATCH_SIZE_PER_GPU ] labels2 = labels [ BATCH_SIZE_PER_GPU :] imgs_list = [ images1 , images2 ] labels_list = [ labels1 , labels2 ] loss = train_job ( imgs_list , labels_list ) The return result loss is a list , the number of elements in this list need to be same as the amount of devices in the training process . Then we concat them and print the total_loss . total_loss = np . array ([ * loss [ 0 ], * loss [ 1 ]]) if i % 20 == 0 : print ( total_loss . mean ()) Use consistent view in OneFlow \u00b6 We have already learned about the mirrored view, where samples will be distributed evenly while the models are the same in every device, and the results of each node need to be assembled to get the complete batch. In addition to mirrored view, OneFlow also provides consistent view. Consistent view is one of the features of OneFlow. Compared with mirrored view, it has a great advantage. OneFlow will use consistent view as default. We can declare it explicitly as the following code. config = flow . function_config () config . default_distribute_strategy ( flow . scope . consistent_view ()) The reason why consistent view is the main feature of OneFlow is that in OneFlow design, if we use consistent_view , multiple devices in a distributed system can get consistently in logic level from user's point of view. We use matrix multiplication as an example in the beginning of article, we only need focus on matrix multiplication itself on mathematics level. But in project, the issue of how to config and use model parallelism or data parallelism can be easily done in OneFlow. OneFlow will handle The data division in data parallelism , model division in model parallelism and serial logic issue quickly and efficiently. In consistent view of OneFlow, we can choose model parallelism, data parallelism, or hybrid parallelism freely. Code Example \u00b6 In the following code, we use consistent view and use two devices to train. The default parallelism method is data parallelism in consistent view. The issue of how to set model parallelism and hybrid parallelism in consistent view will be discussed in parallels features of OneFlow . Complete code: consistent_strategy.py Code explanation \u00b6 In above code: Use flow.config.gpu_device_num to set the amount of devices: flow . config . gpu_device_num ( 2 ) Use tp.Numpy.Placeholder to define the placeholder in consistent view. Because the blob of Numpy.Placeholder represent the op and placeholder in logic. Thus. the BATCH_SIZE is the sum of samples, without artificial split or combination @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : The job function is called directly to obtain the training results. The splitting and concatenating in the distributed training are completed automatically by OneFlow. In the consistent view, there are few differences between the single-machine training program and the distributed training program. for i , ( images , labels ) in enumerate ( zip ( train_images , train_labels )): loss = train_job ( images , labels ) if i % 20 == 0 : print ( loss . mean ()) More extension \u00b6 With the development of machine learning theory and practice, there are many models unable to train in a single device or by data parallelism only. Adopting consistent view in OneFlow, the above problems can be solved well through free selection and combination of parallel methods. We will introduce in parallel features of OneFlow .","title":"Consistent & Mirrored View"},{"location":"extended_topics/consistent_mirrored.html#data-parallelism-and-model-parallelism","text":"In order to better understand consistent and mirrored in OneFlow, we need to understand the difference between data parallelism and model parallelism in distributed training. In order to show the difference more visually, let's look at a simple Op: Matrix multiplication. We assume that during training, there is an input matrix I and the output matrix O is obtained by multiplying matrix I with another matrix W. As shown in the figure, the size of I is (N, C1), the size of W is (C1, C2) and the size of O is (N, C2). In machine learning, we can describe the matrixes as following: Matrix I is the input object, each row is a sample and each column represents the features of the sample. Matrix W represents the parameters of the model. Matrix O is the prediction result. If N in the matrix I is very large, we have large-scale samples. If C2 in matrix W is very large, it means we have a very complex model. If the scale and complexity reach a point, the single machine with a single device will not able to handle the training job. We might consider the distributed training. In a distributed training system, we can choose data parallelism and model parallelism . In order to better understand data parallelism and model parallelism, we use the following figure as the demo of matrix multiplication: The first matrix in grey on the left-hand side of the equation is the input sample. Each row is a sample. The second matrix in blue on the left-hand side of the equation is the parameter(model). In this section, we will see how the operators above are split in different ways in data parallelism and model parallelism.","title":"Data Parallelism and Model Parallelism."},{"location":"extended_topics/consistent_mirrored.html#data-parallelism-diagram","text":"In data parallelism , the sample data are divided into small parts. The divided data will send to each training node and calculate with the complete models . Finally, we combine the information in each node. As shown in the figure below:","title":"Data Parallelism Diagram"},{"location":"extended_topics/consistent_mirrored.html#model-parallelism-diagram","text":"In model parallelism , the model will be divided. Complete data will be sent to each node and calculate with the divided model . Finally, we combine the model in each node. As shown in the figure below: In conclusion: In data parallelism, each node uses the same model to train and the data is divided. In model parallelism, each node receives the same data and the model is divided. We will introduce two parallelism strategies in OneFlow ( mirrored and consistent ) and learn how to choose different parallelism methods in different strategies.","title":"Model Parallelism Diagram"},{"location":"extended_topics/consistent_mirrored.html#two-types-of-placeholder","text":"In use OneFlow build neural network and The Definition and Call of Job Function , we have already introduced the concept of Placeholder and Blob . Actually, in the view of parallelism, the Placeholder of OneFlow can be divided into two types: Use oneflow.typing.Numpy.Placeholder and oneflow.typing.ListNumpy.Placeholder to construct the placeholder, which is corresponding to Consistent and Mirrored . We will explain them in detail in the examples below.","title":"Two Types of Placeholder"},{"location":"extended_topics/consistent_mirrored.html#using-mirrored-view-in-oneflow","text":"Other frameworks like TensorFlow or Pytorch support mirrored view . The mirrored view of OneFlow is similar to them. In mirrored view, the model are copied in each GPU, the graph building on each node is the same, thus we can only use data parallelism . In OneFlow, the default strategy is consistent view, so you should use default_logical_view of flow.function_config() to define: func_config = flow . function_config () func_config . default_logical_view ( flow . scope . mirrored_view ()) In mirrored_view , we can only use data parallelism . When we call the job function, we need to divide the data evenly according to the amount of the devices and put the data after dividing it into a list . Every element in the list is the data to send to each device . The return value type of job function is oneflow.typing.ListNumpy . Every element in the list is corresponding to the results of each device. Concat all elements in the list can make a complete batch.","title":"Using Mirrored View in OneFlow"},{"location":"extended_topics/consistent_mirrored.html#code","text":"In the following code, we use mirrored_view with two devices to train. Complete Code: mirrored_strategy.py We will explain the key part of the code in detail in the following \"code explanation\" section.","title":"Code"},{"location":"extended_topics/consistent_mirrored.html#code-explanation","text":"In the above code: Use flow.config.gpu_device_num to set device amount as 2. flow . config . gpu_device_num ( 2 ) oneflow.typing.ListNumpy.Placeholder defined the sample amount which is divided. And the relationship between BATCH_SIZE_PER_GPU and BATCH_SIZE is BATCH_SIZE=BATCH_SIZE_PER_GPU\u00d7GPU_NUM . def train_job ( images : tp . ListNumpy . Placeholder (( BATCH_SIZE_PER_GPU , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . ListNumpy . Placeholder (( BATCH_SIZE_PER_GPU ,), dtype = flow . int32 ), ) -> tp . ListNumpy : The data after dividing need to be stored in the list and passed to training functions. The number of elements in list need to be same as the amount of devices in the training process . The i-th element in list will be sent to the i-th device: images1 = images [: BATCH_SIZE_PER_GPU ] images2 = images [ BATCH_SIZE_PER_GPU :] labels1 = labels [: BATCH_SIZE_PER_GPU ] labels2 = labels [ BATCH_SIZE_PER_GPU :] imgs_list = [ images1 , images2 ] labels_list = [ labels1 , labels2 ] loss = train_job ( imgs_list , labels_list ) The return result loss is a list , the number of elements in this list need to be same as the amount of devices in the training process . Then we concat them and print the total_loss . total_loss = np . array ([ * loss [ 0 ], * loss [ 1 ]]) if i % 20 == 0 : print ( total_loss . mean ())","title":"Code explanation"},{"location":"extended_topics/consistent_mirrored.html#use-consistent-view-in-oneflow","text":"We have already learned about the mirrored view, where samples will be distributed evenly while the models are the same in every device, and the results of each node need to be assembled to get the complete batch. In addition to mirrored view, OneFlow also provides consistent view. Consistent view is one of the features of OneFlow. Compared with mirrored view, it has a great advantage. OneFlow will use consistent view as default. We can declare it explicitly as the following code. config = flow . function_config () config . default_distribute_strategy ( flow . scope . consistent_view ()) The reason why consistent view is the main feature of OneFlow is that in OneFlow design, if we use consistent_view , multiple devices in a distributed system can get consistently in logic level from user's point of view. We use matrix multiplication as an example in the beginning of article, we only need focus on matrix multiplication itself on mathematics level. But in project, the issue of how to config and use model parallelism or data parallelism can be easily done in OneFlow. OneFlow will handle The data division in data parallelism , model division in model parallelism and serial logic issue quickly and efficiently. In consistent view of OneFlow, we can choose model parallelism, data parallelism, or hybrid parallelism freely.","title":"Use consistent view in OneFlow"},{"location":"extended_topics/consistent_mirrored.html#code-example","text":"In the following code, we use consistent view and use two devices to train. The default parallelism method is data parallelism in consistent view. The issue of how to set model parallelism and hybrid parallelism in consistent view will be discussed in parallels features of OneFlow . Complete code: consistent_strategy.py","title":"Code Example"},{"location":"extended_topics/consistent_mirrored.html#code-explanation_1","text":"In above code: Use flow.config.gpu_device_num to set the amount of devices: flow . config . gpu_device_num ( 2 ) Use tp.Numpy.Placeholder to define the placeholder in consistent view. Because the blob of Numpy.Placeholder represent the op and placeholder in logic. Thus. the BATCH_SIZE is the sum of samples, without artificial split or combination @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : The job function is called directly to obtain the training results. The splitting and concatenating in the distributed training are completed automatically by OneFlow. In the consistent view, there are few differences between the single-machine training program and the distributed training program. for i , ( images , labels ) in enumerate ( zip ( train_images , train_labels )): loss = train_job ( images , labels ) if i % 20 == 0 : print ( loss . mean ())","title":"Code explanation"},{"location":"extended_topics/consistent_mirrored.html#more-extension","text":"With the development of machine learning theory and practice, there are many models unable to train in a single device or by data parallelism only. Adopting consistent view in OneFlow, the above problems can be solved well through free selection and combination of parallel methods. We will introduce in parallel features of OneFlow .","title":"More extension"},{"location":"extended_topics/debug_by_vscode.html","text":"This article describes how to configure VS Code to build OneFlow GUI development environment. If you are not familiar with VS code please refer to official documentation . This article covers: How to compile the Debug version of OneFlow. The necessary extensions of VS code along with installing guidelines. Compile the Debug version of OneFlow. \u00b6 If we use the Release version of OneFlow, we may have problems with debugging because of the compiling optimization, and actual running position may not correspond to the source line. Thus, we need to compile Debug version of OneFlow and generate the json file needed by clangd. When we run cmake, we need add flag of Debug and CMAKE_EXPORT_COMPILE_COMMANDS . cmake .. \\ -DCMAKE_BUILD_TYPE=Debug \\ -DCMAKE_EXPORT_COMPILE_COMMANDS=1 -DCMAKE_BUILD_TYPE=Debug choose the version of Debug. -DCMAKE_EXPORT_COMPILE_COMMANDS will generate a file named compile_commands.json in the build folder. The json file is required by clangd and we will configure it later. Remote - SSH \u00b6 This section is intended only for those who need to develop remotely. For those developer developing on local hosts ** may skip this section**. By the extension \"Remote SSH\" of VS Code, we can connect to a remote server through SSH. With the help of \"Remote SSH\", we can attach to OneFlow running on a remote server and debug OneFlow as if we are debugging a local program. After installing the extension \"Remote - SSH\", press F1 and select Remote-SSH: Connect to Host.. in the pop-up search bar. After that,we can set the SSH connection configuration and connect to the remote host. After connected to the remote host, the extensions window will be divided to \"remote\" and \"local\" automatically. If a extension that needs to be installed on the remote host is detected, it is grayed out with a button Install in SSH: remote server name . Click it to install the corresponding extension on the remote host. As shown in the figure above, we have installed python, clangd and native debug extensions on the remote host to support remote debugging of OneFlow. But the extensions Go, HTML CSS Support are not installed remotely. clangd \u00b6 After some simple configuration, clangd can provide us with code completion, symbol jump and other convenience. Followings are required before we configure clangd: We have already compiled OneFlow and generated compile_commands.json file. We have already installed clangd on remote host through \"Remote - SSH\". It is NOT recommended that install the extension \"ms-vscode.cpptools C/C++\" which is recommended by VS Code. Because it conflicts with clangd. Configure clangd in VS code \u00b6 Create a soft link to the compile_commands.json in \"build\" dictionary in the source root of OneFlow. We need to change to the directory of OneFlow's source root and run the command below: ln -s ./build/compile_commands.json compile_commands.json Then press Ctrl+Shift+P ( command+shift+p on MacOS) to find the Open Remote Settings option and open the settings.json file and add the following configuration: \"clangd.path\" : \"/path/to/bin/clangd\" , \"clangd.arguments\" : [ \"-j\" , \"12\" , \"-clang-tidy\" ] The meaning of clangd.arguments and more options can be found by clangd --help . Using clangd \u00b6 In View->Output panel of VS code, we can choose \"Clang Language Server\" in dropdown list and then we will see parsing output of clangd. After that, VS Code can jumps between symbols of C/C++. Press Ctrl+P ( command+P on MacOS), and then through @symbols name or #symbols name we can find the symbols in current file or in project scope respectively. native debug \u00b6 Press Ctrl + Shift + D ( command+shift+D on MacOS) or click the Run button on activity bar can switch VS Code to the view of Run. And then we choose Create a launch.json file first and next choose gdb template. And then we can set the options: { \"version\" : \"0.2.0\" , \"configurations\" : [ { \"name\" : \"lenet\" , //defined job name \"type\" : \"gdb\" , \"request\" : \"launch\" , \"target\" : \"/home/yaochi/.conda/envs/ycof/bin/python3\" , //python path \"arguments\" : \"lenet_train.py\" , //script \"cwd\" : \"/home/yaochi/of_example\" , //script path \"valuesFormatting\" : \"parseText\" } ] } After we set the breakpoint, we can press F5 to start debugging. Others: \u00b6 If the download speed is too slow in VS Code, you can refer to offcial document for changing hostname or setting proxy. The official introduction about install of clangd. The official introduction about configuration of VS Code. The latest version of clangd may have special requirements of glibc. That may lead to raise some errors on missing libraries. ./bin/clangd: /lib64/libc.so.6: version `GLIBC_2.18' not found (required by ./bin/clangd) We can download the older version of clangd (The recommended version of this article is 9.0.0). Older version of clangd is available on LLVM official site . Download the LLVM tools package with clangd inside.","title":"Use VS Code to Debug OneFlow"},{"location":"extended_topics/debug_by_vscode.html#compile-the-debug-version-of-oneflow","text":"If we use the Release version of OneFlow, we may have problems with debugging because of the compiling optimization, and actual running position may not correspond to the source line. Thus, we need to compile Debug version of OneFlow and generate the json file needed by clangd. When we run cmake, we need add flag of Debug and CMAKE_EXPORT_COMPILE_COMMANDS . cmake .. \\ -DCMAKE_BUILD_TYPE=Debug \\ -DCMAKE_EXPORT_COMPILE_COMMANDS=1 -DCMAKE_BUILD_TYPE=Debug choose the version of Debug. -DCMAKE_EXPORT_COMPILE_COMMANDS will generate a file named compile_commands.json in the build folder. The json file is required by clangd and we will configure it later.","title":"Compile the Debug version of OneFlow."},{"location":"extended_topics/debug_by_vscode.html#remote-ssh","text":"This section is intended only for those who need to develop remotely. For those developer developing on local hosts ** may skip this section**. By the extension \"Remote SSH\" of VS Code, we can connect to a remote server through SSH. With the help of \"Remote SSH\", we can attach to OneFlow running on a remote server and debug OneFlow as if we are debugging a local program. After installing the extension \"Remote - SSH\", press F1 and select Remote-SSH: Connect to Host.. in the pop-up search bar. After that,we can set the SSH connection configuration and connect to the remote host. After connected to the remote host, the extensions window will be divided to \"remote\" and \"local\" automatically. If a extension that needs to be installed on the remote host is detected, it is grayed out with a button Install in SSH: remote server name . Click it to install the corresponding extension on the remote host. As shown in the figure above, we have installed python, clangd and native debug extensions on the remote host to support remote debugging of OneFlow. But the extensions Go, HTML CSS Support are not installed remotely.","title":"Remote - SSH"},{"location":"extended_topics/debug_by_vscode.html#clangd","text":"After some simple configuration, clangd can provide us with code completion, symbol jump and other convenience. Followings are required before we configure clangd: We have already compiled OneFlow and generated compile_commands.json file. We have already installed clangd on remote host through \"Remote - SSH\". It is NOT recommended that install the extension \"ms-vscode.cpptools C/C++\" which is recommended by VS Code. Because it conflicts with clangd.","title":"clangd"},{"location":"extended_topics/debug_by_vscode.html#configure-clangd-in-vs-code","text":"Create a soft link to the compile_commands.json in \"build\" dictionary in the source root of OneFlow. We need to change to the directory of OneFlow's source root and run the command below: ln -s ./build/compile_commands.json compile_commands.json Then press Ctrl+Shift+P ( command+shift+p on MacOS) to find the Open Remote Settings option and open the settings.json file and add the following configuration: \"clangd.path\" : \"/path/to/bin/clangd\" , \"clangd.arguments\" : [ \"-j\" , \"12\" , \"-clang-tidy\" ] The meaning of clangd.arguments and more options can be found by clangd --help .","title":"Configure clangd in VS code"},{"location":"extended_topics/debug_by_vscode.html#using-clangd","text":"In View->Output panel of VS code, we can choose \"Clang Language Server\" in dropdown list and then we will see parsing output of clangd. After that, VS Code can jumps between symbols of C/C++. Press Ctrl+P ( command+P on MacOS), and then through @symbols name or #symbols name we can find the symbols in current file or in project scope respectively.","title":"Using clangd"},{"location":"extended_topics/debug_by_vscode.html#native-debug","text":"Press Ctrl + Shift + D ( command+shift+D on MacOS) or click the Run button on activity bar can switch VS Code to the view of Run. And then we choose Create a launch.json file first and next choose gdb template. And then we can set the options: { \"version\" : \"0.2.0\" , \"configurations\" : [ { \"name\" : \"lenet\" , //defined job name \"type\" : \"gdb\" , \"request\" : \"launch\" , \"target\" : \"/home/yaochi/.conda/envs/ycof/bin/python3\" , //python path \"arguments\" : \"lenet_train.py\" , //script \"cwd\" : \"/home/yaochi/of_example\" , //script path \"valuesFormatting\" : \"parseText\" } ] } After we set the breakpoint, we can press F5 to start debugging.","title":"native debug"},{"location":"extended_topics/debug_by_vscode.html#others","text":"If the download speed is too slow in VS Code, you can refer to offcial document for changing hostname or setting proxy. The official introduction about install of clangd. The official introduction about configuration of VS Code. The latest version of clangd may have special requirements of glibc. That may lead to raise some errors on missing libraries. ./bin/clangd: /lib64/libc.so.6: version `GLIBC_2.18' not found (required by ./bin/clangd) We can download the older version of clangd (The recommended version of this article is 9.0.0). Older version of clangd is available on LLVM official site . Download the LLVM tools package with clangd inside.","title":"Others:"},{"location":"extended_topics/how_to_convert_image_to_ofrecord.html","text":"Convert Image Files to OFRecord Datasets \u00b6 In OFRecord Data Format and Loading and Preparing OFRecord Dataset , we learned how to convert other dataset formats to OFRecord separately and how to load OFRecord datasets. In this article, we will explain how to make image files into OFRecord datasets. Also we provide relevant script for users to use directly or make modification base on that, which includes: Make OFRecord datasets based on MNIST dataset. How OFRecord Reader is encoded. Training on OFRecord dataset. Make OFRecord Datasets Based on Image Files \u00b6 We use MNIST Handwritten Digits dataset to produce an OFRecord format file. we only take 50 pictures for demonstration. Please refer to img2ofrecord for relevant script and dataset. img2ofrecord . Download and unzip the relevant zip file $ wget https://oneflow-static.oss-cn-beijing.aliyuncs.com/oneflow-tutorial-attachments/img2ofrecord.zip $ unzip img2ofrecord.zip Change directory to corresponding path and run OFRecord production script img2ofrecord.py $ cd ./img_to_ofrecord $ python img2ofrecord.py --part_num=5 --save_dir=./dataset/ --img_format=.png --image_root=./images/train_set/ The following output will display as the script runs. The image root is: ./images/train_set/ The amount of OFRecord data part is: 5 The directory of Labels is: ./images/train_label/label.txt The image format is: .png The OFRecord save directory is: ./dataset/ Start Processing...... ./images/train_set/00000030_3.png feature saved ./images/train_set/00000034_0.png feature saved ./images/train_set/00000026_4.png feature saved ./images/train_set/00000043_9.png feature saved ...... Process image successfully !!! Thus far, we have created the OFRecord file and saved it under ./dataset . Code Explanation \u00b6 The hierarchy of code directory is: img_to_ofrecord \u251c\u2500\u2500 images \u251c\u2500\u2500 train_set \u251c\u2500\u2500 00000000_5.png \u251c\u2500\u2500 00000001_0.png \u251c\u2500\u2500 00000002_4.png ...... \u251c\u2500\u2500 train_label \u251c\u2500\u2500 label.txt \u251c\u2500\u2500 img2ofrecord.py \u251c\u2500\u2500 lenet_train.py images directory holds the original training dataset and label file. The label file is stored as json here in following format\uff1a {\"00000030_3.png\": 3} {\"00000034_0.png\": 0} {\"00000026_4.png\": 4} {\"00000043_9.png\": 9} {\"00000047_5.png\": 5} {\"00000003_1.png\": 1} ...... img2ofrecord.py is the script which converts image files in train_set to OFRecord dataset. lenet_train.py is the script loading OFRecord we just made for training. The command options of img2ofrecord.py are: - image_root specify the root directory of the image. - part_num specify the number of OFRecord files to generate. An error is reported if the number is greater than the total number of images. - label_dir specify the directory of the label. - img_format specify the format of the image. - save_dir specify the directory where the OFRecord file will be saved. How OFRecord Reader is Encoded \u00b6 The code associated with the encoding of OFRecord files is in img2ofrecord.py . The encoding process is as follows\uff1a First, encoding the incoming image data. def encode_img_file ( filename , ext = \".jpg\" ): img = cv2 . imread ( filename ) encoded_data = cv2 . imencode ( ext , img )[ 1 ] return encoded_data . tostring () The ext is the image encoding format. Currently, The format supported by ONEFLOW image encoding and decoding is consistent with that of OpenCV, which can be refered in cv::ImwriteFlags for details. JPEG, one of the most common lossy code formats. Please refer to JPEG . PNG, a common lossless bitmap encoding format. Please refer to Portable Network Graphics . TIFF, a extensible compressed encoding format. Please refer to Tagged Image File Format . Second, data is converted to the form of Feature, serialized, and the data length is written to the file. def ndarray2ofrecords ( dsfile , dataname , encoded_data , labelname , encoded_label ): topack = { dataname : bytes_feature ( encoded_data ), labelname : int32_feature ( encoded_label )} ofrecord_features = ofrecord . OFRecord ( feature = topack ) serilizedBytes = ofrecord_features . SerializeToString () length = ofrecord_features . ByteSize () dsfile . write ( struct . pack ( \"q\" , length )) dsfile . write ( serilizedBytes ) Training on OFRecord Dataset \u00b6 We run lenet_train.py . It will read the OFRecord dataset that we have just created and train it on the LeNet model. The outputs of training script should like below: [6.778578] [2.0212684] [1.3814741] [0.47514156] [0.13277876] [0.16388433] [0.03788032] [0.01225162] ...... At this point, we have successfully completed the whole process of dataset production, reading and training.","title":"Convert Image Files to OFRecord Datasets"},{"location":"extended_topics/how_to_convert_image_to_ofrecord.html#convert-image-files-to-ofrecord-datasets","text":"In OFRecord Data Format and Loading and Preparing OFRecord Dataset , we learned how to convert other dataset formats to OFRecord separately and how to load OFRecord datasets. In this article, we will explain how to make image files into OFRecord datasets. Also we provide relevant script for users to use directly or make modification base on that, which includes: Make OFRecord datasets based on MNIST dataset. How OFRecord Reader is encoded. Training on OFRecord dataset.","title":"Convert Image Files to OFRecord Datasets"},{"location":"extended_topics/how_to_convert_image_to_ofrecord.html#make-ofrecord-datasets-based-on-image-files","text":"We use MNIST Handwritten Digits dataset to produce an OFRecord format file. we only take 50 pictures for demonstration. Please refer to img2ofrecord for relevant script and dataset. img2ofrecord . Download and unzip the relevant zip file $ wget https://oneflow-static.oss-cn-beijing.aliyuncs.com/oneflow-tutorial-attachments/img2ofrecord.zip $ unzip img2ofrecord.zip Change directory to corresponding path and run OFRecord production script img2ofrecord.py $ cd ./img_to_ofrecord $ python img2ofrecord.py --part_num=5 --save_dir=./dataset/ --img_format=.png --image_root=./images/train_set/ The following output will display as the script runs. The image root is: ./images/train_set/ The amount of OFRecord data part is: 5 The directory of Labels is: ./images/train_label/label.txt The image format is: .png The OFRecord save directory is: ./dataset/ Start Processing...... ./images/train_set/00000030_3.png feature saved ./images/train_set/00000034_0.png feature saved ./images/train_set/00000026_4.png feature saved ./images/train_set/00000043_9.png feature saved ...... Process image successfully !!! Thus far, we have created the OFRecord file and saved it under ./dataset .","title":"Make OFRecord Datasets Based on Image Files"},{"location":"extended_topics/how_to_convert_image_to_ofrecord.html#code-explanation","text":"The hierarchy of code directory is: img_to_ofrecord \u251c\u2500\u2500 images \u251c\u2500\u2500 train_set \u251c\u2500\u2500 00000000_5.png \u251c\u2500\u2500 00000001_0.png \u251c\u2500\u2500 00000002_4.png ...... \u251c\u2500\u2500 train_label \u251c\u2500\u2500 label.txt \u251c\u2500\u2500 img2ofrecord.py \u251c\u2500\u2500 lenet_train.py images directory holds the original training dataset and label file. The label file is stored as json here in following format\uff1a {\"00000030_3.png\": 3} {\"00000034_0.png\": 0} {\"00000026_4.png\": 4} {\"00000043_9.png\": 9} {\"00000047_5.png\": 5} {\"00000003_1.png\": 1} ...... img2ofrecord.py is the script which converts image files in train_set to OFRecord dataset. lenet_train.py is the script loading OFRecord we just made for training. The command options of img2ofrecord.py are: - image_root specify the root directory of the image. - part_num specify the number of OFRecord files to generate. An error is reported if the number is greater than the total number of images. - label_dir specify the directory of the label. - img_format specify the format of the image. - save_dir specify the directory where the OFRecord file will be saved.","title":"Code Explanation"},{"location":"extended_topics/how_to_convert_image_to_ofrecord.html#how-ofrecord-reader-is-encoded","text":"The code associated with the encoding of OFRecord files is in img2ofrecord.py . The encoding process is as follows\uff1a First, encoding the incoming image data. def encode_img_file ( filename , ext = \".jpg\" ): img = cv2 . imread ( filename ) encoded_data = cv2 . imencode ( ext , img )[ 1 ] return encoded_data . tostring () The ext is the image encoding format. Currently, The format supported by ONEFLOW image encoding and decoding is consistent with that of OpenCV, which can be refered in cv::ImwriteFlags for details. JPEG, one of the most common lossy code formats. Please refer to JPEG . PNG, a common lossless bitmap encoding format. Please refer to Portable Network Graphics . TIFF, a extensible compressed encoding format. Please refer to Tagged Image File Format . Second, data is converted to the form of Feature, serialized, and the data length is written to the file. def ndarray2ofrecords ( dsfile , dataname , encoded_data , labelname , encoded_label ): topack = { dataname : bytes_feature ( encoded_data ), labelname : int32_feature ( encoded_label )} ofrecord_features = ofrecord . OFRecord ( feature = topack ) serilizedBytes = ofrecord_features . SerializeToString () length = ofrecord_features . ByteSize () dsfile . write ( struct . pack ( \"q\" , length )) dsfile . write ( serilizedBytes )","title":"How OFRecord Reader is Encoded"},{"location":"extended_topics/how_to_convert_image_to_ofrecord.html#training-on-ofrecord-dataset","text":"We run lenet_train.py . It will read the OFRecord dataset that we have just created and train it on the LeNet model. The outputs of training script should like below: [6.778578] [2.0212684] [1.3814741] [0.47514156] [0.13277876] [0.16388433] [0.03788032] [0.01225162] ...... At this point, we have successfully completed the whole process of dataset production, reading and training.","title":"Training on OFRecord Dataset"},{"location":"extended_topics/how_to_make_ofdataset.html","text":"In [data input] (... /basics_topics/data_input.md) we learned that it is usually more efficient to load data using DataLoader and related operators. Also, we learned how to use DataLoader and related operators. In article OFRecord , we learn about the storage format of OFRecord files. In this article, we will focus on the loading and generating of OneFlow's OFRecord dataset, which mainly includes: The hierarchy of OFRecord dataset Multiple ways of loading OFRecord dataset The transition between OFRecord dataset and other data formats What is OFRecord Dataset \u00b6 In article OFRecord , we introduce what OFRecord file is and the storage format of OFRecord file . OFRecord dataset is the collection of OFRecord files . The collection of mutiple files that named by OneFlow convention, and that stored in the same directory, is an OFRecord dataset. By default, The files in OFRecord dataset directory are uniformly named in the way of part-xxx , where \"xxx\" is the file id starting from zero, and there can be choices about padding or non-padding. These are the examples of using non-padding name style: mnist_kaggle/train/ \u251c\u2500\u2500 part-0 \u251c\u2500\u2500 part-1 \u251c\u2500\u2500 part-10 \u251c\u2500\u2500 part-11 \u251c\u2500\u2500 part-12 \u251c\u2500\u2500 part-13 \u251c\u2500\u2500 part-14 \u251c\u2500\u2500 part-15 \u251c\u2500\u2500 part-2 \u251c\u2500\u2500 part-3 \u251c\u2500\u2500 part-4 \u251c\u2500\u2500 part-5 \u251c\u2500\u2500 part-6 \u251c\u2500\u2500 part-7 \u251c\u2500\u2500 part-8 \u2514\u2500\u2500 part-9 These are the examples of using padding name style: mnist_kaggle/train/ \u251c\u2500\u2500 part-00000 \u251c\u2500\u2500 part-00001 \u251c\u2500\u2500 part-00002 \u251c\u2500\u2500 part-00003 \u251c\u2500\u2500 part-00004 \u251c\u2500\u2500 part-00005 \u251c\u2500\u2500 part-00006 \u251c\u2500\u2500 part-00007 \u251c\u2500\u2500 part-00008 \u251c\u2500\u2500 part-00009 \u251c\u2500\u2500 part-00010 \u251c\u2500\u2500 part-00011 \u251c\u2500\u2500 part-00012 \u251c\u2500\u2500 part-00013 \u251c\u2500\u2500 part-00014 \u251c\u2500\u2500 part-00015 OneFlow adopts this convention, which is consistent with the default storage filename in spark , so it is convenient to prepare OFRecord data by spark. Actually, we can specify the filename prefix part- , whether we pad the filename id and how many bits to pad. We just need to keep the same parameters when loading dataset, which will be described below. OneFlow provides the API interface to load OFRecord dataset by specifying the path of dataset directory, so that we can have the multi-threading, pipelining and some other advantages brought by OneFlow framework. The Method to Load OFRecord Dataset \u00b6 We use ofrecord_reader to load and preprocess dataset. In article Data Input , we have shown how to use ofrecord_reader API to load OFRecord data and preprocess it. Code: of_data_pipeline.py The prototype of ofrecord_reader is as follows\uff1a def ofrecord_reader ( ofrecord_dir , batch_size = 1 , data_part_num = 1 , part_name_prefix = \"part-\" , part_name_suffix_length =- 1 , random_shuffle = False , shuffle_buffer_size = 1024 , shuffle_after_epoch = False , name = None , ) ofrecord_dir is the directory which stored the dataset batchsize assign the batch size in each epoch data_part_num assign the number of ofrecord data format file in the directory which stored the dataset. It will raise an error if the parameter is greater than the number of the existed files part_name_prefix assign the filename prefix of ofrecord files. Oneflow locates the ofrecord files according to the prefix + index in the dataset directory part_name_suffix_length assigns the padding of ofrecord file index, -1 represents no padding random_shuffle assign whether shuffle the sample order randomly when reading data shuffle_buffer_size assign the buffer size when reading data shuffle_after_epoch assign whether shuffle the sample order after each epoch The benefit of using ofrecord_reader is that ofrecord_reader acts as a normal operator which participates in OneFlow composition optimization and enjoys OneFlow pipeline acceleration. For flexibility and extensibility of the code, we can define a preprocessing OP for ofrecord_reader to deal with specific data formats which are coupled with operational logic (e.g. decoding, decompression and etc.). - For more information on DataLoader and related operator usage refer to Data input . - For more information on customized OP please refer to User op . The transition between other data format data and OFRecord dataset \u00b6 According to the storage format of OFRecord file in article OFRecord and the filename format convention of OFRecord dataset introduced at the beginning, we can prepare OFRecord dataset by ourselves. To prepare dataset easier, we provide jar package from Spark, which is convenient to the interconversion between OFRecord and common data formats (such as TFRecord and JSON). The installation and launch of Spark \u00b6 At first, we should download Spark and Spark-oneflow-connector\uff1a Download the spark-2.4.7-bin-hadoop2.7.tgz from the official website of Spark Download jar package here , which is needed by Spark to support the ofrecord file format Then unzip the spark-2.4.7-bin-hadoop2.7.tgz and configure the environment variable SPARK_HOME : export SPARK_HOME=path/to/spark-2.4.7-bin-hadoop2.7 export PATH=$SPARK_HOME/bin:$PATH We can launch the pyspark shell with the following command\uff1a pyspark --master \"local[*]\"\\ --jars spark-oneflow-connector-assembly-0.1.0_int64.jar\\ --packages org.tensorflow:spark-tensorflow-connector_2.11:1.13.1 Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.4.7 /_/ Using Python version 3.6.10 (default, Mar 25 2020 18:53:43) SparkSession available as 'spark'. We can complete the data conversion between OFRecord dataset and other formats in pyspark shell. Use Spark to view OFRecord dataset \u00b6 We can view OFRecord data with following code\uff1a spark.read.format(\"ofrecord\").load(\"file:///path/to/ofrecord_file\").show() The first 20 rows are displayed by default: +--------------------+------+ | images|labels| +--------------------+------+ |[0.33967614, 0.87...| 2| |[0.266905, 0.9730...| 3| |[0.66661334, 0.67...| 1| |[0.91943026, 0.89...| 6| |[0.014844197, 0.0...| 6| |[0.5366513, 0.748...| 4| |[0.055148937, 0.7...| 7| |[0.7814437, 0.228...| 4| |[0.31193638, 0.55...| 3| |[0.20034336, 0.24...| 4| |[0.09441255, 0.07...| 3| |[0.5177533, 0.397...| 0| |[0.23703437, 0.44...| 9| |[0.9425567, 0.859...| 9| |[0.017339867, 0.0...| 3| |[0.827106, 0.3122...| 0| |[0.8641392, 0.194...| 2| |[0.95585227, 0.29...| 3| |[0.7508129, 0.464...| 4| |[0.035597708, 0.3...| 9| +--------------------+------+ only showing top 20 rows The interconversion with TFRecord dataset \u00b6 we can convert TFRecord to OFRecord with the following command\uff1a reader = spark . read . format ( \"tfrecords\" ) dataframe = reader . load ( \"file:///path/to/tfrecord_file\" ) writer = dataframe . write . format ( \"ofrecord\" ) writer . save ( \"file:///path/to/outputdir\" ) In the above code, the outputdir directory will be created automatically, and ofrecord files will be saved into this directory. Make sure that the \"outputdir\" directory does not exist before executing the command. In addition, we can use the following command to split data into multiple ofrecord files. reader = spark . read . format ( \"tfrecords\" ) dataframe = reader . load ( \"file:///path/to/tfrecord_file\" ) writer = dataframe . repartition ( 10 ) . write . format ( \"ofrecord\" ) writer . save ( \"file://path/to/outputdir\" ) After executing the above commands, 10 ofrecord files of part-xxx format will be generated in \"outputdir\" directory. The process of converting OFRecord file to TFRecord file is similar. we just need to change the format of read/write side: reader = spark . read . format ( \"ofrecord\" ) dataframe = reader . load ( \"file:///path/to/ofrecord_file\" ) writer = dataframe . write . format ( \"tfrecords\" ) writer . save ( \"file:///path/to/outputdir\" ) The interconversion with JSON format \u00b6 We can convert JSON to OFRecord with the following command\uff1a dataframe = spark . read . json ( \"file:///path/to/json_file\" ) writer = dataframe . write . format ( \"ofrecord\" ) writer . save ( \"file:///path/to/outputdir\" ) The following command will convert OFRecord data to JSON files\uff1a reader = spark . read . format ( \"ofrecord\" ) dataframe = reader . load ( \"file:///path/to/ofrecord_file\" ) dataframe . write . json ( \"file://path/to/outputdir\" )","title":"Loading and Preparing OFRecord Dataset"},{"location":"extended_topics/how_to_make_ofdataset.html#what-is-ofrecord-dataset","text":"In article OFRecord , we introduce what OFRecord file is and the storage format of OFRecord file . OFRecord dataset is the collection of OFRecord files . The collection of mutiple files that named by OneFlow convention, and that stored in the same directory, is an OFRecord dataset. By default, The files in OFRecord dataset directory are uniformly named in the way of part-xxx , where \"xxx\" is the file id starting from zero, and there can be choices about padding or non-padding. These are the examples of using non-padding name style: mnist_kaggle/train/ \u251c\u2500\u2500 part-0 \u251c\u2500\u2500 part-1 \u251c\u2500\u2500 part-10 \u251c\u2500\u2500 part-11 \u251c\u2500\u2500 part-12 \u251c\u2500\u2500 part-13 \u251c\u2500\u2500 part-14 \u251c\u2500\u2500 part-15 \u251c\u2500\u2500 part-2 \u251c\u2500\u2500 part-3 \u251c\u2500\u2500 part-4 \u251c\u2500\u2500 part-5 \u251c\u2500\u2500 part-6 \u251c\u2500\u2500 part-7 \u251c\u2500\u2500 part-8 \u2514\u2500\u2500 part-9 These are the examples of using padding name style: mnist_kaggle/train/ \u251c\u2500\u2500 part-00000 \u251c\u2500\u2500 part-00001 \u251c\u2500\u2500 part-00002 \u251c\u2500\u2500 part-00003 \u251c\u2500\u2500 part-00004 \u251c\u2500\u2500 part-00005 \u251c\u2500\u2500 part-00006 \u251c\u2500\u2500 part-00007 \u251c\u2500\u2500 part-00008 \u251c\u2500\u2500 part-00009 \u251c\u2500\u2500 part-00010 \u251c\u2500\u2500 part-00011 \u251c\u2500\u2500 part-00012 \u251c\u2500\u2500 part-00013 \u251c\u2500\u2500 part-00014 \u251c\u2500\u2500 part-00015 OneFlow adopts this convention, which is consistent with the default storage filename in spark , so it is convenient to prepare OFRecord data by spark. Actually, we can specify the filename prefix part- , whether we pad the filename id and how many bits to pad. We just need to keep the same parameters when loading dataset, which will be described below. OneFlow provides the API interface to load OFRecord dataset by specifying the path of dataset directory, so that we can have the multi-threading, pipelining and some other advantages brought by OneFlow framework.","title":"What is OFRecord Dataset"},{"location":"extended_topics/how_to_make_ofdataset.html#the-method-to-load-ofrecord-dataset","text":"We use ofrecord_reader to load and preprocess dataset. In article Data Input , we have shown how to use ofrecord_reader API to load OFRecord data and preprocess it. Code: of_data_pipeline.py The prototype of ofrecord_reader is as follows\uff1a def ofrecord_reader ( ofrecord_dir , batch_size = 1 , data_part_num = 1 , part_name_prefix = \"part-\" , part_name_suffix_length =- 1 , random_shuffle = False , shuffle_buffer_size = 1024 , shuffle_after_epoch = False , name = None , ) ofrecord_dir is the directory which stored the dataset batchsize assign the batch size in each epoch data_part_num assign the number of ofrecord data format file in the directory which stored the dataset. It will raise an error if the parameter is greater than the number of the existed files part_name_prefix assign the filename prefix of ofrecord files. Oneflow locates the ofrecord files according to the prefix + index in the dataset directory part_name_suffix_length assigns the padding of ofrecord file index, -1 represents no padding random_shuffle assign whether shuffle the sample order randomly when reading data shuffle_buffer_size assign the buffer size when reading data shuffle_after_epoch assign whether shuffle the sample order after each epoch The benefit of using ofrecord_reader is that ofrecord_reader acts as a normal operator which participates in OneFlow composition optimization and enjoys OneFlow pipeline acceleration. For flexibility and extensibility of the code, we can define a preprocessing OP for ofrecord_reader to deal with specific data formats which are coupled with operational logic (e.g. decoding, decompression and etc.). - For more information on DataLoader and related operator usage refer to Data input . - For more information on customized OP please refer to User op .","title":"The Method to Load OFRecord Dataset"},{"location":"extended_topics/how_to_make_ofdataset.html#the-transition-between-other-data-format-data-and-ofrecord-dataset","text":"According to the storage format of OFRecord file in article OFRecord and the filename format convention of OFRecord dataset introduced at the beginning, we can prepare OFRecord dataset by ourselves. To prepare dataset easier, we provide jar package from Spark, which is convenient to the interconversion between OFRecord and common data formats (such as TFRecord and JSON).","title":"The transition between other data format data and OFRecord dataset"},{"location":"extended_topics/how_to_make_ofdataset.html#the-installation-and-launch-of-spark","text":"At first, we should download Spark and Spark-oneflow-connector\uff1a Download the spark-2.4.7-bin-hadoop2.7.tgz from the official website of Spark Download jar package here , which is needed by Spark to support the ofrecord file format Then unzip the spark-2.4.7-bin-hadoop2.7.tgz and configure the environment variable SPARK_HOME : export SPARK_HOME=path/to/spark-2.4.7-bin-hadoop2.7 export PATH=$SPARK_HOME/bin:$PATH We can launch the pyspark shell with the following command\uff1a pyspark --master \"local[*]\"\\ --jars spark-oneflow-connector-assembly-0.1.0_int64.jar\\ --packages org.tensorflow:spark-tensorflow-connector_2.11:1.13.1 Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.4.7 /_/ Using Python version 3.6.10 (default, Mar 25 2020 18:53:43) SparkSession available as 'spark'. We can complete the data conversion between OFRecord dataset and other formats in pyspark shell.","title":"The installation and launch of Spark"},{"location":"extended_topics/how_to_make_ofdataset.html#use-spark-to-view-ofrecord-dataset","text":"We can view OFRecord data with following code\uff1a spark.read.format(\"ofrecord\").load(\"file:///path/to/ofrecord_file\").show() The first 20 rows are displayed by default: +--------------------+------+ | images|labels| +--------------------+------+ |[0.33967614, 0.87...| 2| |[0.266905, 0.9730...| 3| |[0.66661334, 0.67...| 1| |[0.91943026, 0.89...| 6| |[0.014844197, 0.0...| 6| |[0.5366513, 0.748...| 4| |[0.055148937, 0.7...| 7| |[0.7814437, 0.228...| 4| |[0.31193638, 0.55...| 3| |[0.20034336, 0.24...| 4| |[0.09441255, 0.07...| 3| |[0.5177533, 0.397...| 0| |[0.23703437, 0.44...| 9| |[0.9425567, 0.859...| 9| |[0.017339867, 0.0...| 3| |[0.827106, 0.3122...| 0| |[0.8641392, 0.194...| 2| |[0.95585227, 0.29...| 3| |[0.7508129, 0.464...| 4| |[0.035597708, 0.3...| 9| +--------------------+------+ only showing top 20 rows","title":"Use Spark to view OFRecord dataset"},{"location":"extended_topics/how_to_make_ofdataset.html#the-interconversion-with-tfrecord-dataset","text":"we can convert TFRecord to OFRecord with the following command\uff1a reader = spark . read . format ( \"tfrecords\" ) dataframe = reader . load ( \"file:///path/to/tfrecord_file\" ) writer = dataframe . write . format ( \"ofrecord\" ) writer . save ( \"file:///path/to/outputdir\" ) In the above code, the outputdir directory will be created automatically, and ofrecord files will be saved into this directory. Make sure that the \"outputdir\" directory does not exist before executing the command. In addition, we can use the following command to split data into multiple ofrecord files. reader = spark . read . format ( \"tfrecords\" ) dataframe = reader . load ( \"file:///path/to/tfrecord_file\" ) writer = dataframe . repartition ( 10 ) . write . format ( \"ofrecord\" ) writer . save ( \"file://path/to/outputdir\" ) After executing the above commands, 10 ofrecord files of part-xxx format will be generated in \"outputdir\" directory. The process of converting OFRecord file to TFRecord file is similar. we just need to change the format of read/write side: reader = spark . read . format ( \"ofrecord\" ) dataframe = reader . load ( \"file:///path/to/ofrecord_file\" ) writer = dataframe . write . format ( \"tfrecords\" ) writer . save ( \"file:///path/to/outputdir\" )","title":"The interconversion with TFRecord dataset"},{"location":"extended_topics/how_to_make_ofdataset.html#the-interconversion-with-json-format","text":"We can convert JSON to OFRecord with the following command\uff1a dataframe = spark . read . json ( \"file:///path/to/json_file\" ) writer = dataframe . write . format ( \"ofrecord\" ) writer . save ( \"file:///path/to/outputdir\" ) The following command will convert OFRecord data to JSON files\uff1a reader = spark . read . format ( \"ofrecord\" ) dataframe = reader . load ( \"file:///path/to/ofrecord_file\" ) dataframe . write . json ( \"file://path/to/outputdir\" )","title":"The interconversion with JSON format"},{"location":"extended_topics/implement_data_loader.html","text":"Customize DataLoader \u00b6 As described in Data Input , OneFlow supports two ways to load data: one is directly use Numpy data, the other one is use DataLoader and some relative operators. Under the large industrial scene, data loading can easily become the bottleneck through the training process. Since we use DataLoader and some preprocessing operators, OneFlow's acceleration mechanism helps to load and preprocess data more efficiently, which can solve that problem. To use DataLoader in OneFlow, we usually apply XXXReader to load the file data, and use XXXDecode to decode or preprocess the data. These two operators work together to complete the function of data loading. Now OneFlow has built some DataLoader internally. If we want to use DataLoader to promote the efficiency of data loading, however, the DataLoader for the corresponding data format is not yet built in OneFlow. At this time, we can implement our own DataLoader to load the customized data format. In this article we implement a Mini Dataloader. You can check the Code in this repository. As an example, the data format that Mini Dataloader supported is : A plain text file with two columns of numbers separated by commas (See the part-000 and part-001 file in code): 1.01,2.02 2.01,4.02 3.0,6.05 4.1,8.205 5,10 6.0,12.0 7.0,14.2 8.0,16.3 9.1,18.03 This article will take Mini Dataloader as an example to explain the key points of implementing customized DataLoader. The composition of Dataloader \u00b6 A complete Dataloader generally includes two types of Op: Data Reader: Which is responsible for loading the data in file system to the input stream of memory and setting the data to the Op's output. Data Decoder: The Data Decoder decodes and outputs the data in Data Reader Op. For some simple data formats, which is no need for decoding, we can omit the Data Decoder and just use Data Reader. As an example, though the data format processed by Mini Dataloader is simple, we still implement the two types of ops: Data Reader and Data Decoder. Among these two Ops: MiniReader is responsible for reading data from files and split strings by commas. Convert the text to the float value and set to the Op's output. The output shape is two columns of each row. MiniDecoder is responsible for splitting the two columns of each row output in above and get two outputs x and y , both of their shape is one column of each row. In test_mini_dataloader.py we can see the usage of both Ops at Python level: miniRecord = MiniReader ( \"./\" , batch_size = batch_size , data_part_num = 2 , part_name_suffix_length = 3 , random_shuffle = True , shuffle_after_epoch = True , ) x , y = MiniDecoder ( miniRecord , name = \"d1\" ) We will introduce how to implement Data Reader and Data Decoder in C++ backend below. Data Reader operator \u00b6 The class relationship in Data Reader \u00b6 We need to implement a class that inherits from DataReader , this class includes two important objects loader_ and parser_ , which inherits from Dataset and Parser separately. loader_ 's job is to load data from the file system to buffer. The Op's author build the logic by overriding the Next method. parser_ 's job is to set the data in buffer to Op's output. The Op's author build the logic by overriding the Parser method. When Data Reader Op works, it will call the relative method in loader_ to open files in the file system, and then call the Next method in loader_ to read data from the file system according to the logic built by Op's author. The pseudocode below shows the class relationship and the calling procedure. The actual code is more complicated and it is not the exact corresponding relationship: class DataReader { void Read ( user_op :: KernelComputeContext * ctx ) { // OneFlow already starts multi-threads to accelerate data processing when code runs here. loader -> next (); parser_ -> Parse (); } Dataset * loader_ ; Parser * parser_ ; }; class MiniDataReader : DataReader { loader_ = new MiniDataSet ; parser_ = new MiniParser ; }; class MiniDataset : Dataset { MiniDataset () { // Find and open dataset in the file system, initialize the input stream. //... } Next () { // The logic of reading data from input stream. } }; class MiniParser : Parser { void Parse (){ // Set the data from DataSet to Op's output. } }; In Data Reader Op's Kernel, it will trigger the Read method in DataReader and complete the sequence of operations which is shown in the pseudocode above. The registration of Op and Kernel \u00b6 We register the MiniReader's Op through the code below: REGISTER_CPU_ONLY_USER_OP ( \"MiniReader\" ) . Output ( \"out\" ) . Attr < std :: string > ( \"data_dir\" ) . Attr < std :: int32_t > ( \"data_part_num\" ) . Attr < std :: string > ( \"part_name_prefix\" , std :: string ( \"part-\" )) . Attr < int32_t > ( \"part_name_suffix_length\" , -1 ) . Attr < int32_t > ( \"batch_size\" ) . Attr < bool > ( \"random_shuffle\" , false ) . Attr < bool > ( \"shuffle_after_epoch\" , false ) . Attr < int64_t > ( \"seed\" , -1 ) . Attr < int32_t > ( \"shuffle_buffer_size\" , 1024 ) . SetTensorDescInferFn ([]( user_op :: InferContext * ctx ) -> Maybe < void > { //... * out_tensor -> mut_shape () = Shape ({ local_batch_size , 2 }); * out_tensor -> mut_data_type () = DataType :: kDouble ; //... }) . SetGetSbpFn ([]( user_op :: SbpContext * ctx ) -> Maybe < void > { ctx -> NewBuilder (). Split ( ctx -> outputs (), 0 ). Build (); //... }); As we can see, because Data Reader is a special Op, it has only output, no input (data comes from the file system, instead of some upstream nodes), we only use Out method to set the output, and set the output shape as two columns per row in SetTensorDescInferFn , the data type is DataType::kDouble . In the same way, when we set SBP Signature in SetGetSbpFn , we only need to set output's SBP attribution. In this case, we set it as Split(0). The other attributions (like data_dir , data_part_num , etc.) follow the requirement of file naming conventions in The OFRecord Data Format . It allows us to reuse some related code in OneFlow to load customized data format like The method to load OFRecord dataset . Then let's look at the implementation of Op's Kernel: class MiniReaderKernel final : public user_op :: OpKernel { public : //... std :: shared_ptr < user_op :: OpKernelState > CreateOpKernelState ( user_op :: KernelInitContext * ctx ) override { std :: shared_ptr < MiniReaderWrapper > reader ( new MiniReaderWrapper ( ctx )); return reader ; } void Compute ( user_op :: KernelComputeContext * ctx , user_op :: OpKernelState * state ) override { auto * reader = dynamic_cast < MiniReaderWrapper *> ( state ); reader -> Read ( ctx ); } //... }; REGISTER_USER_KERNEL ( \"MiniReader\" ) . SetCreateFn < MiniReaderKernel > () . SetIsMatchedHob (( user_op :: HobDeviceTag () == \"cpu\" ) & ( user_op :: HobDataType ( \"out\" , 0 ) == DataType :: kDouble )); According to the knowledge in Customize Op , we have known that MiniReaderKernel::Compute method is responsible for the Op's compute logic. However, we use an override version of Compute that includes two parameters here. It's necessary to introduce the second parameter OpKernelState . When we call the Compute , we need to maintain other objects in addition to get information from ctx . This type of object does not need to be created repeatedly, but their state of information may change as Compute method is called multiple times. In response to this need, OneFlow provides a override version with two parameters of Compute . In order to use it, we need to override CreateOpKernelState at the same time. CreateOpKernelState returns a user_op::OpKernelState derived class object. This object will be the second parameter when Compute is called. So we only need to pack the information we want to maintain, in addition to the ctx , as a derived class of user_op::OpKernelState , instantiate and return it in CreateOpKernelState . In our Mini Reader's Kernel, we first implement a class MiniReaderWarapper that is inherited from user_op::OpKernelState . It is a simple encapsulation of MiniDataReader , the reason why we encapsulate MiniReaderWrapper instead of using MiniDataReader directly is that to meet the requirements of OneFlow. class MiniReaderWrapper final : public user_op :: OpKernelState { public : explicit MiniReaderWrapper ( user_op :: KernelInitContext * ctx ) : reader_ ( ctx ) {} ~ MiniReaderWrapper () = default ; void Read ( user_op :: KernelComputeContext * ctx ) { reader_ . Read ( ctx ); } private : data :: MiniDataReader reader_ ; }; Then, we override CreateOpKernelState , create a MiniReaderwrapper object internally. std :: shared_ptr < user_op :: OpKernelState > CreateOpKernelState ( user_op :: KernelInitContext * ctx ) override { std :: shared_ptr < MiniReaderWrapper > reader ( new MiniReaderWrapper ( ctx )); return reader ; } In this way, OneFlow will call CreateOpKernelState method to create object automatically in appropriate time and pass it to Compute as the second parameter. We can get this object in Compute , and use it: auto * reader = dynamic_cast < MiniReaderWrapper *> ( state ); reader -> Read ( ctx ); As we can see, In MiniReader's Kernel, we just simply call MiniReaderWrapper::Reader , it will trigger the procedure of DataReader::Read that is mentioned in above pseudocode. MiniDataReader \u00b6 As we mentioned in above pseudocode. In MiniDataReader , it will instantiate a MiniDataset and assign to the loader_ pointer. Here is the code: class MiniDataReader final : public DataReader < TensorBuffer > { public : MiniDataReader ( user_op :: KernelInitContext * ctx ) : DataReader < TensorBuffer > ( ctx ) { loader_ . reset ( new MiniDataset ( ctx )); parser_ . reset ( new MiniParser ()); if ( ctx -> Attr < bool > ( \"random_shuffle\" )) { loader_ . reset ( new RandomShuffleDataset < TensorBuffer > ( ctx , std :: move ( loader_ ))); } int32_t batch_size = ctx -> TensorDesc4ArgNameAndIndex ( \"out\" , 0 ) -> shape (). elem_cnt (); loader_ . reset ( new BatchDataset < TensorBuffer > ( batch_size , std :: move ( loader_ ))); StartLoadThread (); } }; In addition to inheriting our Dataset 's MiniDataset class, OneFlow also build other XXXDataset , they can add additional features in the base of existing Dataset . For example, RandomShuffleDataset can be used to shuffle data, BatchDataset can be used to read batch data. When it is all done, we finally call StartLoadThread , which is used to start the loading thread. We will trigger the override method MiniDataset::Next in StartLoadThread . The above construction of MiniDataReader can be used as a template. If you have no special requirements, you don't need to modify it in custom DataLoader. MiniDataset \u00b6 For MiniDataSet , we only need to focus on the constructor and overridden Next method. The constructor obtains user's settings through Attr . Then it will initialize the input stream according to the user's settings. In the following code, JoinDirPath is used to obtain all filenames according to the convention of dataset (the prefix, the amount of files, whether the filename number is padded, etc.). And InitInStream is to initialize the file in dataset as input stream (The member of in_stream ), which is encapsulated by OneFlow, it will be used in Next method later. MiniDataset ( user_op :: KernelInitContext * ctx ) { current_epoch_ = 0 ; shuffle_after_epoch_ = ctx -> Attr < bool > ( \"shuffle_after_epoch\" ); //Join Dir Path JoinDirPath ( ctx ); // in stream InitInStream ( ctx ); } The logic of loading from files is written in virtual function Next : LoadTargetPtrList Next () override { LoadTargetPtrList ret ; LoadTargetPtr sample_ptr ( new TensorBuffer ()); std :: string sampleline ; if ( in_stream_ -> ReadLine ( & sampleline ) != 0 ) { ShuffleAfterEpoch (); in_stream_ -> ReadLine ( & sampleline ); } auto numbers = CommaSplit ( sampleline ); sample_ptr -> Resize ( Shape ({ 2 }), DataType :: kDouble ); auto pNums = sample_ptr -> mut_data < double > (); pNums [ 0 ] = std :: stod ( numbers [ 0 ]); pNums [ 1 ] = std :: stod ( numbers [ 1 ]); ret . push_back ( std :: move ( sample_ptr )); return ret ; } In the above code, we call in_stream_ 's ReadLine method to load file data into string object sampleline . Then we call CommaSplit to split the string by commas, convert to float value, and place it to TensorBuffer object. It is worth to mention that _in_stream_ has two ways to read data from files: int32_t PersistentInStream::ReadLine ( std :: string * l ); int32_t PersistentInStream::ReadFully ( char * s , size_t n ); ReadLine method reads a row of file to l object; ReadFully method reads n bytes data to the memory pointed by s . Both methods use 0 as return value on success. MiniDataset complete the process of file to memory buffer. In next section, we will use MiniParser to set the buffer's content to Op's output. MiniParser \u00b6 MiniPaser inherits from Parser , we just need to rewrite the Parser method. class MiniParser final : public Parser < TensorBuffer > { public : using LoadTargetPtr = std :: shared_ptr < TensorBuffer > ; using LoadTargetPtrList = std :: vector < LoadTargetPtr > ; void Parse ( std :: shared_ptr < LoadTargetPtrList > batch_data , user_op :: KernelComputeContext * ctx ) override { user_op :: Tensor * out_tensor = ctx -> Tensor4ArgNameAndIndex ( \"out\" , 0 ); double * dptr = out_tensor -> mut_dptr < double > (); MultiThreadLoop ( batch_data -> size (), [ & ]( size_t i ) { TensorBuffer * buffer = batch_data -> at ( i ). get (); dptr [ i * 2 ] = * ( buffer -> data < double > ()); dptr [ i * 2 + 1 ] = * ( buffer -> data < double > () + 1 ); }); } }; Parser includes 2 parameters, where batch_data is an encapsulated vector . Each element in this container is the data previously read by MiniDataSet through Next method. The parameter ctx enables us to get the information of Op. Here we mainly obtain the output through ctx and get the pointer dptr to the output buffer. Notice that we use macro MultiThreadLoop in the procedure of setting batch_data 's data to the Op's output dptr . MultiThreadLoop allows our loop logic to be executed in multiple threads. It accept 2 parameters, the first parameter is the total number of loop; the second parameter is a callback function, its prototype is void callback(size_t, i) . OneFlow will create multiple threads, and call the callback function concurrently. The callback function's parameter i indicates the serial number of current loop, allowing us to divide data according to i and complete the business logic. In the above code, we get the i th data in buffer through batch_data->at(i).get() , and set it to the location of the i th row in the output memory area. There are two columns in total. Data Decoder Operator \u00b6 Data Decoder operator is a normal operator. It accepts the output of DataReader as its input, outputs one or multiple Blobs after some operations. In ofrecord_decoder_ops.cpp , we can see various decoders for OFRecord data. The data processed by our Mini Dataloader is simple, so the work done by MiniDecoder is also very simple. It just splits two columns data output from DataReader into two one-column outputs as x and y . Mini Decoder's Op is registered as: REGISTER_CPU_ONLY_USER_OP ( \"mini_decoder\" ) . Input ( \"in\" ) . Output ( \"x\" ) . Output ( \"y\" ) . SetTensorDescInferFn ([]( user_op :: InferContext * ctx ) -> Maybe < void > { user_op :: TensorDesc * in_tensor = ctx -> TensorDesc4ArgNameAndIndex ( \"in\" , 0 ); user_op :: TensorDesc * out_tensor_x = ctx -> TensorDesc4ArgNameAndIndex ( \"x\" , 0 ); user_op :: TensorDesc * out_tensor_y = ctx -> TensorDesc4ArgNameAndIndex ( \"y\" , 0 ); // Set the input, output Blob's attribution // ... }) . SetGetSbpFn ([]( user_op :: SbpContext * ctx ) -> Maybe < void > { ctx -> NewBuilder () . Split ( user_op :: OpArg ( \"in\" , 0 ), 0 ) . Split ( user_op :: OpArg ( \"x\" , 0 ), 0 ) . Split ( user_op :: OpArg ( \"y\" , 0 ), 0 ) . Build (); //... }); The implementation of Mini Decoder's Kernel is as follow: class MiniDecoderKernel final : public user_op :: OpKernel { //... void Compute ( user_op :: KernelComputeContext * ctx ) const override { user_op :: Tensor * in_blob = ctx -> Tensor4ArgNameAndIndex ( \"in\" , 0 ); user_op :: Tensor * out_blob_x = ctx -> Tensor4ArgNameAndIndex ( \"x\" , 0 ); user_op :: Tensor * out_blob_y = ctx -> Tensor4ArgNameAndIndex ( \"y\" , 0 ); int64_t record_num = in_blob -> shape (). At ( 0 ); const double * input = in_blob -> dptr < double > (); double * out_dptr_x = out_blob_x -> mut_dptr < double > (); double * out_dptr_y = out_blob_y -> mut_dptr < double > (); MultiThreadLoop ( record_num , [ & ]( size_t i ){ * ( out_dptr_x + i ) = * ( input + i * 2 ); * ( out_dptr_y + i ) = * ( input + i * 2 + 1 ); }); } //... }; We mainly get the input in_blob in MiniDecoderKernel::Compute , and then in multiple threads loop MultiThreadLoop , split the input data into out_dptr_x and out_dptr_y , that correspond to the output x and y . The use of customized DataLoader \u00b6 As described in customized user op , if we want to use the Op built in C++ backend, we need to encapsulate a Python Wrapper in Python level. Some related work is put in test_mini_dataloader.py : def MiniDecoder ( input_blob , name = None , ): if name is None : name = \"Mini_Decoder_uniqueID\" return ( flow . user_op_builder ( name ) . Op ( \"mini_decoder\" ) . Input ( \"in\" , [ input_blob ]) . Output ( \"x\" ) . Output ( \"y\" ) . Build () . InferAndTryRun () . RemoteBlobList () ) def MiniReader ( minidata_dir : str , batch_size : int = 1 , data_part_num : int = 2 , part_name_prefix : str = \"part-\" , part_name_suffix_length : int = - 1 , random_shuffle : bool = False , shuffle_after_epoch : bool = False , shuffle_buffer_size : int = 1024 , name = None , ): if name is None : name = \"Mini_Reader_uniqueID\" return ( flow . user_op_builder ( name ) . Op ( \"MiniReader\" ) . Output ( \"out\" ) . Attr ( \"data_dir\" , minidata_dir ) . Attr ( \"data_part_num\" , data_part_num ) . Attr ( \"batch_size\" , batch_size ) . Attr ( \"part_name_prefix\" , part_name_prefix ) . Attr ( \"random_shuffle\" , random_shuffle ) . Attr ( \"shuffle_after_epoch\" , shuffle_after_epoch ) . Attr ( \"part_name_suffix_length\" , part_name_suffix_length ) . Attr ( \"shuffle_buffer_size\" , shuffle_buffer_size ) . Build () . InferAndTryRun () . RemoteBlobList ()[ 0 ] ) In test_mini_dataloader.py , we use our implemented MiniReader and MiniDecoder to load and decode the data in dataset ( part-000 and part-001 ), complete a epoch of training. Compile and test Mini Dataloader \u00b6 Check into the corresponding directory data_loader for this article. Change Makefile 's ONEFLOW_ROOT variable as the directory of OneFlow's source code. And then use make to generate miniloader.so file. Run test_mini_dataloader.py script, then we can use Mini Dataloader to load data and complete training. python test_mini_dataloader.py","title":"Customize DataLoader"},{"location":"extended_topics/implement_data_loader.html#customize-dataloader","text":"As described in Data Input , OneFlow supports two ways to load data: one is directly use Numpy data, the other one is use DataLoader and some relative operators. Under the large industrial scene, data loading can easily become the bottleneck through the training process. Since we use DataLoader and some preprocessing operators, OneFlow's acceleration mechanism helps to load and preprocess data more efficiently, which can solve that problem. To use DataLoader in OneFlow, we usually apply XXXReader to load the file data, and use XXXDecode to decode or preprocess the data. These two operators work together to complete the function of data loading. Now OneFlow has built some DataLoader internally. If we want to use DataLoader to promote the efficiency of data loading, however, the DataLoader for the corresponding data format is not yet built in OneFlow. At this time, we can implement our own DataLoader to load the customized data format. In this article we implement a Mini Dataloader. You can check the Code in this repository. As an example, the data format that Mini Dataloader supported is : A plain text file with two columns of numbers separated by commas (See the part-000 and part-001 file in code): 1.01,2.02 2.01,4.02 3.0,6.05 4.1,8.205 5,10 6.0,12.0 7.0,14.2 8.0,16.3 9.1,18.03 This article will take Mini Dataloader as an example to explain the key points of implementing customized DataLoader.","title":"Customize DataLoader"},{"location":"extended_topics/implement_data_loader.html#the-composition-of-dataloader","text":"A complete Dataloader generally includes two types of Op: Data Reader: Which is responsible for loading the data in file system to the input stream of memory and setting the data to the Op's output. Data Decoder: The Data Decoder decodes and outputs the data in Data Reader Op. For some simple data formats, which is no need for decoding, we can omit the Data Decoder and just use Data Reader. As an example, though the data format processed by Mini Dataloader is simple, we still implement the two types of ops: Data Reader and Data Decoder. Among these two Ops: MiniReader is responsible for reading data from files and split strings by commas. Convert the text to the float value and set to the Op's output. The output shape is two columns of each row. MiniDecoder is responsible for splitting the two columns of each row output in above and get two outputs x and y , both of their shape is one column of each row. In test_mini_dataloader.py we can see the usage of both Ops at Python level: miniRecord = MiniReader ( \"./\" , batch_size = batch_size , data_part_num = 2 , part_name_suffix_length = 3 , random_shuffle = True , shuffle_after_epoch = True , ) x , y = MiniDecoder ( miniRecord , name = \"d1\" ) We will introduce how to implement Data Reader and Data Decoder in C++ backend below.","title":"The composition of Dataloader"},{"location":"extended_topics/implement_data_loader.html#data-reader-operator","text":"","title":"Data Reader operator"},{"location":"extended_topics/implement_data_loader.html#the-class-relationship-in-data-reader","text":"We need to implement a class that inherits from DataReader , this class includes two important objects loader_ and parser_ , which inherits from Dataset and Parser separately. loader_ 's job is to load data from the file system to buffer. The Op's author build the logic by overriding the Next method. parser_ 's job is to set the data in buffer to Op's output. The Op's author build the logic by overriding the Parser method. When Data Reader Op works, it will call the relative method in loader_ to open files in the file system, and then call the Next method in loader_ to read data from the file system according to the logic built by Op's author. The pseudocode below shows the class relationship and the calling procedure. The actual code is more complicated and it is not the exact corresponding relationship: class DataReader { void Read ( user_op :: KernelComputeContext * ctx ) { // OneFlow already starts multi-threads to accelerate data processing when code runs here. loader -> next (); parser_ -> Parse (); } Dataset * loader_ ; Parser * parser_ ; }; class MiniDataReader : DataReader { loader_ = new MiniDataSet ; parser_ = new MiniParser ; }; class MiniDataset : Dataset { MiniDataset () { // Find and open dataset in the file system, initialize the input stream. //... } Next () { // The logic of reading data from input stream. } }; class MiniParser : Parser { void Parse (){ // Set the data from DataSet to Op's output. } }; In Data Reader Op's Kernel, it will trigger the Read method in DataReader and complete the sequence of operations which is shown in the pseudocode above.","title":"The class relationship in Data Reader"},{"location":"extended_topics/implement_data_loader.html#the-registration-of-op-and-kernel","text":"We register the MiniReader's Op through the code below: REGISTER_CPU_ONLY_USER_OP ( \"MiniReader\" ) . Output ( \"out\" ) . Attr < std :: string > ( \"data_dir\" ) . Attr < std :: int32_t > ( \"data_part_num\" ) . Attr < std :: string > ( \"part_name_prefix\" , std :: string ( \"part-\" )) . Attr < int32_t > ( \"part_name_suffix_length\" , -1 ) . Attr < int32_t > ( \"batch_size\" ) . Attr < bool > ( \"random_shuffle\" , false ) . Attr < bool > ( \"shuffle_after_epoch\" , false ) . Attr < int64_t > ( \"seed\" , -1 ) . Attr < int32_t > ( \"shuffle_buffer_size\" , 1024 ) . SetTensorDescInferFn ([]( user_op :: InferContext * ctx ) -> Maybe < void > { //... * out_tensor -> mut_shape () = Shape ({ local_batch_size , 2 }); * out_tensor -> mut_data_type () = DataType :: kDouble ; //... }) . SetGetSbpFn ([]( user_op :: SbpContext * ctx ) -> Maybe < void > { ctx -> NewBuilder (). Split ( ctx -> outputs (), 0 ). Build (); //... }); As we can see, because Data Reader is a special Op, it has only output, no input (data comes from the file system, instead of some upstream nodes), we only use Out method to set the output, and set the output shape as two columns per row in SetTensorDescInferFn , the data type is DataType::kDouble . In the same way, when we set SBP Signature in SetGetSbpFn , we only need to set output's SBP attribution. In this case, we set it as Split(0). The other attributions (like data_dir , data_part_num , etc.) follow the requirement of file naming conventions in The OFRecord Data Format . It allows us to reuse some related code in OneFlow to load customized data format like The method to load OFRecord dataset . Then let's look at the implementation of Op's Kernel: class MiniReaderKernel final : public user_op :: OpKernel { public : //... std :: shared_ptr < user_op :: OpKernelState > CreateOpKernelState ( user_op :: KernelInitContext * ctx ) override { std :: shared_ptr < MiniReaderWrapper > reader ( new MiniReaderWrapper ( ctx )); return reader ; } void Compute ( user_op :: KernelComputeContext * ctx , user_op :: OpKernelState * state ) override { auto * reader = dynamic_cast < MiniReaderWrapper *> ( state ); reader -> Read ( ctx ); } //... }; REGISTER_USER_KERNEL ( \"MiniReader\" ) . SetCreateFn < MiniReaderKernel > () . SetIsMatchedHob (( user_op :: HobDeviceTag () == \"cpu\" ) & ( user_op :: HobDataType ( \"out\" , 0 ) == DataType :: kDouble )); According to the knowledge in Customize Op , we have known that MiniReaderKernel::Compute method is responsible for the Op's compute logic. However, we use an override version of Compute that includes two parameters here. It's necessary to introduce the second parameter OpKernelState . When we call the Compute , we need to maintain other objects in addition to get information from ctx . This type of object does not need to be created repeatedly, but their state of information may change as Compute method is called multiple times. In response to this need, OneFlow provides a override version with two parameters of Compute . In order to use it, we need to override CreateOpKernelState at the same time. CreateOpKernelState returns a user_op::OpKernelState derived class object. This object will be the second parameter when Compute is called. So we only need to pack the information we want to maintain, in addition to the ctx , as a derived class of user_op::OpKernelState , instantiate and return it in CreateOpKernelState . In our Mini Reader's Kernel, we first implement a class MiniReaderWarapper that is inherited from user_op::OpKernelState . It is a simple encapsulation of MiniDataReader , the reason why we encapsulate MiniReaderWrapper instead of using MiniDataReader directly is that to meet the requirements of OneFlow. class MiniReaderWrapper final : public user_op :: OpKernelState { public : explicit MiniReaderWrapper ( user_op :: KernelInitContext * ctx ) : reader_ ( ctx ) {} ~ MiniReaderWrapper () = default ; void Read ( user_op :: KernelComputeContext * ctx ) { reader_ . Read ( ctx ); } private : data :: MiniDataReader reader_ ; }; Then, we override CreateOpKernelState , create a MiniReaderwrapper object internally. std :: shared_ptr < user_op :: OpKernelState > CreateOpKernelState ( user_op :: KernelInitContext * ctx ) override { std :: shared_ptr < MiniReaderWrapper > reader ( new MiniReaderWrapper ( ctx )); return reader ; } In this way, OneFlow will call CreateOpKernelState method to create object automatically in appropriate time and pass it to Compute as the second parameter. We can get this object in Compute , and use it: auto * reader = dynamic_cast < MiniReaderWrapper *> ( state ); reader -> Read ( ctx ); As we can see, In MiniReader's Kernel, we just simply call MiniReaderWrapper::Reader , it will trigger the procedure of DataReader::Read that is mentioned in above pseudocode.","title":"The registration of Op and Kernel"},{"location":"extended_topics/implement_data_loader.html#minidatareader","text":"As we mentioned in above pseudocode. In MiniDataReader , it will instantiate a MiniDataset and assign to the loader_ pointer. Here is the code: class MiniDataReader final : public DataReader < TensorBuffer > { public : MiniDataReader ( user_op :: KernelInitContext * ctx ) : DataReader < TensorBuffer > ( ctx ) { loader_ . reset ( new MiniDataset ( ctx )); parser_ . reset ( new MiniParser ()); if ( ctx -> Attr < bool > ( \"random_shuffle\" )) { loader_ . reset ( new RandomShuffleDataset < TensorBuffer > ( ctx , std :: move ( loader_ ))); } int32_t batch_size = ctx -> TensorDesc4ArgNameAndIndex ( \"out\" , 0 ) -> shape (). elem_cnt (); loader_ . reset ( new BatchDataset < TensorBuffer > ( batch_size , std :: move ( loader_ ))); StartLoadThread (); } }; In addition to inheriting our Dataset 's MiniDataset class, OneFlow also build other XXXDataset , they can add additional features in the base of existing Dataset . For example, RandomShuffleDataset can be used to shuffle data, BatchDataset can be used to read batch data. When it is all done, we finally call StartLoadThread , which is used to start the loading thread. We will trigger the override method MiniDataset::Next in StartLoadThread . The above construction of MiniDataReader can be used as a template. If you have no special requirements, you don't need to modify it in custom DataLoader.","title":"MiniDataReader"},{"location":"extended_topics/implement_data_loader.html#minidataset","text":"For MiniDataSet , we only need to focus on the constructor and overridden Next method. The constructor obtains user's settings through Attr . Then it will initialize the input stream according to the user's settings. In the following code, JoinDirPath is used to obtain all filenames according to the convention of dataset (the prefix, the amount of files, whether the filename number is padded, etc.). And InitInStream is to initialize the file in dataset as input stream (The member of in_stream ), which is encapsulated by OneFlow, it will be used in Next method later. MiniDataset ( user_op :: KernelInitContext * ctx ) { current_epoch_ = 0 ; shuffle_after_epoch_ = ctx -> Attr < bool > ( \"shuffle_after_epoch\" ); //Join Dir Path JoinDirPath ( ctx ); // in stream InitInStream ( ctx ); } The logic of loading from files is written in virtual function Next : LoadTargetPtrList Next () override { LoadTargetPtrList ret ; LoadTargetPtr sample_ptr ( new TensorBuffer ()); std :: string sampleline ; if ( in_stream_ -> ReadLine ( & sampleline ) != 0 ) { ShuffleAfterEpoch (); in_stream_ -> ReadLine ( & sampleline ); } auto numbers = CommaSplit ( sampleline ); sample_ptr -> Resize ( Shape ({ 2 }), DataType :: kDouble ); auto pNums = sample_ptr -> mut_data < double > (); pNums [ 0 ] = std :: stod ( numbers [ 0 ]); pNums [ 1 ] = std :: stod ( numbers [ 1 ]); ret . push_back ( std :: move ( sample_ptr )); return ret ; } In the above code, we call in_stream_ 's ReadLine method to load file data into string object sampleline . Then we call CommaSplit to split the string by commas, convert to float value, and place it to TensorBuffer object. It is worth to mention that _in_stream_ has two ways to read data from files: int32_t PersistentInStream::ReadLine ( std :: string * l ); int32_t PersistentInStream::ReadFully ( char * s , size_t n ); ReadLine method reads a row of file to l object; ReadFully method reads n bytes data to the memory pointed by s . Both methods use 0 as return value on success. MiniDataset complete the process of file to memory buffer. In next section, we will use MiniParser to set the buffer's content to Op's output.","title":"MiniDataset"},{"location":"extended_topics/implement_data_loader.html#miniparser","text":"MiniPaser inherits from Parser , we just need to rewrite the Parser method. class MiniParser final : public Parser < TensorBuffer > { public : using LoadTargetPtr = std :: shared_ptr < TensorBuffer > ; using LoadTargetPtrList = std :: vector < LoadTargetPtr > ; void Parse ( std :: shared_ptr < LoadTargetPtrList > batch_data , user_op :: KernelComputeContext * ctx ) override { user_op :: Tensor * out_tensor = ctx -> Tensor4ArgNameAndIndex ( \"out\" , 0 ); double * dptr = out_tensor -> mut_dptr < double > (); MultiThreadLoop ( batch_data -> size (), [ & ]( size_t i ) { TensorBuffer * buffer = batch_data -> at ( i ). get (); dptr [ i * 2 ] = * ( buffer -> data < double > ()); dptr [ i * 2 + 1 ] = * ( buffer -> data < double > () + 1 ); }); } }; Parser includes 2 parameters, where batch_data is an encapsulated vector . Each element in this container is the data previously read by MiniDataSet through Next method. The parameter ctx enables us to get the information of Op. Here we mainly obtain the output through ctx and get the pointer dptr to the output buffer. Notice that we use macro MultiThreadLoop in the procedure of setting batch_data 's data to the Op's output dptr . MultiThreadLoop allows our loop logic to be executed in multiple threads. It accept 2 parameters, the first parameter is the total number of loop; the second parameter is a callback function, its prototype is void callback(size_t, i) . OneFlow will create multiple threads, and call the callback function concurrently. The callback function's parameter i indicates the serial number of current loop, allowing us to divide data according to i and complete the business logic. In the above code, we get the i th data in buffer through batch_data->at(i).get() , and set it to the location of the i th row in the output memory area. There are two columns in total.","title":"MiniParser"},{"location":"extended_topics/implement_data_loader.html#data-decoder-operator","text":"Data Decoder operator is a normal operator. It accepts the output of DataReader as its input, outputs one or multiple Blobs after some operations. In ofrecord_decoder_ops.cpp , we can see various decoders for OFRecord data. The data processed by our Mini Dataloader is simple, so the work done by MiniDecoder is also very simple. It just splits two columns data output from DataReader into two one-column outputs as x and y . Mini Decoder's Op is registered as: REGISTER_CPU_ONLY_USER_OP ( \"mini_decoder\" ) . Input ( \"in\" ) . Output ( \"x\" ) . Output ( \"y\" ) . SetTensorDescInferFn ([]( user_op :: InferContext * ctx ) -> Maybe < void > { user_op :: TensorDesc * in_tensor = ctx -> TensorDesc4ArgNameAndIndex ( \"in\" , 0 ); user_op :: TensorDesc * out_tensor_x = ctx -> TensorDesc4ArgNameAndIndex ( \"x\" , 0 ); user_op :: TensorDesc * out_tensor_y = ctx -> TensorDesc4ArgNameAndIndex ( \"y\" , 0 ); // Set the input, output Blob's attribution // ... }) . SetGetSbpFn ([]( user_op :: SbpContext * ctx ) -> Maybe < void > { ctx -> NewBuilder () . Split ( user_op :: OpArg ( \"in\" , 0 ), 0 ) . Split ( user_op :: OpArg ( \"x\" , 0 ), 0 ) . Split ( user_op :: OpArg ( \"y\" , 0 ), 0 ) . Build (); //... }); The implementation of Mini Decoder's Kernel is as follow: class MiniDecoderKernel final : public user_op :: OpKernel { //... void Compute ( user_op :: KernelComputeContext * ctx ) const override { user_op :: Tensor * in_blob = ctx -> Tensor4ArgNameAndIndex ( \"in\" , 0 ); user_op :: Tensor * out_blob_x = ctx -> Tensor4ArgNameAndIndex ( \"x\" , 0 ); user_op :: Tensor * out_blob_y = ctx -> Tensor4ArgNameAndIndex ( \"y\" , 0 ); int64_t record_num = in_blob -> shape (). At ( 0 ); const double * input = in_blob -> dptr < double > (); double * out_dptr_x = out_blob_x -> mut_dptr < double > (); double * out_dptr_y = out_blob_y -> mut_dptr < double > (); MultiThreadLoop ( record_num , [ & ]( size_t i ){ * ( out_dptr_x + i ) = * ( input + i * 2 ); * ( out_dptr_y + i ) = * ( input + i * 2 + 1 ); }); } //... }; We mainly get the input in_blob in MiniDecoderKernel::Compute , and then in multiple threads loop MultiThreadLoop , split the input data into out_dptr_x and out_dptr_y , that correspond to the output x and y .","title":"Data Decoder Operator"},{"location":"extended_topics/implement_data_loader.html#the-use-of-customized-dataloader","text":"As described in customized user op , if we want to use the Op built in C++ backend, we need to encapsulate a Python Wrapper in Python level. Some related work is put in test_mini_dataloader.py : def MiniDecoder ( input_blob , name = None , ): if name is None : name = \"Mini_Decoder_uniqueID\" return ( flow . user_op_builder ( name ) . Op ( \"mini_decoder\" ) . Input ( \"in\" , [ input_blob ]) . Output ( \"x\" ) . Output ( \"y\" ) . Build () . InferAndTryRun () . RemoteBlobList () ) def MiniReader ( minidata_dir : str , batch_size : int = 1 , data_part_num : int = 2 , part_name_prefix : str = \"part-\" , part_name_suffix_length : int = - 1 , random_shuffle : bool = False , shuffle_after_epoch : bool = False , shuffle_buffer_size : int = 1024 , name = None , ): if name is None : name = \"Mini_Reader_uniqueID\" return ( flow . user_op_builder ( name ) . Op ( \"MiniReader\" ) . Output ( \"out\" ) . Attr ( \"data_dir\" , minidata_dir ) . Attr ( \"data_part_num\" , data_part_num ) . Attr ( \"batch_size\" , batch_size ) . Attr ( \"part_name_prefix\" , part_name_prefix ) . Attr ( \"random_shuffle\" , random_shuffle ) . Attr ( \"shuffle_after_epoch\" , shuffle_after_epoch ) . Attr ( \"part_name_suffix_length\" , part_name_suffix_length ) . Attr ( \"shuffle_buffer_size\" , shuffle_buffer_size ) . Build () . InferAndTryRun () . RemoteBlobList ()[ 0 ] ) In test_mini_dataloader.py , we use our implemented MiniReader and MiniDecoder to load and decode the data in dataset ( part-000 and part-001 ), complete a epoch of training.","title":"The use of customized DataLoader"},{"location":"extended_topics/implement_data_loader.html#compile-and-test-mini-dataloader","text":"Check into the corresponding directory data_loader for this article. Change Makefile 's ONEFLOW_ROOT variable as the directory of OneFlow's source code. And then use make to generate miniloader.so file. Run test_mini_dataloader.py script, then we can use Mini Dataloader to load data and complete training. python test_mini_dataloader.py","title":"Compile and test Mini Dataloader"},{"location":"extended_topics/job_function_define_call.html","text":"The Definition and Call of Job Function \u00b6 In OneFlow, we encapsulate the training, inference and some other tasks into a \"job function\". The job function is used to connect the user's business logic and the computing resource managed by OneFlow. In OneFlow, the function decorated by @oneflow.global_function decorator is the OneFlow's job function We mainly define the structure of the model and choose the optimization target in job function. In addition, we can also pass some hyperparameters about training and some configuration of the environment to the job function (like the following example: get_train_config() ), OneFlow will manage the memory, GPU and other computing resource according to our configuration. In this article, we will specifically learn about: how to define and call the job function how to get the return value of job function The Relationship Between Job Function and Running Process of OneFlow \u00b6 The job function is divided into two phases: definition and call. It's related to OneFlow's operating mechanism. Briefly, the OneFlow Python layer API simply describes the configuration and the training environment of the model. These information will pass to the C++ backend. After compilation, graph building and so on, the computation graph is obtained. Finally, the job function will be executed in OneFlow runtime. The job function describes the model and the training environment. In this phase there's no data. We can only define the shape and data type of the nodes (as known as PlaceHolder ) for creating and compiling the computation graph of OneFlow. The job function will be called after the OneFlow runtime starts. We can pass the data by calling job function and get the results. We will introduce the definition and calling method of job functions in detail as below. The Definition of Job Function \u00b6 We encapsulate the model in Python and use oneflow.global_function to decorate. Then the definition is completed. The job function mainly describes two things: The structure of model The optimizing target in training phase In the following code, we build a Multi-Layer Perceptron model and use flow.nn.sparse_softmax_cross_entropy_with_logits to compute the cross-entropy loss as our optimizing target. @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Callback [ tp . Numpy ]: # mlp initializer = flow . truncated_normal ( 0.1 ) reshape = flow . reshape ( images , [ images . shape [ 0 ], - 1 ]) hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"hidden\" , ) logits = flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , name = \"output\" ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.1 ]) flow . optimizer . SGD ( lr_scheduler , momentum = 0 ) . minimize ( loss ) return loss The Parameters of oneflow.global_function \u00b6 oneflow.global_function decorator accepts two parameters, type and function_config . The parameter type accepts a string, which can only set as train or predict . When we define a training model, we set it as train . We set is as predict when we define a model for testing or inferencing. The parameter function_config accepts an object which is constructed by oneflow.function_config() . In function_config object, we can use its method or attribute to config. As the following code. def get_train_config (): config = flow . function_config () config . default_data_type ( flow . float ) return config We set the default data type, then, we can pass the function_config object to the global_function decorator. @flow . global_function ( type = \"train\" , function_config = get_train_config ()) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : For the complete code, you can refer to Consistent and Mirrored 's mixed_parallel_mlp.py PlaceHolder \u00b6 Noted that the images \u3001 logits \u3001 labels \u3001 loss and some other objects have no data in our definition of the job function. They are used to describe the shape and attribute of data , which is called PlaceHolder . For the parameters of the job function, we use Numpy.Placeholder , ListNumpy.Placeholder , ListListNumpy.Placeholder in the oneflow.typing package to annotate the data type of them as numpy.ndarray , Sequence[numpy.ndarray] and Sequence[Sequence[numpy.ndarray]] respectively. Besides the types of oneflow.typing , the variables returned from OneFlow operators or layers in the job function, like the reshape \u3001 hidden \u3001 logits \u3001 loss in the code above, are also PlaceHolder. All the variables mentioned above inherit the base class BlobDef directly or indirectly. We call this object type as Blob in OneFlow. The Blob has no data when defining the job function. It only plays the role of data placeholder for building the graph. The Return Value of the Job Function \u00b6 The concept of the data placeholder Blob is emphasized above because the return value of the job function cannot be arbitrarily specified. It must be Blob type object or a container which only contains the Blob object. For example, the loss returned in the above code is a Blob object The return values of job function should be annotated. As an example, -> tp.Numpy in above code means the function returns a Blob object. As another example, we can annotate the return value type as -> Tuple[tp.Numpy, tp.Numpy] .It means the function returns a tuple which contains two Blob object You can refer to Get the result of the job function for specific examples. The Call of Job Function \u00b6 OneFlow uses decorator to convert Python function into OneFlow's job function. It is transparent to user. We can call the job function just like we call a Python function. Every time we call the job function, OneFlow will complete the forward propagation, back propagation, parameter updates, and more in framework. In the code below. When we get the data, we will pass parameters and call the train_job function to print loss . ( train_images , train_labels ), ( test_images , test_labels ) = flow . data . load_mnist ( BATCH_SIZE ) for epoch in range ( 3 ): for i , ( images , labels ) in enumerate ( zip ( train_images , train_labels )): loss = train_job ( images , labels ) if i % 20 == 0 : print ( loss . mean ()) As you can see, by calling the job function train_job , the numpy data is directly returned. The method shown above is synchronous. OneFlow also supports asynchronous invocation. For more details you can refer to the article Get the result of the job function .","title":"The Definition and Call of Job Function"},{"location":"extended_topics/job_function_define_call.html#the-definition-and-call-of-job-function","text":"In OneFlow, we encapsulate the training, inference and some other tasks into a \"job function\". The job function is used to connect the user's business logic and the computing resource managed by OneFlow. In OneFlow, the function decorated by @oneflow.global_function decorator is the OneFlow's job function We mainly define the structure of the model and choose the optimization target in job function. In addition, we can also pass some hyperparameters about training and some configuration of the environment to the job function (like the following example: get_train_config() ), OneFlow will manage the memory, GPU and other computing resource according to our configuration. In this article, we will specifically learn about: how to define and call the job function how to get the return value of job function","title":"The Definition and Call of Job Function"},{"location":"extended_topics/job_function_define_call.html#the-relationship-between-job-function-and-running-process-of-oneflow","text":"The job function is divided into two phases: definition and call. It's related to OneFlow's operating mechanism. Briefly, the OneFlow Python layer API simply describes the configuration and the training environment of the model. These information will pass to the C++ backend. After compilation, graph building and so on, the computation graph is obtained. Finally, the job function will be executed in OneFlow runtime. The job function describes the model and the training environment. In this phase there's no data. We can only define the shape and data type of the nodes (as known as PlaceHolder ) for creating and compiling the computation graph of OneFlow. The job function will be called after the OneFlow runtime starts. We can pass the data by calling job function and get the results. We will introduce the definition and calling method of job functions in detail as below.","title":"The Relationship Between Job Function and Running Process of OneFlow"},{"location":"extended_topics/job_function_define_call.html#the-definition-of-job-function","text":"We encapsulate the model in Python and use oneflow.global_function to decorate. Then the definition is completed. The job function mainly describes two things: The structure of model The optimizing target in training phase In the following code, we build a Multi-Layer Perceptron model and use flow.nn.sparse_softmax_cross_entropy_with_logits to compute the cross-entropy loss as our optimizing target. @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Callback [ tp . Numpy ]: # mlp initializer = flow . truncated_normal ( 0.1 ) reshape = flow . reshape ( images , [ images . shape [ 0 ], - 1 ]) hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"hidden\" , ) logits = flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , name = \"output\" ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.1 ]) flow . optimizer . SGD ( lr_scheduler , momentum = 0 ) . minimize ( loss ) return loss","title":"The Definition of Job Function"},{"location":"extended_topics/job_function_define_call.html#the-parameters-of-oneflowglobal_function","text":"oneflow.global_function decorator accepts two parameters, type and function_config . The parameter type accepts a string, which can only set as train or predict . When we define a training model, we set it as train . We set is as predict when we define a model for testing or inferencing. The parameter function_config accepts an object which is constructed by oneflow.function_config() . In function_config object, we can use its method or attribute to config. As the following code. def get_train_config (): config = flow . function_config () config . default_data_type ( flow . float ) return config We set the default data type, then, we can pass the function_config object to the global_function decorator. @flow . global_function ( type = \"train\" , function_config = get_train_config ()) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : For the complete code, you can refer to Consistent and Mirrored 's mixed_parallel_mlp.py","title":"The Parameters of oneflow.global_function"},{"location":"extended_topics/job_function_define_call.html#placeholder","text":"Noted that the images \u3001 logits \u3001 labels \u3001 loss and some other objects have no data in our definition of the job function. They are used to describe the shape and attribute of data , which is called PlaceHolder . For the parameters of the job function, we use Numpy.Placeholder , ListNumpy.Placeholder , ListListNumpy.Placeholder in the oneflow.typing package to annotate the data type of them as numpy.ndarray , Sequence[numpy.ndarray] and Sequence[Sequence[numpy.ndarray]] respectively. Besides the types of oneflow.typing , the variables returned from OneFlow operators or layers in the job function, like the reshape \u3001 hidden \u3001 logits \u3001 loss in the code above, are also PlaceHolder. All the variables mentioned above inherit the base class BlobDef directly or indirectly. We call this object type as Blob in OneFlow. The Blob has no data when defining the job function. It only plays the role of data placeholder for building the graph.","title":"PlaceHolder"},{"location":"extended_topics/job_function_define_call.html#the-return-value-of-the-job-function","text":"The concept of the data placeholder Blob is emphasized above because the return value of the job function cannot be arbitrarily specified. It must be Blob type object or a container which only contains the Blob object. For example, the loss returned in the above code is a Blob object The return values of job function should be annotated. As an example, -> tp.Numpy in above code means the function returns a Blob object. As another example, we can annotate the return value type as -> Tuple[tp.Numpy, tp.Numpy] .It means the function returns a tuple which contains two Blob object You can refer to Get the result of the job function for specific examples.","title":"The Return Value of the Job Function"},{"location":"extended_topics/job_function_define_call.html#the-call-of-job-function","text":"OneFlow uses decorator to convert Python function into OneFlow's job function. It is transparent to user. We can call the job function just like we call a Python function. Every time we call the job function, OneFlow will complete the forward propagation, back propagation, parameter updates, and more in framework. In the code below. When we get the data, we will pass parameters and call the train_job function to print loss . ( train_images , train_labels ), ( test_images , test_labels ) = flow . data . load_mnist ( BATCH_SIZE ) for epoch in range ( 3 ): for i , ( images , labels ) in enumerate ( zip ( train_images , train_labels )): loss = train_job ( images , labels ) if i % 20 == 0 : print ( loss . mean ()) As you can see, by calling the job function train_job , the numpy data is directly returned. The method shown above is synchronous. OneFlow also supports asynchronous invocation. For more details you can refer to the article Get the result of the job function .","title":"The Call of Job Function"},{"location":"extended_topics/model_mixed_parallel.html","text":"Features of Parallelism in OneFlow \u00b6 In Consistent and Mirrored view , we have already known OneFlow provides two types of view: mirrored and consistent view and we learned about the consistent view in OneFlow have some special features. Because in consistent_view , OneFlow provides a logically consistent view. During distributed training, users can freely choose to use data parallelism, model parallelism or hybrid parallelism. In this article, we will keep going through the special consistent view in OneFlow. We will learn about: Data parallelism in consistent_view flow chart. Hybrid parallelism in consistent_view flow chart. The advantages of hybrid parallelism and the applicable scenario. Example of hybrid parallelism. Network Logical Diagram in Model Training \u00b6 We need to set up a simple multi-layer network first and use this network to discuss parallelism methods. The structure like the figure shows: In each layer, we have samples (in grey), models (in blue) and operators (circles) which operating on both of them. To simplify our discussion, we can limit the sample and model as a matrix . The operator applying on them we call it matrix multiplication . Compare the figure above, we can easily get the logic of the network: The input of layer 0 is Data 0 matrix and Model 0 matrix. We apply op (matrix multiplication) and get output Data 1 . The input of layer 1 is Data 1 matrix and Model 1 matrix. We apply op and get output . The layer 2 is output layer and Data 2 is the output of network. Of course, it can play as input in a deeper network. In consistent view, OneFlow supports the data parallelism, model parallelism and hybrid parallelism. We will introduce them in order but hybrid parallelism is the key point. The Features of Parallelism in Consistent View \u00b6 Data Parallelism \u00b6 We have already known that in consistent view. The default parallelism method is data parallelism. If we choose mirrored view, we can only use data parallelism. If you pass numpy data directly when you call the job function (instead of using OneFlow's [DataLoader and related operators] (... /basics_topics/data_input.md#dataloader)), the difference between them are: In mirrored view, when we use data parallelism. We need to split and reorganize data according to the number of device and use list to pass and receive data. But in consistent view we have the consistency on logic. Splitting data and reorganizing data will be completed by OneFlow framework. The following figure is in consistent view, using data parallelism to achieve original logical network process: In data parallelism, we use two devices for training. As we use data parallelism , we can see that for each original logical layer, the sample is divided in average to each device. We have a complete training model in each device. The data after splitting are processed by op . Finally we combine the data in each device and get the complete data. Model parallelism \u00b6 In consistent view, we can choose model parallelism (the configuration details we will talk about it later). The flow diagram is as follows: In model parallelism example, we still use two devices for training. In each layer of original logic model is processed by op on part of model and complete data . Then they are combined and we get the complete results. One thing we need to mention is in above figure. The output from each device on layer 0 cannot use as the input in layer 1: Because in model parallelism, in order to complete the operation. We need partial model and complete data. To solve this problem, OneFlow use boxing mechanism. boxing will count the data in each node in distributed training and divide or assemble data properly then send to corresponding GPU. Besides the model assembling in model parallelism, boxing is also used for reverse gradient synchronization in data parallelism. The algorithm in boxing is complex. But it is transparent to users. The Illustration of boxing is just to prevent users from being confused. In this article, we only need to remember that OneFlow will automatically solve the data distribution issue. Choose the optimal parallelism method \u00b6 The difference between data parallelism and model parallelism is not constant. The sample, model size and model structure decide the performance in distributed training. We need to analyze the data to choose the optimal one. To be concluded: In data parallelism case, the information needed to be synchronized is gradient in backpropagation. Thus, we need to make sure that synchronization of information between different nodes is faster than calculation inside nodes. For example, the Convolution Layer has few parameters, but it needs large scale of calculation. Therefore, it is suitable for data parallelism. In model parallelism, we divide the logical model equally and send them to each device , which will solve the oversize model problem. Thus it is suitable for the neural network with massive parameters (like fully connected layer) to use model parallelism. In fact, we can use hybrid parallelism , it means OneFlow uses different parallelism in different parts of training process. For example, at the beginning of the neural network, it has few parameters and a lot of calculation, which makes it better to use data parallelism. For the layer with a lot of parameters, such as fully connected layer, we should use model parallelism. The following figure is the demonstration for the neural network which use hybrid parallelism . Currently, other popular frameworks either do not support mixed parallelism or require detailed customization. But in OneFlow, the hybrid parallelism distributed training can be configured through simple settings, and the distributed system can also be deeply optimized with the ultra-high degree of freedom pipelining mode. Hybrid Parallelism Example: \u00b6 Code \u00b6 In consistent view, we use hybrid parallelism to MLP model: the input layer and hidden layer use data parallelism, output layer use model parallelism. Complete Code: hybrid_parallelism_mlp.py More explanations can be seen in \"code explanations\" Code explanation \u00b6 The above code is modified from the demo in 3 min quick start . Compare two versions of code, we can see it is easy to configure the parallelism method in consistent_view with few codes. The crucial parts are: Use oneflow.config.gpu_device_num to set the device number in training: flow . config . gpu_device_num ( 2 ) reshape and hidden using data parallelism as default. The output layer can set model_distribute as flow.distribute.split(axis=0) to change to model parallelism: def mlp ( data ): initializer = flow . truncated_normal ( 0.1 ) reshape = flow . reshape ( data , [ data . shape [ 0 ], - 1 ]) hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"dense1\" , ) return flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , # dense for column storage with split(0) slicing. model_distribute = flow . distribute . split ( axis = 0 ), name = \"dense2\" , ) You may be curious about why split(axis=0) is column cutting. To be explained, dense is column-oriented storage in OneFlow. Thus the flow.distribute.split(axis=0) in above code is split by column. In addition, flow.layers.dense use model_distribute to set parallelism mode, it use the common get_variable to create blob in basic level from inner, and internally calls the more general interface get_variable to create blob . The get_variable interface uses a parameter named distribute to set the parallelism mode. As you can see, we can change the single machine training program to a distributed, hybrid parallel program with few modifications, which is one of the features that distinguishes OneFlow from other frameworks. Pipelining Example \u00b6 Besides the model parallelism, OneFlow also provides a more flexible parallelism method called pipelining, it allow user use scope.placement to specify the device of the operator. In pipelining, some parts of layers of the whole network are on one device and some are on other devices. They work consecutively as relay, switch between devices in different phases. In the following example, we change a few codes in \"Using consistent view in OneFlow\" of Consistent and Mirrored view and demonstrate pipelining. Code \u00b6 Complete Code: hybrid_parallelism_lenet.py Please refer to code explanation later for more details. Code Explanation \u00b6 There are only two important lines of code and they have similar effect: Use oneflow.scope.placement to specify the operator run on device 0 in hidden layer. with flow . scope . placement ( \"gpu\" , \"0:0\" ): hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"hidden\" , Use oneflow.scope.placement to specify the operator in output layer run on device 1. with flow . scope . placement ( \"gpu\" , \"0:1\" ): output = flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , name = \"outlayer\" ) More details of scope.placement can be found in the API documentation . Pipelining can allow user to specify which device to be used for each op. It is very useful for user who master the distributed training to optimize deeply . In addition, OneFlow also provides API oneflow.unpack , oneflow.pack . Combined with the own features of task scheduling in OneFlow, they make the pipelining easier to be used and more efficient. We will introduce them in other article.","title":"Features of Parallelism in OneFlow"},{"location":"extended_topics/model_mixed_parallel.html#features-of-parallelism-in-oneflow","text":"In Consistent and Mirrored view , we have already known OneFlow provides two types of view: mirrored and consistent view and we learned about the consistent view in OneFlow have some special features. Because in consistent_view , OneFlow provides a logically consistent view. During distributed training, users can freely choose to use data parallelism, model parallelism or hybrid parallelism. In this article, we will keep going through the special consistent view in OneFlow. We will learn about: Data parallelism in consistent_view flow chart. Hybrid parallelism in consistent_view flow chart. The advantages of hybrid parallelism and the applicable scenario. Example of hybrid parallelism.","title":"Features of Parallelism in OneFlow"},{"location":"extended_topics/model_mixed_parallel.html#network-logical-diagram-in-model-training","text":"We need to set up a simple multi-layer network first and use this network to discuss parallelism methods. The structure like the figure shows: In each layer, we have samples (in grey), models (in blue) and operators (circles) which operating on both of them. To simplify our discussion, we can limit the sample and model as a matrix . The operator applying on them we call it matrix multiplication . Compare the figure above, we can easily get the logic of the network: The input of layer 0 is Data 0 matrix and Model 0 matrix. We apply op (matrix multiplication) and get output Data 1 . The input of layer 1 is Data 1 matrix and Model 1 matrix. We apply op and get output . The layer 2 is output layer and Data 2 is the output of network. Of course, it can play as input in a deeper network. In consistent view, OneFlow supports the data parallelism, model parallelism and hybrid parallelism. We will introduce them in order but hybrid parallelism is the key point.","title":"Network Logical Diagram in Model Training"},{"location":"extended_topics/model_mixed_parallel.html#the-features-of-parallelism-in-consistent-view","text":"","title":"The Features of Parallelism in Consistent View"},{"location":"extended_topics/model_mixed_parallel.html#data-parallelism","text":"We have already known that in consistent view. The default parallelism method is data parallelism. If we choose mirrored view, we can only use data parallelism. If you pass numpy data directly when you call the job function (instead of using OneFlow's [DataLoader and related operators] (... /basics_topics/data_input.md#dataloader)), the difference between them are: In mirrored view, when we use data parallelism. We need to split and reorganize data according to the number of device and use list to pass and receive data. But in consistent view we have the consistency on logic. Splitting data and reorganizing data will be completed by OneFlow framework. The following figure is in consistent view, using data parallelism to achieve original logical network process: In data parallelism, we use two devices for training. As we use data parallelism , we can see that for each original logical layer, the sample is divided in average to each device. We have a complete training model in each device. The data after splitting are processed by op . Finally we combine the data in each device and get the complete data.","title":"Data Parallelism"},{"location":"extended_topics/model_mixed_parallel.html#model-parallelism","text":"In consistent view, we can choose model parallelism (the configuration details we will talk about it later). The flow diagram is as follows: In model parallelism example, we still use two devices for training. In each layer of original logic model is processed by op on part of model and complete data . Then they are combined and we get the complete results. One thing we need to mention is in above figure. The output from each device on layer 0 cannot use as the input in layer 1: Because in model parallelism, in order to complete the operation. We need partial model and complete data. To solve this problem, OneFlow use boxing mechanism. boxing will count the data in each node in distributed training and divide or assemble data properly then send to corresponding GPU. Besides the model assembling in model parallelism, boxing is also used for reverse gradient synchronization in data parallelism. The algorithm in boxing is complex. But it is transparent to users. The Illustration of boxing is just to prevent users from being confused. In this article, we only need to remember that OneFlow will automatically solve the data distribution issue.","title":"Model parallelism"},{"location":"extended_topics/model_mixed_parallel.html#choose-the-optimal-parallelism-method","text":"The difference between data parallelism and model parallelism is not constant. The sample, model size and model structure decide the performance in distributed training. We need to analyze the data to choose the optimal one. To be concluded: In data parallelism case, the information needed to be synchronized is gradient in backpropagation. Thus, we need to make sure that synchronization of information between different nodes is faster than calculation inside nodes. For example, the Convolution Layer has few parameters, but it needs large scale of calculation. Therefore, it is suitable for data parallelism. In model parallelism, we divide the logical model equally and send them to each device , which will solve the oversize model problem. Thus it is suitable for the neural network with massive parameters (like fully connected layer) to use model parallelism. In fact, we can use hybrid parallelism , it means OneFlow uses different parallelism in different parts of training process. For example, at the beginning of the neural network, it has few parameters and a lot of calculation, which makes it better to use data parallelism. For the layer with a lot of parameters, such as fully connected layer, we should use model parallelism. The following figure is the demonstration for the neural network which use hybrid parallelism . Currently, other popular frameworks either do not support mixed parallelism or require detailed customization. But in OneFlow, the hybrid parallelism distributed training can be configured through simple settings, and the distributed system can also be deeply optimized with the ultra-high degree of freedom pipelining mode.","title":"Choose the optimal parallelism method"},{"location":"extended_topics/model_mixed_parallel.html#hybrid-parallelism-example","text":"","title":"Hybrid Parallelism Example:"},{"location":"extended_topics/model_mixed_parallel.html#code","text":"In consistent view, we use hybrid parallelism to MLP model: the input layer and hidden layer use data parallelism, output layer use model parallelism. Complete Code: hybrid_parallelism_mlp.py More explanations can be seen in \"code explanations\"","title":"Code"},{"location":"extended_topics/model_mixed_parallel.html#code-explanation","text":"The above code is modified from the demo in 3 min quick start . Compare two versions of code, we can see it is easy to configure the parallelism method in consistent_view with few codes. The crucial parts are: Use oneflow.config.gpu_device_num to set the device number in training: flow . config . gpu_device_num ( 2 ) reshape and hidden using data parallelism as default. The output layer can set model_distribute as flow.distribute.split(axis=0) to change to model parallelism: def mlp ( data ): initializer = flow . truncated_normal ( 0.1 ) reshape = flow . reshape ( data , [ data . shape [ 0 ], - 1 ]) hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"dense1\" , ) return flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , # dense for column storage with split(0) slicing. model_distribute = flow . distribute . split ( axis = 0 ), name = \"dense2\" , ) You may be curious about why split(axis=0) is column cutting. To be explained, dense is column-oriented storage in OneFlow. Thus the flow.distribute.split(axis=0) in above code is split by column. In addition, flow.layers.dense use model_distribute to set parallelism mode, it use the common get_variable to create blob in basic level from inner, and internally calls the more general interface get_variable to create blob . The get_variable interface uses a parameter named distribute to set the parallelism mode. As you can see, we can change the single machine training program to a distributed, hybrid parallel program with few modifications, which is one of the features that distinguishes OneFlow from other frameworks.","title":"Code explanation"},{"location":"extended_topics/model_mixed_parallel.html#pipelining-example","text":"Besides the model parallelism, OneFlow also provides a more flexible parallelism method called pipelining, it allow user use scope.placement to specify the device of the operator. In pipelining, some parts of layers of the whole network are on one device and some are on other devices. They work consecutively as relay, switch between devices in different phases. In the following example, we change a few codes in \"Using consistent view in OneFlow\" of Consistent and Mirrored view and demonstrate pipelining.","title":"Pipelining Example"},{"location":"extended_topics/model_mixed_parallel.html#code_1","text":"Complete Code: hybrid_parallelism_lenet.py Please refer to code explanation later for more details.","title":"Code"},{"location":"extended_topics/model_mixed_parallel.html#code-explanation_1","text":"There are only two important lines of code and they have similar effect: Use oneflow.scope.placement to specify the operator run on device 0 in hidden layer. with flow . scope . placement ( \"gpu\" , \"0:0\" ): hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"hidden\" , Use oneflow.scope.placement to specify the operator in output layer run on device 1. with flow . scope . placement ( \"gpu\" , \"0:1\" ): output = flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , name = \"outlayer\" ) More details of scope.placement can be found in the API documentation . Pipelining can allow user to specify which device to be used for each op. It is very useful for user who master the distributed training to optimize deeply . In addition, OneFlow also provides API oneflow.unpack , oneflow.pack . Combined with the own features of task scheduling in OneFlow, they make the pipelining easier to be used and more efficient. We will introduce them in other article.","title":"Code Explanation"},{"location":"extended_topics/ofrecord.html","text":"Deep Learning applications need complex multi-stage data preprocessing pipeline, the first step of data pipeline is data loading. OneFlow supports multiple data formats in data loading, among which OFRecord format is the native data format of OneFlow. The data format definition of OFRecord is similar to TFRecord of Tensorflow. Users familiar with TFRecord can start with OneFlow's OFRecord quickly. Key points of this article\uff1a The data type used in OFRecord How to convert data to OFRecord object and serialize it The file format of OFRecord It should be helpful for users to learn how to make ofdataset after learning the above contents. Data Types of OFRecord \u00b6 Internally, OneFlow use Protocol Buffers to describe the serialization format of OFRecord. The related definitions can be found in the oneflow/core/record/record.proto file\uff1a syntax = \"proto2\"; package oneflow; message BytesList { repeated bytes value = 1; } message FloatList { repeated float value = 1 [packed = true]; } message DoubleList { repeated double value = 1 [packed = true]; } message Int32List { repeated int32 value = 1 [packed = true]; } message Int64List { repeated int64 value = 1 [packed = true]; } message Feature { oneof kind { BytesList bytes_list = 1; FloatList float_list = 2; DoubleList double_list = 3; Int32List int32_list = 4; Int64List int64_list = 5; } } message OFRecord { map<string, Feature> feature = 1; } Firstly let's explain the above important data types in details\uff1a OFRecord: the instantiated object of OFRecord, which can be used to store all data that need to be serialized. It is composed of many key-value pairs of string->Feature; Feature: can store one of the data type including BytesList, FloatList, DoubleList, Int32List, Int64List; The corresponding interfaces with the same name of OFRecord, Feature, XXXList and other data types will be generated by Protocol Buffers , making it possible to build corresponding objects at the Python level. Convert Data into Feature Format \u00b6 Users can convert data to Feature format with the invocation of ofrecord.xxxList and ofrecord.Feature . We also encapsulate the interface generated by protocol buffers to make it more convenient for users. import oneflow.core.record.record_pb2 as ofrecord def int32_feature ( value ): if not isinstance ( value , ( list , tuple )): value = [ value ] return ofrecord . Feature ( int32_list = ofrecord . Int32List ( value = value )) def int64_feature ( value ): if not isinstance ( value , ( list , tuple )): value = [ value ] return ofrecord . Feature ( int64_list = ofrecord . Int64List ( value = value )) def float_feature ( value ): if not isinstance ( value , ( list , tuple )): value = [ value ] return ofrecord . Feature ( float_list = ofrecord . FloatList ( value = value )) def double_feature ( value ): if not isinstance ( value , ( list , tuple )): value = [ value ] return ofrecord . Feature ( double_list = ofrecord . DoubleList ( value = value )) def bytes_feature ( value ): if not isinstance ( value , ( list , tuple )): value = [ value ] if not six . PY2 : if isinstance ( value [ 0 ], str ): value = [ x . encode () for x in value ] return ofrecord . Feature ( bytes_list = ofrecord . BytesList ( value = value )) Creating and Serializing OFRecord Object \u00b6 In the following example, we will create an OFRecord object which contains two features then serialize with its SerializeToString method obserations = 28 * 28 f = open ( \"./dataset/part-0\" , \"wb\" ) for loop in range ( 0 , 3 ): image = [ random . random () for x in range ( 0 , obserations )] label = [ random . randint ( 0 , 9 )] topack = { \"images\" : float_feature ( image ), \"labels\" : int64_feature ( label ), } ofrecord_features = ofrecord . OFRecord ( feature = topack ) serilizedBytes = ofrecord_features . SerializeToString () With the above example, we can summarize the steps for serializing data\uff1a First, users can convert data which needs to be serialized to a Feature object with the invocation of ofrecord.Feature and ofrecord.XXXList Second, store the Feature objects obtained in the previous step as string->Feature key-value format in Python dict Third, create OFRecord object with the invocation of ofrecord.OFRecord Last, get the serialized result of OFRecord object with its SerializeToString method The serialized result can be saved as a file with ofrecord format. OFRecord Format File \u00b6 According to the format convention of OneFlow, users can get OFRecord file after serializing the OFRecord object. Multiple OFRecord objects can be stored in one OFRecord file which can be used in OneFlow data-pipeline . The specific operations can be seen at how to make ofrecord dataset . According to the OneFlow convention, each OFRecord object is stored in the following format. uint64 length byte data[length] The length of the data are stored in the first eight bytes and then followed by the serialized data. length = ofrecord_features . ByteSize () f . write ( struct . pack ( \"q\" , length )) f . write ( serilizedBytes ) Code \u00b6 The following complete code shows how to generate an OFRecord file and then manually read datas by calling the OFRecord interface generated by protobuf . Actually, OneFlow provides flow.data.decode_ofrecord and other interfaces, which can more easily extract the contents of OFRecord files(dataset). See how to make ofrecord dataset for details. Write OFRecord Object to File \u00b6 In the following code, we randomly generate 3 samples and their corresponding labels, each sample is a 28*28 picture. After these three samples are converted into OFRecord objects, they are stored in the file according to the OneFlow convention format. Complete code\uff1a ofrecord_to_string.py Read data from OFRecord file \u00b6 The code below shows how to parse and read data from OFRecord file generated in the above example. Getting the OFRecord object by calling the FromString method to deserialize the file contents then display the data\uff1a The complete code\uff1a ofrecord_from_string.py","title":"The OFRecord Data Format"},{"location":"extended_topics/ofrecord.html#data-types-of-ofrecord","text":"Internally, OneFlow use Protocol Buffers to describe the serialization format of OFRecord. The related definitions can be found in the oneflow/core/record/record.proto file\uff1a syntax = \"proto2\"; package oneflow; message BytesList { repeated bytes value = 1; } message FloatList { repeated float value = 1 [packed = true]; } message DoubleList { repeated double value = 1 [packed = true]; } message Int32List { repeated int32 value = 1 [packed = true]; } message Int64List { repeated int64 value = 1 [packed = true]; } message Feature { oneof kind { BytesList bytes_list = 1; FloatList float_list = 2; DoubleList double_list = 3; Int32List int32_list = 4; Int64List int64_list = 5; } } message OFRecord { map<string, Feature> feature = 1; } Firstly let's explain the above important data types in details\uff1a OFRecord: the instantiated object of OFRecord, which can be used to store all data that need to be serialized. It is composed of many key-value pairs of string->Feature; Feature: can store one of the data type including BytesList, FloatList, DoubleList, Int32List, Int64List; The corresponding interfaces with the same name of OFRecord, Feature, XXXList and other data types will be generated by Protocol Buffers , making it possible to build corresponding objects at the Python level.","title":"Data Types of OFRecord"},{"location":"extended_topics/ofrecord.html#convert-data-into-feature-format","text":"Users can convert data to Feature format with the invocation of ofrecord.xxxList and ofrecord.Feature . We also encapsulate the interface generated by protocol buffers to make it more convenient for users. import oneflow.core.record.record_pb2 as ofrecord def int32_feature ( value ): if not isinstance ( value , ( list , tuple )): value = [ value ] return ofrecord . Feature ( int32_list = ofrecord . Int32List ( value = value )) def int64_feature ( value ): if not isinstance ( value , ( list , tuple )): value = [ value ] return ofrecord . Feature ( int64_list = ofrecord . Int64List ( value = value )) def float_feature ( value ): if not isinstance ( value , ( list , tuple )): value = [ value ] return ofrecord . Feature ( float_list = ofrecord . FloatList ( value = value )) def double_feature ( value ): if not isinstance ( value , ( list , tuple )): value = [ value ] return ofrecord . Feature ( double_list = ofrecord . DoubleList ( value = value )) def bytes_feature ( value ): if not isinstance ( value , ( list , tuple )): value = [ value ] if not six . PY2 : if isinstance ( value [ 0 ], str ): value = [ x . encode () for x in value ] return ofrecord . Feature ( bytes_list = ofrecord . BytesList ( value = value ))","title":"Convert Data into Feature Format"},{"location":"extended_topics/ofrecord.html#creating-and-serializing-ofrecord-object","text":"In the following example, we will create an OFRecord object which contains two features then serialize with its SerializeToString method obserations = 28 * 28 f = open ( \"./dataset/part-0\" , \"wb\" ) for loop in range ( 0 , 3 ): image = [ random . random () for x in range ( 0 , obserations )] label = [ random . randint ( 0 , 9 )] topack = { \"images\" : float_feature ( image ), \"labels\" : int64_feature ( label ), } ofrecord_features = ofrecord . OFRecord ( feature = topack ) serilizedBytes = ofrecord_features . SerializeToString () With the above example, we can summarize the steps for serializing data\uff1a First, users can convert data which needs to be serialized to a Feature object with the invocation of ofrecord.Feature and ofrecord.XXXList Second, store the Feature objects obtained in the previous step as string->Feature key-value format in Python dict Third, create OFRecord object with the invocation of ofrecord.OFRecord Last, get the serialized result of OFRecord object with its SerializeToString method The serialized result can be saved as a file with ofrecord format.","title":"Creating and Serializing OFRecord Object"},{"location":"extended_topics/ofrecord.html#ofrecord-format-file","text":"According to the format convention of OneFlow, users can get OFRecord file after serializing the OFRecord object. Multiple OFRecord objects can be stored in one OFRecord file which can be used in OneFlow data-pipeline . The specific operations can be seen at how to make ofrecord dataset . According to the OneFlow convention, each OFRecord object is stored in the following format. uint64 length byte data[length] The length of the data are stored in the first eight bytes and then followed by the serialized data. length = ofrecord_features . ByteSize () f . write ( struct . pack ( \"q\" , length )) f . write ( serilizedBytes )","title":"OFRecord Format File"},{"location":"extended_topics/ofrecord.html#code","text":"The following complete code shows how to generate an OFRecord file and then manually read datas by calling the OFRecord interface generated by protobuf . Actually, OneFlow provides flow.data.decode_ofrecord and other interfaces, which can more easily extract the contents of OFRecord files(dataset). See how to make ofrecord dataset for details.","title":"Code"},{"location":"extended_topics/ofrecord.html#write-ofrecord-object-to-file","text":"In the following code, we randomly generate 3 samples and their corresponding labels, each sample is a 28*28 picture. After these three samples are converted into OFRecord objects, they are stored in the file according to the OneFlow convention format. Complete code\uff1a ofrecord_to_string.py","title":"Write OFRecord Object to File"},{"location":"extended_topics/ofrecord.html#read-data-from-ofrecord-file","text":"The code below shows how to parse and read data from OFRecord file generated in the above example. Getting the OFRecord object by calling the FromString method to deserialize the file contents then display the data\uff1a The complete code\uff1a ofrecord_from_string.py","title":"Read data from OFRecord file"},{"location":"extended_topics/oneflow_convert_tools.html","text":"oneflow_convert_tools \u00b6 oneflow_onnx \u00b6 Introduction \u00b6 oneflow_onnx tool package includes two major functions: one is to export OneFlow out of ONNX, while the other is to transform ONNX models, which are obtained from other training frameworks, into Oneflow models. This tool package has already been adapted to TensorFlow/Pytorch/PaddlePaddle pre-trained models. The process of oneflow_onnx extracting ONNX and transforming it into OneFlow's format is called X2OneFlow (X representing TensorFlow/Pytorch/PaddlePaddle). OneFlow2ONNX models are supported. Specifically, OneFlow's lazy mode model can be transfomed into ONNX's format. Transformable OneFlow model can be obtained by using the method explained on flow.checkpoint.save . For more information, please refer to OneFlow2ONNX Model List . X2OneFlow models are supported. TensorFlow/Pytorch/PaddlePaddle model can be transformed into OneFlow's format through ONNX. OneFlow2ONNX operators are supported. Currently, oneflow_onnx is fully capable of exporting ONNX Opset10, and parts of OneFlow operator can transform ONNX Opsets that are in lower order. Please refer to OneFlow2ONNX Operator Lists for more information. X2OneFlow operators are supported. Currently, oneflow_onnx is fully capable of supporting most CV operators in TensorFlow/Pytorch/PaddlePaddle. Please refer to X2OneFlow Operator Lists for more information. Code generation is also supported. oneflow_onnx is able to generate OneFlow code and transforming models simultaneously . Please refer to X2OneFlow Code Generation List for more information. To sum up, OneFlow2ONNX can support over 80 ONNX OP X2OneFlow can support 80 ONNX OP, 50+ TensorFlow OP, 80+ Pytorch OP, and 50+ PaddlePaddle OP which covers most operations when doing CV model classifications. Since the OPs and models we support are all in eager mode API, users are required to install versions of PaddlePaddle >= 2.0.0, TensorFlow >= 2.0.0, and there is no specific requirements for Pytorch. Until now, X2OneFlow has successfully transformed 50+ official models from TensorFlow/Pytorch/PaddlePaddle, and you're always welcomed to experience our product. Environment Dependencies \u00b6 User's Environment Configuration \u00b6 python> = 3 .5 onnx> = 1 .8.0 onnx-simplifier> = 0 .3.3 onnxoptimizer> = 0 .2.5 onnxruntime> = 1 .6.0 oneflow ( https://github.com/Oneflow-Inc/oneflow#install-with-pip-package ) If you'd llike to use X2OneFlow, the following versions of deep learning frameworks are needed: pytorch> = 1 .7.0 paddlepaddle> = 2 .0.0 paddle2onnx> = 0 .6 tensorflow> = 2 .0.0 tf2onnx> = 1 .8.4 Installation \u00b6 Method 1 \u00b6 pip install oneflow_onn Method 2 git clone https://github.com/Oneflow-Inc/oneflow_convert_tools cd oneflow_onnx python3 setup.py install Usage \u00b6 Please refer to Examples Related Documents \u00b6 OneFlow2ONNX Model List X2OneFlow Model List OneFlow2ONNX Operator List X2OneFlow Operator List Examples nchw2nhwc_tool \u00b6 Introduction \u00b6 This tool is to transform NCHW, which is trained through OneFlow, into NHWC Format. Please click here for more information save_serving_tool \u00b6 Introduction \u00b6 This tool is to transform OneFlow models into models that can be used on the Serving end. Please click here for more information","title":"OneFlow And ONNX Convert"},{"location":"extended_topics/oneflow_convert_tools.html#oneflow_convert_tools","text":"","title":"oneflow_convert_tools"},{"location":"extended_topics/oneflow_convert_tools.html#oneflow_onnx","text":"","title":"oneflow_onnx"},{"location":"extended_topics/oneflow_convert_tools.html#introduction","text":"oneflow_onnx tool package includes two major functions: one is to export OneFlow out of ONNX, while the other is to transform ONNX models, which are obtained from other training frameworks, into Oneflow models. This tool package has already been adapted to TensorFlow/Pytorch/PaddlePaddle pre-trained models. The process of oneflow_onnx extracting ONNX and transforming it into OneFlow's format is called X2OneFlow (X representing TensorFlow/Pytorch/PaddlePaddle). OneFlow2ONNX models are supported. Specifically, OneFlow's lazy mode model can be transfomed into ONNX's format. Transformable OneFlow model can be obtained by using the method explained on flow.checkpoint.save . For more information, please refer to OneFlow2ONNX Model List . X2OneFlow models are supported. TensorFlow/Pytorch/PaddlePaddle model can be transformed into OneFlow's format through ONNX. OneFlow2ONNX operators are supported. Currently, oneflow_onnx is fully capable of exporting ONNX Opset10, and parts of OneFlow operator can transform ONNX Opsets that are in lower order. Please refer to OneFlow2ONNX Operator Lists for more information. X2OneFlow operators are supported. Currently, oneflow_onnx is fully capable of supporting most CV operators in TensorFlow/Pytorch/PaddlePaddle. Please refer to X2OneFlow Operator Lists for more information. Code generation is also supported. oneflow_onnx is able to generate OneFlow code and transforming models simultaneously . Please refer to X2OneFlow Code Generation List for more information. To sum up, OneFlow2ONNX can support over 80 ONNX OP X2OneFlow can support 80 ONNX OP, 50+ TensorFlow OP, 80+ Pytorch OP, and 50+ PaddlePaddle OP which covers most operations when doing CV model classifications. Since the OPs and models we support are all in eager mode API, users are required to install versions of PaddlePaddle >= 2.0.0, TensorFlow >= 2.0.0, and there is no specific requirements for Pytorch. Until now, X2OneFlow has successfully transformed 50+ official models from TensorFlow/Pytorch/PaddlePaddle, and you're always welcomed to experience our product.","title":"Introduction"},{"location":"extended_topics/oneflow_convert_tools.html#environment-dependencies","text":"","title":"Environment Dependencies"},{"location":"extended_topics/oneflow_convert_tools.html#users-environment-configuration","text":"python> = 3 .5 onnx> = 1 .8.0 onnx-simplifier> = 0 .3.3 onnxoptimizer> = 0 .2.5 onnxruntime> = 1 .6.0 oneflow ( https://github.com/Oneflow-Inc/oneflow#install-with-pip-package ) If you'd llike to use X2OneFlow, the following versions of deep learning frameworks are needed: pytorch> = 1 .7.0 paddlepaddle> = 2 .0.0 paddle2onnx> = 0 .6 tensorflow> = 2 .0.0 tf2onnx> = 1 .8.4","title":"User's Environment Configuration"},{"location":"extended_topics/oneflow_convert_tools.html#installation","text":"","title":"Installation"},{"location":"extended_topics/oneflow_convert_tools.html#method-1","text":"pip install oneflow_onn Method 2 git clone https://github.com/Oneflow-Inc/oneflow_convert_tools cd oneflow_onnx python3 setup.py install","title":"Method 1"},{"location":"extended_topics/oneflow_convert_tools.html#usage","text":"Please refer to Examples","title":"Usage"},{"location":"extended_topics/oneflow_convert_tools.html#related-documents","text":"OneFlow2ONNX Model List X2OneFlow Model List OneFlow2ONNX Operator List X2OneFlow Operator List Examples","title":"Related Documents"},{"location":"extended_topics/oneflow_convert_tools.html#nchw2nhwc_tool","text":"","title":"nchw2nhwc_tool"},{"location":"extended_topics/oneflow_convert_tools.html#introduction_1","text":"This tool is to transform NCHW, which is trained through OneFlow, into NHWC Format. Please click here for more information","title":"Introduction"},{"location":"extended_topics/oneflow_convert_tools.html#save_serving_tool","text":"","title":"save_serving_tool"},{"location":"extended_topics/oneflow_convert_tools.html#introduction_2","text":"This tool is to transform OneFlow models into models that can be used on the Serving end. Please click here for more information","title":"Introduction"},{"location":"extended_topics/user_op.html","text":"Create an New Operator \u00b6 Background \u00b6 What is a Custom Op \u00b6 OneFlow abstracts all kinds of data processing into op (operator). Op acts on the input tensor and writes the result of the operation to the output tensor. OneFlow provides relatively comprehensive ops and they can be found in ops directory . When OneFlow's existing Python operators are not sufficient to build a neural network or when Python operators do not meet performance requirements. You can use C++ to develop custom op in OneFlow. OneFlow provides a mechanism with which you can create custom op and register it in OneFlow then use custom op in Python. The following diagram demonstrates the registration system for a custom op in OneFlow. In the OneFlow framework, there are three types of registries associated with custom op. OpGradRegistry \uff1aManage gradient registration for automatic gradient calculation in backward graph. OpRegistry \uff1aManage op registrations for generating forward digraph and building Task Graph . OpKernelRegistry \uff1aManage kernel registrations for performing user logic at runtime. We actually write custom op in C++ and generate a dynamic link library (so file). By loading the corresponding so file in Python that you can use the custom op. The data structure of user op can be viewed at user_op_conf.proto \uff1a syntax = \"proto2\"; package oneflow; import \"oneflow/core/framework/user_op_attr.proto\"; message UserOpConf { message ListString { repeated string s = 1; } required string op_type_name = 1; map<string, ListString> input = 2; map<string, ListString> output = 3; map<string, UserOpAttrVal> attr = 4; } The op_type_name is a string which representing the class of op and indicate the globally unique ID of the op class. OneFlow queries and confirms the op class by op_type_name which will appear several times in the rest of this document. Basic Concepts \u00b6 Op_type_name\uff1aAs mentioned above, op_type_name is the unique ID of op class. OneFlow queries and confirms op class by op_type_name, and then instantiates the op. The relationship between op class and op is similar to the relationship between class and object. Op\uff1aLogical operators contain information of input and output shapes for mapping and reasoning, but do not contain logic for processing the data. Kernel\uff1aWhen a logical op running, the processing logic will affect by physical device and data type. The specific processing logic is done by the kernel. Generally op has a one-to-many relationship with the kernel and we need to register the kernel for all the physical devices and data types that op supports. Registration\uff1aRegistration can be used to establish a link between a custom op and the OneFlow framework. A series of macros named REGISTER_XXX are provided in OneFlow to help with registration of op. Loading the dynamic library\uff1aThe custom op and its kernel are linked as dynamic library so files that need to be loaded before using them in Python and OneFlow provides oneflow.config.load_library to load the so files of custom op. Python wrapper\uff1aCalling a custom op implemented at the C++ layer in Python requires writing a wrapper at the Python layer and OneFlow provides oneflow.user_op_builder to do this task. Process of Writing a Custom Op \u00b6 Implementation and registration of op\uff1aThe implementation of op is primarily used for forward digraph composition which includes specifying the name of op, inputs, outputs, configuration attributes and the necessary functions to infer the shape and data type of the tensor. Implementation and registration of the kernel for an op: The kernel is responsible for the specific computational process during running and an op may correspond to multiple kernels (optional) Implementation and registration of op's corresponding grad: If the custom op needs to support backward spreading. Then we need to implement and register a backward function for it. Compile and link to get so file Load the so file in Python and use oneflow.user_op_builder to wrap a custom op written in C++. Testing. Example \u00b6 We will implement a custom op called \"myrelu\" which supports both CPU and GPU operations. For the complete code please refer to: code/extended_topics/create_user_op . Implementation and Registration of Op \u00b6 We defined op and completed the registration in myrelu_op.cpp : #include \"oneflow/core/framework/framework.h\" namespace oneflow { namespace { REGISTER_USER_OP ( \"myrelu\" ) . Input ( \"in\" ) . Output ( \"out\" ) . SetTensorDescInferFn ( []( user_op :: InferContext * ctx ) -> Maybe < void > { * ctx -> Shape4ArgNameAndIndex ( \"out\" , 0 ) = * ctx -> Shape4ArgNameAndIndex ( \"in\" , 0 ); * ctx -> Dtype4ArgNameAndIndex ( \"out\" , 0 ) = * ctx -> Dtype4ArgNameAndIndex ( \"in\" , 0 ); return Maybe < void >:: Ok (); }); } // namespace } // namespace oneflow Analysis of the above codes: oneflow/core/framework/framework.h contains all the controllers we need to create an op. Almost all the APIs related to user op are in the namespace oneflow::user_op , so we use the namespace oneflow to simplify the type name. The macro REGISTER_USER_OP is used to register the op and accepts myrelu as op_type_name . After registering with REGISTER_USER_OP , it actually returns an OpRegistry class (path: oneflow\\coreframework\\user_op_registry.h ) which can be called to complete the setting of a custom op: Input(\"in\") means that it has an input named \"in\". Output(\"out\") means that it has an output named \"out\". SetTensorDescInferFn is used to set the shape and data type of the inferring function which describe the relationship between the input of this operator and shape and type of the output of this operator. In the above code, the shape and data type of the output is consistent with input. Implementation and Registration of CPU Kernel \u00b6 We implemented the CPU kernel in myrelu_cpu_kernel.cpp and registered it\uff1a #include \"oneflow/core/framework/framework.h\" namespace oneflow { namespace { template < typename T > void MyRelu ( DeviceCtx * ctx , const int64_t n , const T * x , T * y ) { T zero = ( T )( 0 ); for ( int64_t i = 0 ; i != n ; ++ i ) { y [ i ] = std :: max ( x [ i ], zero ); } } template < DeviceType device_type , typename T > class ReluKernel final : public user_op :: OpKernel { public : ReluKernel () = default ; ~ ReluKernel () = default ; private : void Compute ( user_op :: KernelComputeContext * ctx ) const override { const user_op :: Tensor * in_tensor = ctx -> Tensor4ArgNameAndIndex ( \"in\" , 0 ); user_op :: Tensor * out_tensor = ctx -> Tensor4ArgNameAndIndex ( \"out\" , 0 ); MyRelu < T > ( ctx -> device_ctx (), in_tensor -> shape (). elem_cnt (), in_tensor -> dptr < T > (), out_tensor -> mut_dptr < T > ()); } bool AlwaysComputeWhenAllOutputsEmpty () const override { return false ; } }; #define REGISTER_RELU_KERNEL(device, dtype) \\ REGISTER_USER_KERNEL(\"myrelu\") \\ .SetCreateFn<ReluKernel<device, dtype>>() \\ .SetIsMatchedHob( \\ (user_op::HobDeviceTag() == device) & \\ (user_op::HobDataType(\"out\", 0) \\ == GetDataType<dtype>::value)); REGISTER_RELU_KERNEL ( DeviceType :: kCPU , float ) REGISTER_RELU_KERNEL ( DeviceType :: kCPU , double ) } // namespace } // namespace oneflow To implement the kernel in OneFlow, you must define a class which inherits from oneflow::user_op::OpKernel and rewrite the virtual functions of it. In the above code, we rewrite Compute and AlwaysComputeWhenAllOutputsEmpty and their respective meanings are: Compute must be rewritten to implement the specific operating logic. AlwaysComputeWhenAllOutputsEmpty must be rewritten to return false in most cases. For very few ops that need to maintain state internally, and therefore need to call the kernel for calculation even if the output is empty, it should return true . After implementing the kernel class, you need to call REGISTER_USER_KERNEL to register it. The string parameter that REGISTER_USER_KERNEL(\"myrelu\") accepts is op_type_name which is used to complete registration and querying. You also need to use op_type_name when wrapping op at the Python layer. REGISTER_USER_KERNEL(\"myrelu\") returns an OpKernelRegistry object. The methods that need to be called to set the registration information are mention in the code above. SetCreateFn<T>() : The method of this template's parameter T is our implementation of the kernel class which OneFlow will use it to create the kernel object. SetIsMatchedHob \uff1aBecause an op may have more than one kernels. You need to call SetIsMatchedHob to select a specific kernel for the calculation according to the physical device and data format. This method accepts an expression and when the expression is true , OneFlow will call the kernel to complete the calculation. Implementation and Registration of GPU Kernel \u00b6 We implemented the GPU version of the kernel in myrelu_gpu_kernel.cu and registered it\uff1a #include \"oneflow/core/framework/framework.h\" #include <cub/cub.cuh> namespace oneflow { namespace { template < typename T > __global__ void ReluForwardGpu ( const int n , const T * x , T * y ) { CUDA_1D_KERNEL_LOOP ( i , n ) { y [ i ] = x [ i ] > 0 ? x [ i ] : 0 ; } } class ReluGpuFloatKernel final : public user_op :: OpKernel { public : ReluGpuFloatKernel () = default ; ~ ReluGpuFloatKernel () = default ; private : void Compute ( user_op :: KernelComputeContext * ctx ) const override { const user_op :: Tensor * in_tensor = ctx -> Tensor4ArgNameAndIndex ( \"in\" , 0 ); user_op :: Tensor * out_tensor = ctx -> Tensor4ArgNameAndIndex ( \"out\" , 0 ); int32_t n = in_tensor -> shape (). elem_cnt (); const float * in_ptr = in_tensor -> dptr < float > (); float * out_ptr = out_tensor -> mut_dptr < float > (); ReluForwardGpu < float > <<< 32 , 1024 , 0 , ctx -> device_ctx () -> cuda_stream () >>> ( n , in_ptr , out_ptr ); } bool AlwaysComputeWhenAllOutputsEmpty () const override { return false ; } }; #define REGISTER_RELU_KERNEL(device, dtype) \\ REGISTER_USER_KERNEL(\"myrelu\") \\ .SetCreateFn<ReluGpuFloatKernel>() \\ .SetIsMatchedHob( \\ (user_op::HobDeviceTag() == device) & \\ (user_op::HobDataType(\"out\", 0) \\ == GetDataType<dtype>::value)); REGISTER_RELU_KERNEL ( DeviceType :: kGPU , float ) REGISTER_RELU_KERNEL ( DeviceType :: kGPU , double ) } // namespace } // namespace oneflow The process of implementing and registering the GPU kernel is almost identical to the CPU kernel. The main differences are: Because CUDA programming is used, the CUDA header files are included. Compute uses GPU methods. SetIsMatchedHob set the matching device as GPU. Besides that, because of the use of CUDA, we need to use the nvcc compiler (instead of g++) to compile the GPU kernel. Compiling Option Description \u00b6 The oneflow.sysconfig contains the get_compile_flags , get_include , get_lib , and get_link_flags which corresponding to: Compiling Options Dictionary of header file Dictionary of link library Linking options For example\uff1a >>> import oneflow >>> oneflow.sysconfig.get_compile_flags() ['-I/home/yaochi/oneflow/build/python_scripts/oneflow/include', '-DHALF_ENABLE_CPP11_USER_LITERALS=0', '-DWITH_CUDA', '-D_GLIBCXX_USE_CXX11_ABI=0'] You can also get compile and link options directly by using command\uff1a python -c \"import oneflow; print(' '.join(oneflow.sysconfig.get_compile_flags()))\" python -c \"import oneflow; print(' '.join(oneflow.sysconfig.get_link_flags()))\" For the GPU kernel, the cudart library also needs to be specified when linking. Get Dynamic Library by Compilation and Linking \u00b6 For this simple example, you can use the following Makefile to build: CFLAGS = $( shell python -c \"import oneflow; print(' '.join(oneflow.sysconfig.get_compile_flags()))\" ) LFLAGS = $( shell python -c \"import oneflow; print(' '.join(oneflow.sysconfig.get_link_flags()))\" ) CUDAPATH = /usr/local/cuda-10.1/lib64 all: final_relu.so myrelu_op.o: myrelu_op.cpp g++ -std = c++11 -c myrelu_op.cpp \\ -o myrelu_op.o \\ -fPIC \\ ${ CFLAGS } \\ ${ LFLAGS } \\ -O2 myrelu_cpu_kernel.o: myrelu_cpu_kernel.cpp g++ -std = c++11 -c myrelu_cpu_kernel.cpp \\ -o myrelu_cpu_kernel.o \\ $( CFLAGS ) -fPIC myrelu_gpu_kernel.o: myrelu_gpu_kernel.cu nvcc -std = c++11 -c myrelu_gpu_kernel.cu \\ -o myrelu_gpu_kernel.o \\ $( CFLAGS ) -x cu -Xcompiler -fPIC final_relu.so: myrelu_op.o myrelu_cpu_kernel.o myrelu_gpu_kernel.o g++ -std = c++11 myrelu_op.o \\ myrelu_cpu_kernel.o \\ myrelu_gpu_kernel.o \\ -shared -o final_relu.so \\ $( CFLAGS ) \\ -fPIC \\ -L $( CUDAPATH ) \\ -lcudart \\ $( LFLAGS ) clean: rm -rf *.so *.o We use g++ to compile myrelu_op.cpp and myrelu_cpu_kernel.cpp , use nvcc to compile myrelu_gpu_kernel.cpp . Then get the target file (\".o\" file) and link the target file to final_ relu.so . We are going to load final_relu.so in Python then use wrappers and custom op. Using the Custom Op in Python \u00b6 Using a custom op in Python needs the following steps: Load the so file by oneflow.config.load_library . Use oneflow.user_op_builder to generating Python wrapper for custom op. Call the above result of Python wrapper. The following code encapsulates myrelu at the Python layer and call it: import oneflow as flow import numpy as np import oneflow.typing as tp # load modules flow . config . load_library ( \"final_relu.so\" ) # default configuration flow . config . gpu_device_num ( 1 ) # python op wrapper function def myrelu ( input_blob ): op = ( flow . user_op_builder ( \"op_myrelu\" ) . Op ( \"myrelu\" ) . Input ( \"in\" , [ input_blob ]) . Output ( \"out\" ) . Build () ) return op . InferAndTryRun () . SoleOutputBlob () # network code @flow . global_function () def MyJob ( x : tp . Numpy . Placeholder (( 5 ,), dtype = flow . float32 )) -> tp . Numpy : return myrelu ( x ) if __name__ == \"__main__\" : input = np . array ([ - 2 , - 1 , 0 , 1 , 2 ], dtype = np . float32 ) output = MyJob ( input ) print ( input ) print ( output ) The expected results are\uff1a [-2. -1. 0. 1. 2.] [0. 0. 0. 1. 2.] In the above code: flow.config.load_library(\"final_relu.so\") is to load the so file. We are focus on the process of building and running the python wrapper in myrelu . flow.user_op_builder(\"op_myrelu\") actually returns a UserOpConfBuilder object named op_myrelu . op = ( flow . user_op_builder ( \"op_myrelu\" ) . Op ( \"myrelu\" ) . Input ( \"in\" , [ input_blob ]) . Output ( \"out\" ) . Build () ) This object contains Op , Input and and etc methods which are used to encapsulate custom op. Details explanation are as follows: Op(\"myrelu\") : The parameter must be the op_type_name from the previous C++ registration which OneFlow uses to find the registered op type and instantiate the op. Input(\"in\", [input_blob]) : Corresponds to Input when op is registered in C++ that the first parameter must be the same as the string set by Input when op is registered in C++. The second parameter is the blob of the input which is a list . Because an op allows multiple inputs. Output(\"out\") : Corresponds to Output when op registered in C++. Build \uff1aAfter the above settings are complete, call Build to get the Python wrapper from the custom op. The following code will get the blob of the custom op: return op . InferAndTryRun () . SoleOutputBlob () InferAndTryRun completes the derivation and returns UserOp . If the returned blob has only one output. We cab use SoleOutputBlob to get the unique output. Otherwise use RemoteBlobList to get a list of multiple blobs. So far, we have built the myrelu which is a relatively simple op. But if we need to build a more complex op, we should use some additional features in the registration process. We'll introduce it from the aspects of op registration, kernel registration, gradient registration and Python layer wrapping. Detailed Introduction of OpRegistry \u00b6 Attr \u00b6 Some ops require configuration properties in addition to inputs and outputs. For example, the reshape needs to be configured the shape and the conv needs to be configured the alignment method. We can use the Attr at registration to set attributes for op. For example: OpRegistry & Attr < cpp_type > ( const std :: string & name ); We just need to specify the name and type of the attribute. For example\uff1a REGISTER_USER_OP ( \"reshape\" ) . Input ( \"in\" ) . Output ( \"out\" ) . Attr < shape > ( \"shape\" ) REGISTER_USER_OP ( \"conv2d\" ) . Input ( \"in\" ) . Input ( \"weight\" ) . Output ( \"out\" ) . Attr < std :: vector < int32_t >> ( \"padding_before\" ) In OneFlow, we currently support the following C++ data: UserOpAttrType Corresponding C++ data types kAtInt32 int32_t kAtInt64 int64_t kAtBool bool kAtFloat float kAtDouble double kAtShape oneflow::Shape kAtListInt32 std::vector kAtListInt64 std::vector kAtListFloat std::vector< float > kAtString std::string We can pass an additional parameter and configure a default value for it which is the corresponding C++ datatype in the table. Such as\uff1a . Attr < bool > ( \"is_transpose\" , false ) . Attr < int32_t > ( \"size\" , 10 ) . Attr < std :: vector < int32_t >> ( \"vector_of_size\" , std :: vector < int32_t > { 10 , 11 , 12 }) SetCheckAttrFn \u00b6 For some Attributes , they require a more detailed delineation of the range which can be specified by SetCheckAttrFn when registering the Op. Take Conv op as an example, it has a configuration option called data_format which is a string type but the data must be channels_first or channels_last . . Attr < std :: string > ( \"data_format\" , std :: string ( \"NCHW\" )) . SetCheckAttrFn ( []( const user_op :: UserOpDefWrapper & def , const user_op :: UserOpConfWrapper & conf ) -> Maybe < void > { std :: string data_format = conf . attr < std :: string > ( \"data_format\" ); if ( data_format == \"channels_first\" || data_format == \"channels_last\" ) { return Maybe < void >:: Ok (); } return oneflow :: Error :: CheckFailed () << \"data_format value: \" << data_format << \" for Conv op is illegal.\" ; }) Set a function to check that returns Maybe<void>::Ok() when the value of the attribute matches the requirement. Otherwise returns oneflow::Error::CheckFailed() . Multiple In/Output \u00b6 For some ops, they may have multiple input or output and we need to specify the number of inputs and outputs when we register it. Input example\uff1a // input must have 1 blob . Input ( \"input\" ) // input must have 5 blobs . Input ( \"input\" , 5 ) // input input must have at least 5 blobs . InputWithMinimum ( \"input\" , 5 ) // input can have no blob or 1 blob . OptionalInput ( \"input\" ) // input can have no blob or 5 blobs . OptionalInput ( \"input\" , 5 ) // input can have no blob or at least 5 blobs . OptionalInputWithMininum ( \"input\" , 5 ) Output setting is similar to Input. SetGetSbpFn \u00b6 SetGetSbpFn is for config the SBP of this op . Example of \"add_n\"\uff1a REGISTER_USER_OP ( \"add_n\" ) . InputWithMinimum ( \"in\" , 2 ) . Output ( \"out\" ) . SetGetSbpFn ([]( user_op :: SbpContext * ctx ) { int64_t num_axes = ctx -> LogicalTensorDesc4InputArgNameAndIndex ( \"in\" , 0 ). shape (). NumAxes (); for ( int64_t i = 0 ; i < num_axes ; ++ i ) { ctx -> NewBuilder (). Split ( ctx -> inputs (), i ). Split ( user_op :: OpArg ( \"out\" , 0 ), i ). Build (); } ctx -> NewBuilder (). PartialSum ( ctx -> inputs ()). PartialSum ( user_op :: OpArg ( \"out\" , 0 )). Build (); return Maybe < void >:: Ok (); }); Detailed Introduction of OpKernelRegistry \u00b6 SetInferTmpSizeFn \u00b6 In some kernel implementations of op, some extra buffer may be required to store temporary data during the Compute . We can specify the buffer size when registering the kernel by using the SetInferTmpSizeFn . Then we get the buffer and use it in the Compute function. The following code registers the kernel with SetInferTmpSizeFn to specify a buffer size as 1024 bytes: REGISTER_USER_KERNEL ( \"XOp\" ) . SetInferTmpSizeFn ( []( const oneflow :: user_op :: InferContext * ) { return 1024 ; }); Once the buffer size is set by SetInferTmpSizeFn , this buffer can be retrieved in Compute by calling the KernelContext::Tensor4ArgNameAndIndex . This buffer is encapsulated as oneflow::user_op::Tensor which can be converted to other types of pointers by calling the dptr or mut_dptr . class XKernel final : public oneflow :: user_op :: OpKernel { void Compute ( oneflow :: user_op :: KernelContext * ctx ) override { oneflow :: user_op :: Tensor * tmp = ctx -> Tensor4ArgNameAndIndex ( \"tmp_buffer\" , 0 ); //The conversion yields a char* buffer of 1024 bytes. char * pBuff = tmp -> mut_dptr < char > (); ... } }; Detailed Introduction of OpGradRegistry \u00b6 Oneflow is automatically get gradient during backward map expansion and the OneFlow framework uses Automatic Differentiation to get the gradient which means automatically find the gradient of the entire expression using the chain rule. In order to automatically get gradient a custom op, we need to register it with REGISTER_USER_OP_GRAD . From a mathematical point of view, the registration process is the computation of the backward derivation that we specify for our custom op. From a programming point of view, it is to set up a backward-generating function for a custom op. Within that function, we write code that specifies how the input gradient of that op is to be calculated. In order to calculate the gradient of a custom op, we need to construct the gradient of the input base on the input and output of the custom op. In most cases, we can represent the process of calculating the gradient of the input through the existing operators and their combination in OneFlow. The calculation of the input gradient usually consists of the following steps: Use ctx->DefineOp() and BackwardOpBuilder to represent methods for calculating input gradients. Because input gradient calculations may be combinations of multiple operations. Therefore DefineOp and BackwardOpBuilder may be used for multiple times. After defining the calculation process in the previous step, the required gradient is finally recorded in the output of some operator. We need to call the ctx->FwOp().InputGradBind() to combine the result of the previous calculation to the input gradient of the custom op. The following example (the complete code, including tests, can be found in myop_grad repository ). A custom op called myop will be used to register backward generating functions. This op is only used in this document to show the registration process which compute function is set as 3*x*x . Then it is easy to obtain the relationship between its forward and backward propagation as shown below. The gradient of x in the reverse process is computed as 6*x*dy . The forward op of myop is defined as follows: REGISTER_USER_OP ( \"myop\" ). Input ( \"in\" ). Output ( \"out\" ). SetTensorDescInferFn ( []( user_op :: InferContext * ctx ) -> Maybe < void > { * ctx -> Shape4ArgNameAndIndex ( \"out\" , 0 ) = * ctx -> Shape4ArgNameAndIndex ( \"in\" , 0 ); * ctx -> Dtype4ArgNameAndIndex ( \"out\" , 0 ) = * ctx -> Dtype4ArgNameAndIndex ( \"in\" , 0 ); return Maybe < void >:: Ok (); }); That is myop contains the only input in and the only output out . The reverse gradient registration of myop is as follows: REGISTER_USER_OP_GRAD ( \"myop\" ). SetBackwardOpConfGenFn ( []( user_op :: BackwardOpConfContext * ctx ) { const auto op1_name = ctx -> FwOp (). op_name () + \"_grad1\" ; // The operator op1_name is used to calculate the gradient of myop.in ctx -> DefineOp ( op1_name , [ & ctx ]( user_op :: BackwardOpBuilder & builder ) { return builder . OpTypeName ( \"multiply\" ) . InputBind ( \"x\" , ctx -> FwOp (). input ( \"in\" , 0 )) //multiply.x <- myop.in . InputBind ( \"y\" , ctx -> FwOp (). output_grad ( \"out\" , 0 )) //multiply.y <- the gradient of myop.out . Output ( \"out\" ) . Build (); }); const auto op2_name = ctx -> FwOp (). op_name () + \"_grad2\" ; // The operator op2_name is used to calculate 6*op1_name. ctx -> DefineOp ( op2_name , [ & ctx , & op1_name ]( user_op :: BackwardOpBuilder & builder ) { return builder . OpTypeName ( \"scalar_mul\" ) . InputBind ( \"in\" , ctx -> GetOp ( op1_name ). output ( \"out\" , 0 )) . Attr ( \"has_float_operand\" , true ) . Attr ( \"has_int_operand\" , false ) . Attr ( \"float_operand\" , static_cast < double > ( 6 )) . Attr ( \"int_operand\" , static_cast < int64_t > ( 6 )) . Output ( \"out\" ) . Build (); }); // (the gradient of myop.in) <- op1_name.out ctx -> FwOp (). InputGradBind ( user_op :: OpArg ( \"in\" , 0 ), [ & ctx , & op2_name ]() -> const std :: string & { return ctx -> GetOp ( op2_name ) . output ( \"out\" , 0 ); }); }); The string parameter accepted by REGISTER_USER_OP_GRAD(\"myop\") is op_type_name which needs to be the same as registered with REGISTER_USER_OP . REGISTER_USER_OP_GRAD(\"myop\") returns an oneflow::user_op::OpGradRegistry object that we can call it to set the custom op's backward generating function. In the above gradient registration process, the expression for the gradient of myop is 6*x*dy which is demonstrated in the code. First op1_name is defined and x*dy is solved by using the existing operator multiply : // The operator op1_name is used to calculate the gradient of myop.in ctx -> DefineOp ( op1_name , [ & ctx ]( user_op :: BackwardOpBuilder & builder ) { return builder . OpTypeName ( \"multiply\" ) . InputBind ( \"x\" , ctx -> FwOp (). input ( \"in\" , 0 )) //multiply.x <- myop.in . InputBind ( \"y\" , ctx -> FwOp (). output_grad ( \"out\" , 0 )) //multiply.y <- myop.out\u7684\u68af\u5ea6 . Output ( \"out\" ) . Build (); }); Then op2_name is defined and use the existing operator op2_name to solve for 6*op1_name . // The operator op2_name is used to calculate 6*op1_name. ctx -> DefineOp ( op2_name , [ & ctx , & op1_name ]( user_op :: BackwardOpBuilder & builder ) { return builder . OpTypeName ( \"scalar_mul\" ) . InputBind ( \"in\" , ctx -> GetOp ( op1_name ). output ( \"out\" , 0 )) . Attr ( \"has_float_operand\" , true ) . Attr ( \"has_int_operand\" , false ) . Attr ( \"float_operand\" , static_cast < double > ( 6 )) . Attr ( \"int_operand\" , static_cast < int64_t > ( 6 )) . Output ( \"out\" ) . Build (); }); Finally bind the output of op2_name (i.e., 6*x*dy ) to the input of myop to complete the registration. // (the gradient of myop.in) <- op1_name.out ctx -> FwOp (). InputGradBind ( user_op :: OpArg ( \"in\" , 0 ), [ & ctx , & op2_name ]() -> const std :: string & { return ctx -> GetOp ( op2_name ) . output ( \"out\" , 0 ); }); The above code is the complete process of registering a gradient and the related classes and methods will be described in below. SetBackwardOpConfGenFn \u00b6 We use OpGradRegistry::SetBackwardOpConfGenFn(fn) to set the backward generating function fn which has the following prototype: void fn ( BackwardOpConfContext * ctx ); BackwardOpConfContext* ctx has all information needed to generate the op. BackwardOpConfContext \u00b6 The common methods and their purpose used in BackwardOpConfContext as follows: UserOpWrapper& FwOp(); : Get forward op. GetOp(op_name) : Create and get the corresponding op based on op_name . GetOp uses a lazy init mechanism and the corresponding op is not actually created until GetOp is called. void DefineOp(op_name, fn) \uff1aDefine fn of the op named op_name . When ctx->GetOp(op_name) is called, fn is triggered in the OneFlow for Op creation and if the op has already been created. Then the result is retrieved directly. The fn receives a BackwardOpBuilder parameter for constructing the reverse op. We will introduce BackwardOpBuilder later on. Detailed Introduction of BackwardOpBuilder \u00b6 BackwardOpBuilder is used to build a reverse op. The fragment of above code is an example: ctx -> DefineOp ( op1_name , [ & ctx ]( user_op :: BackwardOpBuilder & builder ) { return builder . OpTypeName ( \"multiply\" ) . InputBind ( \"x\" , ctx -> FwOp (). input ( \"in\" , 0 )) //multiply.x <- myop.in . InputBind ( \"y\" , ctx -> FwOp (). output_grad ( \"out\" , 0 )) //multiply.y <- myop.out\u7684\u68af\u5ea6 . Output ( \"out\" ) . Build (); }); In this function, we call Build to build a reverse op for computing x*dy . The purpose of each operator is as follows: OpTypeName(\"multiply\") specifies the op_type_name of an op that is used to help us compute the reverse gradient. InputBind(arg_name, blob) binds the input arg_name of multiply to the specified blob and can be called for multiple times. If the arg_name corresponds to multiple blob which means the order of Input is the order of the corresponding index. Output(arg_name, num) Specifies the number of output blobes that actually correspond to the arg_name which defaults to 1 if num is not filled in. Attr(attr_name, val) sets the value of the attribute which same in the registration. Calling Build() after above configuration, then the construction of the reverse op is completed. Detailed Introduction of UserOpWrapper \u00b6 Calling ctx->FwOp() will return the UserOpWrapper of myop and complete the gradient binding by calling the UserOpWrapper . ctx -> FwOp (). InputGradBind ( user_op :: OpArg ( \"in\" , 0 ), [ & ctx , & op2_name ]() -> const std :: string & { return ctx -> GetOp ( op2_name ) . output ( \"out\" , 0 ); }); Common methods for UserOpWrapper are: InputGradBind(input, grad_fn) \uff1aBind the input of the forward op and get the gradient function grad_fn . OneFlow automatically determines whether input needs to generate a backward gradient, if needed, it will trigger grad_fn and binds the input. input(arg_name, index) \uff1aGet the blob corresponding to the arg_name of input. output(arg_name,index) \uff1aGet the blob corresponding to the arg_name of output. output_grad(output_arg_name, index) \uff1aGet the output_arg_name of the forward op which is the blob of the corresponding backward gradient. attr(attr_name) \uff1aGet the value corresponding to the attr_name . arg_tensor_desc(arg_name, index) \uff1aReturns the input/output tensor information of the forward op which including shape , dtype and etc. Customized Op for Calculating Gradients \u00b6 As we mentioned earlier, in most cases, the process of calculating a gradient can be represented by a combination of existing ops. However, when it is difficult to use an existing op to solve the gradient for a particular forward op that we need to design and create operators specifically for the gradient calculation. Example can be found in: relu_op.cpp . Detailed Introduction of UserOpConfBuilder \u00b6 In Python frontend of OneFlow, we provide UserOpConfBuilder to build the wrapper of custom op which is used in Use custom opp in Python previously. Here is the summary of the relationship between UserOpConfBuilder in Python layer and C++ layer. For example, we have wrapped a cast : def cast ( x , dtype , name ): return ( flow . user_op_builder ( name ) . Op ( \"cast\" ) . Input ( \"in\" , [ x ]) . Output ( \"out\" ) . Attr ( \"dtype\" , dtype ) . Build () . InferAndTryRun () . RemoteBlobList ()[ 0 ] ) ) Op(op_type_name) \uff1aThe accepted parameter is op_type_name when it is registered in C++. Input(input_name, input_blob_list) \uff1a input_name should be the same as the first parameter of Input when registering this op in C++. Output(output_name, num=1) \uff1a output_name and num should be the same as Output of op when registration in C++. Attr(attr_name, attr_value) \uff1a attr_name corresponds to the attribute of OpRegistry::Attr used for C++ registration and attr_value should be the same type as the attribute type when declaration. Build() \uff1aBuild the user op for the Python layer. The derivation can be done by calling InferAndTryRun in the user op and the result can be retrieved by calling RemoteBlobList or SoleOutputBlob . RemoteBlobList \uff1aGet all outputs which applies to op with multiple outputs and all ops are placed in a list. SoleOutputBlob \uff1aGet unique outputs which applies to op with one output.","title":"User Defined OP"},{"location":"extended_topics/user_op.html#create-an-new-operator","text":"","title":"Create an New Operator"},{"location":"extended_topics/user_op.html#background","text":"","title":"Background"},{"location":"extended_topics/user_op.html#what-is-a-custom-op","text":"OneFlow abstracts all kinds of data processing into op (operator). Op acts on the input tensor and writes the result of the operation to the output tensor. OneFlow provides relatively comprehensive ops and they can be found in ops directory . When OneFlow's existing Python operators are not sufficient to build a neural network or when Python operators do not meet performance requirements. You can use C++ to develop custom op in OneFlow. OneFlow provides a mechanism with which you can create custom op and register it in OneFlow then use custom op in Python. The following diagram demonstrates the registration system for a custom op in OneFlow. In the OneFlow framework, there are three types of registries associated with custom op. OpGradRegistry \uff1aManage gradient registration for automatic gradient calculation in backward graph. OpRegistry \uff1aManage op registrations for generating forward digraph and building Task Graph . OpKernelRegistry \uff1aManage kernel registrations for performing user logic at runtime. We actually write custom op in C++ and generate a dynamic link library (so file). By loading the corresponding so file in Python that you can use the custom op. The data structure of user op can be viewed at user_op_conf.proto \uff1a syntax = \"proto2\"; package oneflow; import \"oneflow/core/framework/user_op_attr.proto\"; message UserOpConf { message ListString { repeated string s = 1; } required string op_type_name = 1; map<string, ListString> input = 2; map<string, ListString> output = 3; map<string, UserOpAttrVal> attr = 4; } The op_type_name is a string which representing the class of op and indicate the globally unique ID of the op class. OneFlow queries and confirms the op class by op_type_name which will appear several times in the rest of this document.","title":"What is a Custom Op"},{"location":"extended_topics/user_op.html#basic-concepts","text":"Op_type_name\uff1aAs mentioned above, op_type_name is the unique ID of op class. OneFlow queries and confirms op class by op_type_name, and then instantiates the op. The relationship between op class and op is similar to the relationship between class and object. Op\uff1aLogical operators contain information of input and output shapes for mapping and reasoning, but do not contain logic for processing the data. Kernel\uff1aWhen a logical op running, the processing logic will affect by physical device and data type. The specific processing logic is done by the kernel. Generally op has a one-to-many relationship with the kernel and we need to register the kernel for all the physical devices and data types that op supports. Registration\uff1aRegistration can be used to establish a link between a custom op and the OneFlow framework. A series of macros named REGISTER_XXX are provided in OneFlow to help with registration of op. Loading the dynamic library\uff1aThe custom op and its kernel are linked as dynamic library so files that need to be loaded before using them in Python and OneFlow provides oneflow.config.load_library to load the so files of custom op. Python wrapper\uff1aCalling a custom op implemented at the C++ layer in Python requires writing a wrapper at the Python layer and OneFlow provides oneflow.user_op_builder to do this task.","title":"Basic Concepts"},{"location":"extended_topics/user_op.html#process-of-writing-a-custom-op","text":"Implementation and registration of op\uff1aThe implementation of op is primarily used for forward digraph composition which includes specifying the name of op, inputs, outputs, configuration attributes and the necessary functions to infer the shape and data type of the tensor. Implementation and registration of the kernel for an op: The kernel is responsible for the specific computational process during running and an op may correspond to multiple kernels (optional) Implementation and registration of op's corresponding grad: If the custom op needs to support backward spreading. Then we need to implement and register a backward function for it. Compile and link to get so file Load the so file in Python and use oneflow.user_op_builder to wrap a custom op written in C++. Testing.","title":"Process of Writing a Custom Op"},{"location":"extended_topics/user_op.html#example","text":"We will implement a custom op called \"myrelu\" which supports both CPU and GPU operations. For the complete code please refer to: code/extended_topics/create_user_op .","title":"Example"},{"location":"extended_topics/user_op.html#implementation-and-registration-of-op","text":"We defined op and completed the registration in myrelu_op.cpp : #include \"oneflow/core/framework/framework.h\" namespace oneflow { namespace { REGISTER_USER_OP ( \"myrelu\" ) . Input ( \"in\" ) . Output ( \"out\" ) . SetTensorDescInferFn ( []( user_op :: InferContext * ctx ) -> Maybe < void > { * ctx -> Shape4ArgNameAndIndex ( \"out\" , 0 ) = * ctx -> Shape4ArgNameAndIndex ( \"in\" , 0 ); * ctx -> Dtype4ArgNameAndIndex ( \"out\" , 0 ) = * ctx -> Dtype4ArgNameAndIndex ( \"in\" , 0 ); return Maybe < void >:: Ok (); }); } // namespace } // namespace oneflow Analysis of the above codes: oneflow/core/framework/framework.h contains all the controllers we need to create an op. Almost all the APIs related to user op are in the namespace oneflow::user_op , so we use the namespace oneflow to simplify the type name. The macro REGISTER_USER_OP is used to register the op and accepts myrelu as op_type_name . After registering with REGISTER_USER_OP , it actually returns an OpRegistry class (path: oneflow\\coreframework\\user_op_registry.h ) which can be called to complete the setting of a custom op: Input(\"in\") means that it has an input named \"in\". Output(\"out\") means that it has an output named \"out\". SetTensorDescInferFn is used to set the shape and data type of the inferring function which describe the relationship between the input of this operator and shape and type of the output of this operator. In the above code, the shape and data type of the output is consistent with input.","title":"Implementation and Registration of Op"},{"location":"extended_topics/user_op.html#implementation-and-registration-of-cpu-kernel","text":"We implemented the CPU kernel in myrelu_cpu_kernel.cpp and registered it\uff1a #include \"oneflow/core/framework/framework.h\" namespace oneflow { namespace { template < typename T > void MyRelu ( DeviceCtx * ctx , const int64_t n , const T * x , T * y ) { T zero = ( T )( 0 ); for ( int64_t i = 0 ; i != n ; ++ i ) { y [ i ] = std :: max ( x [ i ], zero ); } } template < DeviceType device_type , typename T > class ReluKernel final : public user_op :: OpKernel { public : ReluKernel () = default ; ~ ReluKernel () = default ; private : void Compute ( user_op :: KernelComputeContext * ctx ) const override { const user_op :: Tensor * in_tensor = ctx -> Tensor4ArgNameAndIndex ( \"in\" , 0 ); user_op :: Tensor * out_tensor = ctx -> Tensor4ArgNameAndIndex ( \"out\" , 0 ); MyRelu < T > ( ctx -> device_ctx (), in_tensor -> shape (). elem_cnt (), in_tensor -> dptr < T > (), out_tensor -> mut_dptr < T > ()); } bool AlwaysComputeWhenAllOutputsEmpty () const override { return false ; } }; #define REGISTER_RELU_KERNEL(device, dtype) \\ REGISTER_USER_KERNEL(\"myrelu\") \\ .SetCreateFn<ReluKernel<device, dtype>>() \\ .SetIsMatchedHob( \\ (user_op::HobDeviceTag() == device) & \\ (user_op::HobDataType(\"out\", 0) \\ == GetDataType<dtype>::value)); REGISTER_RELU_KERNEL ( DeviceType :: kCPU , float ) REGISTER_RELU_KERNEL ( DeviceType :: kCPU , double ) } // namespace } // namespace oneflow To implement the kernel in OneFlow, you must define a class which inherits from oneflow::user_op::OpKernel and rewrite the virtual functions of it. In the above code, we rewrite Compute and AlwaysComputeWhenAllOutputsEmpty and their respective meanings are: Compute must be rewritten to implement the specific operating logic. AlwaysComputeWhenAllOutputsEmpty must be rewritten to return false in most cases. For very few ops that need to maintain state internally, and therefore need to call the kernel for calculation even if the output is empty, it should return true . After implementing the kernel class, you need to call REGISTER_USER_KERNEL to register it. The string parameter that REGISTER_USER_KERNEL(\"myrelu\") accepts is op_type_name which is used to complete registration and querying. You also need to use op_type_name when wrapping op at the Python layer. REGISTER_USER_KERNEL(\"myrelu\") returns an OpKernelRegistry object. The methods that need to be called to set the registration information are mention in the code above. SetCreateFn<T>() : The method of this template's parameter T is our implementation of the kernel class which OneFlow will use it to create the kernel object. SetIsMatchedHob \uff1aBecause an op may have more than one kernels. You need to call SetIsMatchedHob to select a specific kernel for the calculation according to the physical device and data format. This method accepts an expression and when the expression is true , OneFlow will call the kernel to complete the calculation.","title":"Implementation and Registration of CPU Kernel"},{"location":"extended_topics/user_op.html#implementation-and-registration-of-gpu-kernel","text":"We implemented the GPU version of the kernel in myrelu_gpu_kernel.cu and registered it\uff1a #include \"oneflow/core/framework/framework.h\" #include <cub/cub.cuh> namespace oneflow { namespace { template < typename T > __global__ void ReluForwardGpu ( const int n , const T * x , T * y ) { CUDA_1D_KERNEL_LOOP ( i , n ) { y [ i ] = x [ i ] > 0 ? x [ i ] : 0 ; } } class ReluGpuFloatKernel final : public user_op :: OpKernel { public : ReluGpuFloatKernel () = default ; ~ ReluGpuFloatKernel () = default ; private : void Compute ( user_op :: KernelComputeContext * ctx ) const override { const user_op :: Tensor * in_tensor = ctx -> Tensor4ArgNameAndIndex ( \"in\" , 0 ); user_op :: Tensor * out_tensor = ctx -> Tensor4ArgNameAndIndex ( \"out\" , 0 ); int32_t n = in_tensor -> shape (). elem_cnt (); const float * in_ptr = in_tensor -> dptr < float > (); float * out_ptr = out_tensor -> mut_dptr < float > (); ReluForwardGpu < float > <<< 32 , 1024 , 0 , ctx -> device_ctx () -> cuda_stream () >>> ( n , in_ptr , out_ptr ); } bool AlwaysComputeWhenAllOutputsEmpty () const override { return false ; } }; #define REGISTER_RELU_KERNEL(device, dtype) \\ REGISTER_USER_KERNEL(\"myrelu\") \\ .SetCreateFn<ReluGpuFloatKernel>() \\ .SetIsMatchedHob( \\ (user_op::HobDeviceTag() == device) & \\ (user_op::HobDataType(\"out\", 0) \\ == GetDataType<dtype>::value)); REGISTER_RELU_KERNEL ( DeviceType :: kGPU , float ) REGISTER_RELU_KERNEL ( DeviceType :: kGPU , double ) } // namespace } // namespace oneflow The process of implementing and registering the GPU kernel is almost identical to the CPU kernel. The main differences are: Because CUDA programming is used, the CUDA header files are included. Compute uses GPU methods. SetIsMatchedHob set the matching device as GPU. Besides that, because of the use of CUDA, we need to use the nvcc compiler (instead of g++) to compile the GPU kernel.","title":"Implementation and Registration of GPU Kernel"},{"location":"extended_topics/user_op.html#compiling-option-description","text":"The oneflow.sysconfig contains the get_compile_flags , get_include , get_lib , and get_link_flags which corresponding to: Compiling Options Dictionary of header file Dictionary of link library Linking options For example\uff1a >>> import oneflow >>> oneflow.sysconfig.get_compile_flags() ['-I/home/yaochi/oneflow/build/python_scripts/oneflow/include', '-DHALF_ENABLE_CPP11_USER_LITERALS=0', '-DWITH_CUDA', '-D_GLIBCXX_USE_CXX11_ABI=0'] You can also get compile and link options directly by using command\uff1a python -c \"import oneflow; print(' '.join(oneflow.sysconfig.get_compile_flags()))\" python -c \"import oneflow; print(' '.join(oneflow.sysconfig.get_link_flags()))\" For the GPU kernel, the cudart library also needs to be specified when linking.","title":"Compiling Option Description"},{"location":"extended_topics/user_op.html#get-dynamic-library-by-compilation-and-linking","text":"For this simple example, you can use the following Makefile to build: CFLAGS = $( shell python -c \"import oneflow; print(' '.join(oneflow.sysconfig.get_compile_flags()))\" ) LFLAGS = $( shell python -c \"import oneflow; print(' '.join(oneflow.sysconfig.get_link_flags()))\" ) CUDAPATH = /usr/local/cuda-10.1/lib64 all: final_relu.so myrelu_op.o: myrelu_op.cpp g++ -std = c++11 -c myrelu_op.cpp \\ -o myrelu_op.o \\ -fPIC \\ ${ CFLAGS } \\ ${ LFLAGS } \\ -O2 myrelu_cpu_kernel.o: myrelu_cpu_kernel.cpp g++ -std = c++11 -c myrelu_cpu_kernel.cpp \\ -o myrelu_cpu_kernel.o \\ $( CFLAGS ) -fPIC myrelu_gpu_kernel.o: myrelu_gpu_kernel.cu nvcc -std = c++11 -c myrelu_gpu_kernel.cu \\ -o myrelu_gpu_kernel.o \\ $( CFLAGS ) -x cu -Xcompiler -fPIC final_relu.so: myrelu_op.o myrelu_cpu_kernel.o myrelu_gpu_kernel.o g++ -std = c++11 myrelu_op.o \\ myrelu_cpu_kernel.o \\ myrelu_gpu_kernel.o \\ -shared -o final_relu.so \\ $( CFLAGS ) \\ -fPIC \\ -L $( CUDAPATH ) \\ -lcudart \\ $( LFLAGS ) clean: rm -rf *.so *.o We use g++ to compile myrelu_op.cpp and myrelu_cpu_kernel.cpp , use nvcc to compile myrelu_gpu_kernel.cpp . Then get the target file (\".o\" file) and link the target file to final_ relu.so . We are going to load final_relu.so in Python then use wrappers and custom op.","title":"Get Dynamic Library by Compilation and Linking"},{"location":"extended_topics/user_op.html#using-the-custom-op-in-python","text":"Using a custom op in Python needs the following steps: Load the so file by oneflow.config.load_library . Use oneflow.user_op_builder to generating Python wrapper for custom op. Call the above result of Python wrapper. The following code encapsulates myrelu at the Python layer and call it: import oneflow as flow import numpy as np import oneflow.typing as tp # load modules flow . config . load_library ( \"final_relu.so\" ) # default configuration flow . config . gpu_device_num ( 1 ) # python op wrapper function def myrelu ( input_blob ): op = ( flow . user_op_builder ( \"op_myrelu\" ) . Op ( \"myrelu\" ) . Input ( \"in\" , [ input_blob ]) . Output ( \"out\" ) . Build () ) return op . InferAndTryRun () . SoleOutputBlob () # network code @flow . global_function () def MyJob ( x : tp . Numpy . Placeholder (( 5 ,), dtype = flow . float32 )) -> tp . Numpy : return myrelu ( x ) if __name__ == \"__main__\" : input = np . array ([ - 2 , - 1 , 0 , 1 , 2 ], dtype = np . float32 ) output = MyJob ( input ) print ( input ) print ( output ) The expected results are\uff1a [-2. -1. 0. 1. 2.] [0. 0. 0. 1. 2.] In the above code: flow.config.load_library(\"final_relu.so\") is to load the so file. We are focus on the process of building and running the python wrapper in myrelu . flow.user_op_builder(\"op_myrelu\") actually returns a UserOpConfBuilder object named op_myrelu . op = ( flow . user_op_builder ( \"op_myrelu\" ) . Op ( \"myrelu\" ) . Input ( \"in\" , [ input_blob ]) . Output ( \"out\" ) . Build () ) This object contains Op , Input and and etc methods which are used to encapsulate custom op. Details explanation are as follows: Op(\"myrelu\") : The parameter must be the op_type_name from the previous C++ registration which OneFlow uses to find the registered op type and instantiate the op. Input(\"in\", [input_blob]) : Corresponds to Input when op is registered in C++ that the first parameter must be the same as the string set by Input when op is registered in C++. The second parameter is the blob of the input which is a list . Because an op allows multiple inputs. Output(\"out\") : Corresponds to Output when op registered in C++. Build \uff1aAfter the above settings are complete, call Build to get the Python wrapper from the custom op. The following code will get the blob of the custom op: return op . InferAndTryRun () . SoleOutputBlob () InferAndTryRun completes the derivation and returns UserOp . If the returned blob has only one output. We cab use SoleOutputBlob to get the unique output. Otherwise use RemoteBlobList to get a list of multiple blobs. So far, we have built the myrelu which is a relatively simple op. But if we need to build a more complex op, we should use some additional features in the registration process. We'll introduce it from the aspects of op registration, kernel registration, gradient registration and Python layer wrapping.","title":"Using the Custom Op in Python"},{"location":"extended_topics/user_op.html#detailed-introduction-of-opregistry","text":"","title":"Detailed Introduction of OpRegistry"},{"location":"extended_topics/user_op.html#attr","text":"Some ops require configuration properties in addition to inputs and outputs. For example, the reshape needs to be configured the shape and the conv needs to be configured the alignment method. We can use the Attr at registration to set attributes for op. For example: OpRegistry & Attr < cpp_type > ( const std :: string & name ); We just need to specify the name and type of the attribute. For example\uff1a REGISTER_USER_OP ( \"reshape\" ) . Input ( \"in\" ) . Output ( \"out\" ) . Attr < shape > ( \"shape\" ) REGISTER_USER_OP ( \"conv2d\" ) . Input ( \"in\" ) . Input ( \"weight\" ) . Output ( \"out\" ) . Attr < std :: vector < int32_t >> ( \"padding_before\" ) In OneFlow, we currently support the following C++ data: UserOpAttrType Corresponding C++ data types kAtInt32 int32_t kAtInt64 int64_t kAtBool bool kAtFloat float kAtDouble double kAtShape oneflow::Shape kAtListInt32 std::vector kAtListInt64 std::vector kAtListFloat std::vector< float > kAtString std::string We can pass an additional parameter and configure a default value for it which is the corresponding C++ datatype in the table. Such as\uff1a . Attr < bool > ( \"is_transpose\" , false ) . Attr < int32_t > ( \"size\" , 10 ) . Attr < std :: vector < int32_t >> ( \"vector_of_size\" , std :: vector < int32_t > { 10 , 11 , 12 })","title":"Attr"},{"location":"extended_topics/user_op.html#setcheckattrfn","text":"For some Attributes , they require a more detailed delineation of the range which can be specified by SetCheckAttrFn when registering the Op. Take Conv op as an example, it has a configuration option called data_format which is a string type but the data must be channels_first or channels_last . . Attr < std :: string > ( \"data_format\" , std :: string ( \"NCHW\" )) . SetCheckAttrFn ( []( const user_op :: UserOpDefWrapper & def , const user_op :: UserOpConfWrapper & conf ) -> Maybe < void > { std :: string data_format = conf . attr < std :: string > ( \"data_format\" ); if ( data_format == \"channels_first\" || data_format == \"channels_last\" ) { return Maybe < void >:: Ok (); } return oneflow :: Error :: CheckFailed () << \"data_format value: \" << data_format << \" for Conv op is illegal.\" ; }) Set a function to check that returns Maybe<void>::Ok() when the value of the attribute matches the requirement. Otherwise returns oneflow::Error::CheckFailed() .","title":"SetCheckAttrFn"},{"location":"extended_topics/user_op.html#multiple-inoutput","text":"For some ops, they may have multiple input or output and we need to specify the number of inputs and outputs when we register it. Input example\uff1a // input must have 1 blob . Input ( \"input\" ) // input must have 5 blobs . Input ( \"input\" , 5 ) // input input must have at least 5 blobs . InputWithMinimum ( \"input\" , 5 ) // input can have no blob or 1 blob . OptionalInput ( \"input\" ) // input can have no blob or 5 blobs . OptionalInput ( \"input\" , 5 ) // input can have no blob or at least 5 blobs . OptionalInputWithMininum ( \"input\" , 5 ) Output setting is similar to Input.","title":"Multiple In/Output"},{"location":"extended_topics/user_op.html#setgetsbpfn","text":"SetGetSbpFn is for config the SBP of this op . Example of \"add_n\"\uff1a REGISTER_USER_OP ( \"add_n\" ) . InputWithMinimum ( \"in\" , 2 ) . Output ( \"out\" ) . SetGetSbpFn ([]( user_op :: SbpContext * ctx ) { int64_t num_axes = ctx -> LogicalTensorDesc4InputArgNameAndIndex ( \"in\" , 0 ). shape (). NumAxes (); for ( int64_t i = 0 ; i < num_axes ; ++ i ) { ctx -> NewBuilder (). Split ( ctx -> inputs (), i ). Split ( user_op :: OpArg ( \"out\" , 0 ), i ). Build (); } ctx -> NewBuilder (). PartialSum ( ctx -> inputs ()). PartialSum ( user_op :: OpArg ( \"out\" , 0 )). Build (); return Maybe < void >:: Ok (); });","title":"SetGetSbpFn"},{"location":"extended_topics/user_op.html#detailed-introduction-of-opkernelregistry","text":"","title":"Detailed Introduction of OpKernelRegistry"},{"location":"extended_topics/user_op.html#setinfertmpsizefn","text":"In some kernel implementations of op, some extra buffer may be required to store temporary data during the Compute . We can specify the buffer size when registering the kernel by using the SetInferTmpSizeFn . Then we get the buffer and use it in the Compute function. The following code registers the kernel with SetInferTmpSizeFn to specify a buffer size as 1024 bytes: REGISTER_USER_KERNEL ( \"XOp\" ) . SetInferTmpSizeFn ( []( const oneflow :: user_op :: InferContext * ) { return 1024 ; }); Once the buffer size is set by SetInferTmpSizeFn , this buffer can be retrieved in Compute by calling the KernelContext::Tensor4ArgNameAndIndex . This buffer is encapsulated as oneflow::user_op::Tensor which can be converted to other types of pointers by calling the dptr or mut_dptr . class XKernel final : public oneflow :: user_op :: OpKernel { void Compute ( oneflow :: user_op :: KernelContext * ctx ) override { oneflow :: user_op :: Tensor * tmp = ctx -> Tensor4ArgNameAndIndex ( \"tmp_buffer\" , 0 ); //The conversion yields a char* buffer of 1024 bytes. char * pBuff = tmp -> mut_dptr < char > (); ... } };","title":"SetInferTmpSizeFn"},{"location":"extended_topics/user_op.html#detailed-introduction-of-opgradregistry","text":"Oneflow is automatically get gradient during backward map expansion and the OneFlow framework uses Automatic Differentiation to get the gradient which means automatically find the gradient of the entire expression using the chain rule. In order to automatically get gradient a custom op, we need to register it with REGISTER_USER_OP_GRAD . From a mathematical point of view, the registration process is the computation of the backward derivation that we specify for our custom op. From a programming point of view, it is to set up a backward-generating function for a custom op. Within that function, we write code that specifies how the input gradient of that op is to be calculated. In order to calculate the gradient of a custom op, we need to construct the gradient of the input base on the input and output of the custom op. In most cases, we can represent the process of calculating the gradient of the input through the existing operators and their combination in OneFlow. The calculation of the input gradient usually consists of the following steps: Use ctx->DefineOp() and BackwardOpBuilder to represent methods for calculating input gradients. Because input gradient calculations may be combinations of multiple operations. Therefore DefineOp and BackwardOpBuilder may be used for multiple times. After defining the calculation process in the previous step, the required gradient is finally recorded in the output of some operator. We need to call the ctx->FwOp().InputGradBind() to combine the result of the previous calculation to the input gradient of the custom op. The following example (the complete code, including tests, can be found in myop_grad repository ). A custom op called myop will be used to register backward generating functions. This op is only used in this document to show the registration process which compute function is set as 3*x*x . Then it is easy to obtain the relationship between its forward and backward propagation as shown below. The gradient of x in the reverse process is computed as 6*x*dy . The forward op of myop is defined as follows: REGISTER_USER_OP ( \"myop\" ). Input ( \"in\" ). Output ( \"out\" ). SetTensorDescInferFn ( []( user_op :: InferContext * ctx ) -> Maybe < void > { * ctx -> Shape4ArgNameAndIndex ( \"out\" , 0 ) = * ctx -> Shape4ArgNameAndIndex ( \"in\" , 0 ); * ctx -> Dtype4ArgNameAndIndex ( \"out\" , 0 ) = * ctx -> Dtype4ArgNameAndIndex ( \"in\" , 0 ); return Maybe < void >:: Ok (); }); That is myop contains the only input in and the only output out . The reverse gradient registration of myop is as follows: REGISTER_USER_OP_GRAD ( \"myop\" ). SetBackwardOpConfGenFn ( []( user_op :: BackwardOpConfContext * ctx ) { const auto op1_name = ctx -> FwOp (). op_name () + \"_grad1\" ; // The operator op1_name is used to calculate the gradient of myop.in ctx -> DefineOp ( op1_name , [ & ctx ]( user_op :: BackwardOpBuilder & builder ) { return builder . OpTypeName ( \"multiply\" ) . InputBind ( \"x\" , ctx -> FwOp (). input ( \"in\" , 0 )) //multiply.x <- myop.in . InputBind ( \"y\" , ctx -> FwOp (). output_grad ( \"out\" , 0 )) //multiply.y <- the gradient of myop.out . Output ( \"out\" ) . Build (); }); const auto op2_name = ctx -> FwOp (). op_name () + \"_grad2\" ; // The operator op2_name is used to calculate 6*op1_name. ctx -> DefineOp ( op2_name , [ & ctx , & op1_name ]( user_op :: BackwardOpBuilder & builder ) { return builder . OpTypeName ( \"scalar_mul\" ) . InputBind ( \"in\" , ctx -> GetOp ( op1_name ). output ( \"out\" , 0 )) . Attr ( \"has_float_operand\" , true ) . Attr ( \"has_int_operand\" , false ) . Attr ( \"float_operand\" , static_cast < double > ( 6 )) . Attr ( \"int_operand\" , static_cast < int64_t > ( 6 )) . Output ( \"out\" ) . Build (); }); // (the gradient of myop.in) <- op1_name.out ctx -> FwOp (). InputGradBind ( user_op :: OpArg ( \"in\" , 0 ), [ & ctx , & op2_name ]() -> const std :: string & { return ctx -> GetOp ( op2_name ) . output ( \"out\" , 0 ); }); }); The string parameter accepted by REGISTER_USER_OP_GRAD(\"myop\") is op_type_name which needs to be the same as registered with REGISTER_USER_OP . REGISTER_USER_OP_GRAD(\"myop\") returns an oneflow::user_op::OpGradRegistry object that we can call it to set the custom op's backward generating function. In the above gradient registration process, the expression for the gradient of myop is 6*x*dy which is demonstrated in the code. First op1_name is defined and x*dy is solved by using the existing operator multiply : // The operator op1_name is used to calculate the gradient of myop.in ctx -> DefineOp ( op1_name , [ & ctx ]( user_op :: BackwardOpBuilder & builder ) { return builder . OpTypeName ( \"multiply\" ) . InputBind ( \"x\" , ctx -> FwOp (). input ( \"in\" , 0 )) //multiply.x <- myop.in . InputBind ( \"y\" , ctx -> FwOp (). output_grad ( \"out\" , 0 )) //multiply.y <- myop.out\u7684\u68af\u5ea6 . Output ( \"out\" ) . Build (); }); Then op2_name is defined and use the existing operator op2_name to solve for 6*op1_name . // The operator op2_name is used to calculate 6*op1_name. ctx -> DefineOp ( op2_name , [ & ctx , & op1_name ]( user_op :: BackwardOpBuilder & builder ) { return builder . OpTypeName ( \"scalar_mul\" ) . InputBind ( \"in\" , ctx -> GetOp ( op1_name ). output ( \"out\" , 0 )) . Attr ( \"has_float_operand\" , true ) . Attr ( \"has_int_operand\" , false ) . Attr ( \"float_operand\" , static_cast < double > ( 6 )) . Attr ( \"int_operand\" , static_cast < int64_t > ( 6 )) . Output ( \"out\" ) . Build (); }); Finally bind the output of op2_name (i.e., 6*x*dy ) to the input of myop to complete the registration. // (the gradient of myop.in) <- op1_name.out ctx -> FwOp (). InputGradBind ( user_op :: OpArg ( \"in\" , 0 ), [ & ctx , & op2_name ]() -> const std :: string & { return ctx -> GetOp ( op2_name ) . output ( \"out\" , 0 ); }); The above code is the complete process of registering a gradient and the related classes and methods will be described in below.","title":"Detailed Introduction of OpGradRegistry"},{"location":"extended_topics/user_op.html#setbackwardopconfgenfn","text":"We use OpGradRegistry::SetBackwardOpConfGenFn(fn) to set the backward generating function fn which has the following prototype: void fn ( BackwardOpConfContext * ctx ); BackwardOpConfContext* ctx has all information needed to generate the op.","title":"SetBackwardOpConfGenFn"},{"location":"extended_topics/user_op.html#backwardopconfcontext","text":"The common methods and their purpose used in BackwardOpConfContext as follows: UserOpWrapper& FwOp(); : Get forward op. GetOp(op_name) : Create and get the corresponding op based on op_name . GetOp uses a lazy init mechanism and the corresponding op is not actually created until GetOp is called. void DefineOp(op_name, fn) \uff1aDefine fn of the op named op_name . When ctx->GetOp(op_name) is called, fn is triggered in the OneFlow for Op creation and if the op has already been created. Then the result is retrieved directly. The fn receives a BackwardOpBuilder parameter for constructing the reverse op. We will introduce BackwardOpBuilder later on.","title":"BackwardOpConfContext"},{"location":"extended_topics/user_op.html#detailed-introduction-of-backwardopbuilder","text":"BackwardOpBuilder is used to build a reverse op. The fragment of above code is an example: ctx -> DefineOp ( op1_name , [ & ctx ]( user_op :: BackwardOpBuilder & builder ) { return builder . OpTypeName ( \"multiply\" ) . InputBind ( \"x\" , ctx -> FwOp (). input ( \"in\" , 0 )) //multiply.x <- myop.in . InputBind ( \"y\" , ctx -> FwOp (). output_grad ( \"out\" , 0 )) //multiply.y <- myop.out\u7684\u68af\u5ea6 . Output ( \"out\" ) . Build (); }); In this function, we call Build to build a reverse op for computing x*dy . The purpose of each operator is as follows: OpTypeName(\"multiply\") specifies the op_type_name of an op that is used to help us compute the reverse gradient. InputBind(arg_name, blob) binds the input arg_name of multiply to the specified blob and can be called for multiple times. If the arg_name corresponds to multiple blob which means the order of Input is the order of the corresponding index. Output(arg_name, num) Specifies the number of output blobes that actually correspond to the arg_name which defaults to 1 if num is not filled in. Attr(attr_name, val) sets the value of the attribute which same in the registration. Calling Build() after above configuration, then the construction of the reverse op is completed.","title":"Detailed Introduction of BackwardOpBuilder"},{"location":"extended_topics/user_op.html#detailed-introduction-of-useropwrapper","text":"Calling ctx->FwOp() will return the UserOpWrapper of myop and complete the gradient binding by calling the UserOpWrapper . ctx -> FwOp (). InputGradBind ( user_op :: OpArg ( \"in\" , 0 ), [ & ctx , & op2_name ]() -> const std :: string & { return ctx -> GetOp ( op2_name ) . output ( \"out\" , 0 ); }); Common methods for UserOpWrapper are: InputGradBind(input, grad_fn) \uff1aBind the input of the forward op and get the gradient function grad_fn . OneFlow automatically determines whether input needs to generate a backward gradient, if needed, it will trigger grad_fn and binds the input. input(arg_name, index) \uff1aGet the blob corresponding to the arg_name of input. output(arg_name,index) \uff1aGet the blob corresponding to the arg_name of output. output_grad(output_arg_name, index) \uff1aGet the output_arg_name of the forward op which is the blob of the corresponding backward gradient. attr(attr_name) \uff1aGet the value corresponding to the attr_name . arg_tensor_desc(arg_name, index) \uff1aReturns the input/output tensor information of the forward op which including shape , dtype and etc.","title":"Detailed Introduction of UserOpWrapper"},{"location":"extended_topics/user_op.html#customized-op-for-calculating-gradients","text":"As we mentioned earlier, in most cases, the process of calculating a gradient can be represented by a combination of existing ops. However, when it is difficult to use an existing op to solve the gradient for a particular forward op that we need to design and create operators specifically for the gradient calculation. Example can be found in: relu_op.cpp .","title":"Customized Op for Calculating Gradients"},{"location":"extended_topics/user_op.html#detailed-introduction-of-useropconfbuilder","text":"In Python frontend of OneFlow, we provide UserOpConfBuilder to build the wrapper of custom op which is used in Use custom opp in Python previously. Here is the summary of the relationship between UserOpConfBuilder in Python layer and C++ layer. For example, we have wrapped a cast : def cast ( x , dtype , name ): return ( flow . user_op_builder ( name ) . Op ( \"cast\" ) . Input ( \"in\" , [ x ]) . Output ( \"out\" ) . Attr ( \"dtype\" , dtype ) . Build () . InferAndTryRun () . RemoteBlobList ()[ 0 ] ) ) Op(op_type_name) \uff1aThe accepted parameter is op_type_name when it is registered in C++. Input(input_name, input_blob_list) \uff1a input_name should be the same as the first parameter of Input when registering this op in C++. Output(output_name, num=1) \uff1a output_name and num should be the same as Output of op when registration in C++. Attr(attr_name, attr_value) \uff1a attr_name corresponds to the attribute of OpRegistry::Attr used for C++ registration and attr_value should be the same type as the attribute type when declaration. Build() \uff1aBuild the user op for the Python layer. The derivation can be done by calling InferAndTryRun in the user op and the result can be retrieved by calling RemoteBlobList or SoleOutputBlob . RemoteBlobList \uff1aGet all outputs which applies to op with multiple outputs and all ops are placed in a list. SoleOutputBlob \uff1aGet unique outputs which applies to op with one output.","title":"Detailed Introduction of UserOpConfBuilder"},{"location":"extended_topics/watch_watch_diff.html","text":"How to Obtain Runtime Data \u00b6 OneFlow support oneflow.watch and oneflow.watch_diff . We can use them to register a callback function to get data and gradient tensor in job functions at runtime. Using Guidance \u00b6 To get data or gradient tensor in job function, we need to follow these steps: Write a callback function and the parameters of the callback function should be annotated to indicate the data type. The logic of the callback function need to be set up by user themselves. When defining the job functions, we use oneflow.watch or oneflow.watch_diff to register callback function. We obtain data tensor from the former one and their corresponding gradient from the latter one. At the appropriate time when the job function is running, OneFlow will call the previous callback function which was registered earlier and pass the monitored data to the callback function then execute the logic in the callback function. Take oneflow.watch as example: def my_watch ( x : T ): #process x @global_function () def foo () -> T : #define network ... oneflow . watch ( x , my_watch ) #... The T in the code above is the data type in oneflow.typing . Like oneflow.typing.Numpy . Please refer to this article . We will use the following examples to demonstrate how to use watch and watch_diff . Use watch to Obtain the Data when Running \u00b6 The following is an example to demonstrate how to use oneflow.watch to obtain the data from middle layer in OneFlow. Code: test_watch.py Run above code: python3 test_watch.py We can get results like the followings: in: [ 0.15727027 0.45887455 0.10939325 0.66666406 -0.62354755] out: [0.15727027 0.45887455 0.10939325 0.66666406 0. ] Code Explanation \u00b6 In the example, we focus on y in ReluJob . Thus, we call flow.watch(y, watch_handler) to monitor y . The function oneflow.watch needs two parameters: The first parameter is y which we focus on. The second parameter is a callback function. When OneFlow use device resources to execute ReluJob , it will send y as a parameter to callback function. We define our callback function watch_handler to print out its parameters. User can use customized callback function to process the data from OneFlow according to their own requirements. Use watch_diff to Obtain Gradient when Running \u00b6 The following is an example to demonstrate how to use oneflow.watch_diff to obtain the gradient at runtime. Code: test_watch_diff.py Run above code: python3 test_watch.py We should have the following results: [ ... [ 1.39966095e-03 3.49164731e-03 3.31605263e-02 4.50417027e-03 7.73609674e-04 4.89911772e-02 2.47627571e-02 7.65468649e-05 -1.18361652e-01 1.20161276e-03]] (100, 10) float32 Code Explanation \u00b6 In the example above, we use oneflow.watch_diff to obtain the gradient. The processe is the same as the example which using oneflow.watch to obtain data tensor. First, we define the callback function: def watch_diff_handler ( blob : tp . Numpy ): print ( \"watch_diff_handler:\" , blob , blob . shape , blob . dtype ) Then we use oneflow.watch_diff to register the callback function in job function: flow . watch_diff ( logits , watch_diff_handler ) When running, OneFlow framework will call watch_diff_handler and send the gradient corresponding with logits to watch_diff_handler .","title":"Obtain Runtime Data"},{"location":"extended_topics/watch_watch_diff.html#how-to-obtain-runtime-data","text":"OneFlow support oneflow.watch and oneflow.watch_diff . We can use them to register a callback function to get data and gradient tensor in job functions at runtime.","title":"How to Obtain Runtime Data"},{"location":"extended_topics/watch_watch_diff.html#using-guidance","text":"To get data or gradient tensor in job function, we need to follow these steps: Write a callback function and the parameters of the callback function should be annotated to indicate the data type. The logic of the callback function need to be set up by user themselves. When defining the job functions, we use oneflow.watch or oneflow.watch_diff to register callback function. We obtain data tensor from the former one and their corresponding gradient from the latter one. At the appropriate time when the job function is running, OneFlow will call the previous callback function which was registered earlier and pass the monitored data to the callback function then execute the logic in the callback function. Take oneflow.watch as example: def my_watch ( x : T ): #process x @global_function () def foo () -> T : #define network ... oneflow . watch ( x , my_watch ) #... The T in the code above is the data type in oneflow.typing . Like oneflow.typing.Numpy . Please refer to this article . We will use the following examples to demonstrate how to use watch and watch_diff .","title":"Using Guidance"},{"location":"extended_topics/watch_watch_diff.html#use-watch-to-obtain-the-data-when-running","text":"The following is an example to demonstrate how to use oneflow.watch to obtain the data from middle layer in OneFlow. Code: test_watch.py Run above code: python3 test_watch.py We can get results like the followings: in: [ 0.15727027 0.45887455 0.10939325 0.66666406 -0.62354755] out: [0.15727027 0.45887455 0.10939325 0.66666406 0. ]","title":"Use watch to Obtain the Data when Running"},{"location":"extended_topics/watch_watch_diff.html#code-explanation","text":"In the example, we focus on y in ReluJob . Thus, we call flow.watch(y, watch_handler) to monitor y . The function oneflow.watch needs two parameters: The first parameter is y which we focus on. The second parameter is a callback function. When OneFlow use device resources to execute ReluJob , it will send y as a parameter to callback function. We define our callback function watch_handler to print out its parameters. User can use customized callback function to process the data from OneFlow according to their own requirements.","title":"Code Explanation"},{"location":"extended_topics/watch_watch_diff.html#use-watch_diff-to-obtain-gradient-when-running","text":"The following is an example to demonstrate how to use oneflow.watch_diff to obtain the gradient at runtime. Code: test_watch_diff.py Run above code: python3 test_watch.py We should have the following results: [ ... [ 1.39966095e-03 3.49164731e-03 3.31605263e-02 4.50417027e-03 7.73609674e-04 4.89911772e-02 2.47627571e-02 7.65468649e-05 -1.18361652e-01 1.20161276e-03]] (100, 10) float32","title":"Use watch_diff to Obtain Gradient when Running"},{"location":"extended_topics/watch_watch_diff.html#code-explanation_1","text":"In the example above, we use oneflow.watch_diff to obtain the gradient. The processe is the same as the example which using oneflow.watch to obtain data tensor. First, we define the callback function: def watch_diff_handler ( blob : tp . Numpy ): print ( \"watch_diff_handler:\" , blob , blob . shape , blob . dtype ) Then we use oneflow.watch_diff to register the callback function in job function: flow . watch_diff ( logits , watch_diff_handler ) When running, OneFlow framework will call watch_diff_handler and send the gradient corresponding with logits to watch_diff_handler .","title":"Code Explanation"},{"location":"quick_start/install.html","text":"Install OneFlow Stable Version \u00b6 Install the latest stable version of OneFlow with CUDA support using the following command: python3 -m pip install -f https://release.oneflow.info oneflow==0.4.0+cu102 Install the latest version of the OneFlow master branch using the following command (not recommended for production environments): python3 -m pip install oneflow -f https://staging.oneflow.info/branch/master/cu102 If you are informed that the corresponding version cannot be found, please try upgrading pip : python3 -m pip install --upgrade pip #--user Chinese users can use the domestic mirror to accelerate: python3 -m pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple Detailed instructions can be found in the pypi mirror help \u3002 System Requirements: Python >= 3.6 CUDA driver requirements are available in the OneFlow source code repository README Build from source \u00b6 If you want to install OneFlow by building from source, please refer to README . Also refer to Troubleshooting for common issues you might encounter when compiling and running OneFlow. Install OneFlow with legacy CUDA support \u00b6 To install OneFlow with legacy CUDA support, run one of the following command: Stable: python3 -m pip install --find-links https://release.oneflow.info oneflow==0.4.0+[PLATFORM] Nightly: python3 -m pip install oneflow -f https://staging.oneflow.info/branch/master/[PLATFORM] All available [PLATFORM] : Platform CUDA Driver Version Supported GPUs cu112 >= 450.80.02 GTX 10xx, RTX 20xx, A100, RTX 30xx cu111 >= 450.80.02 GTX 10xx, RTX 20xx, A100, RTX 30xx cu110, cu110_xla >= 450.36.06 GTX 10xx, RTX 20xx, A100 cu102, cu102_xla >= 440.33 GTX 10xx, RTX 20xx cu101, cu101_xla >= 418.39 GTX 10xx, RTX 20xx cu100, cu100_xla >= 410.48 GTX 10xx, RTX 20xx cpu N/A N/A QQ channel \u00b6 If you encounter any problems during the installation and want for help, please join the QQ channel or submit issues on Github . QQ channel ID: 331883 or scan QR code below","title":"Installation"},{"location":"quick_start/install.html#install-oneflow-stable-version","text":"Install the latest stable version of OneFlow with CUDA support using the following command: python3 -m pip install -f https://release.oneflow.info oneflow==0.4.0+cu102 Install the latest version of the OneFlow master branch using the following command (not recommended for production environments): python3 -m pip install oneflow -f https://staging.oneflow.info/branch/master/cu102 If you are informed that the corresponding version cannot be found, please try upgrading pip : python3 -m pip install --upgrade pip #--user Chinese users can use the domestic mirror to accelerate: python3 -m pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple Detailed instructions can be found in the pypi mirror help \u3002 System Requirements: Python >= 3.6 CUDA driver requirements are available in the OneFlow source code repository README","title":"Install OneFlow Stable Version"},{"location":"quick_start/install.html#build-from-source","text":"If you want to install OneFlow by building from source, please refer to README . Also refer to Troubleshooting for common issues you might encounter when compiling and running OneFlow.","title":"Build from source"},{"location":"quick_start/install.html#install-oneflow-with-legacy-cuda-support","text":"To install OneFlow with legacy CUDA support, run one of the following command: Stable: python3 -m pip install --find-links https://release.oneflow.info oneflow==0.4.0+[PLATFORM] Nightly: python3 -m pip install oneflow -f https://staging.oneflow.info/branch/master/[PLATFORM] All available [PLATFORM] : Platform CUDA Driver Version Supported GPUs cu112 >= 450.80.02 GTX 10xx, RTX 20xx, A100, RTX 30xx cu111 >= 450.80.02 GTX 10xx, RTX 20xx, A100, RTX 30xx cu110, cu110_xla >= 450.36.06 GTX 10xx, RTX 20xx, A100 cu102, cu102_xla >= 440.33 GTX 10xx, RTX 20xx cu101, cu101_xla >= 418.39 GTX 10xx, RTX 20xx cu100, cu100_xla >= 410.48 GTX 10xx, RTX 20xx cpu N/A N/A","title":"Install OneFlow with legacy CUDA support"},{"location":"quick_start/install.html#qq-channel","text":"If you encounter any problems during the installation and want for help, please join the QQ channel or submit issues on Github . QQ channel ID: 331883 or scan QR code below","title":"QQ channel"},{"location":"quick_start/lenet_mnist.html","text":"This article covers topics below: Configuring the hardware and software environment using the OneFlow interface Define models using OneFlow's interface Model training with train type How to save/load model Use the predict type for model evaluation Using predict type for image recognition This article demonstrates the key steps of how to train a LeNet model with MNIST dataset using OneFlow. The full example code is attached at the end of article. You can see the effects of each script by running the following commands (The script operation rely on the default GPU No.0 on your machine. If you install the CPU version of OneFlow, the script will automatically call the CPU for training/evaluation). First of all, clone the documentation repository and switch to the corresponding path: git clone https://github.com/Oneflow-Inc/oneflow-documentation.git cd oneflow-documentation/en/docs/code/quick_start/ Training model python lenet_train.py The commands above will train a model with MNIST dataset and save it. Output\uff1a File mnist.npz already exist, path: ./mnist.npz 5.9947124 1.0865117 0.5317516 0.20937675 0.26428983 0.21764673 0.23443426 ... A trained model is the prerequisite of lenet_eval.py and lenet_test.py . We can directly download a trained model to skip the training progress: #change directory to: en/docs/code/quick_start/ wget https://oneflow-public.oss-cn-beijing.aliyuncs.com/online_document/docs/quick_start/lenet_models_1.zip unzip lenet_models_1.zip Evaluation python lenet_eval.py The command above uses the MNIST's testing set to evaluate the trained model and print out the accuracy. Output\uff1a File mnist.npz already exist, path: ./mnist.npz accuracy: 99.4% Image recognition python lenet_test.py ./9.png # Output\uff1aprediction: 9 The above command will use the trained model to predict the content of file \"9.png\". We can also download and verify more from prepared images . Introduction of MNIST Dataset \u00b6 MNIST is a handwritten digits database including training set and testing set. Training set includes 60000 pictures and their corresponding label. Yann LeCun and others have normalized all the images and packed them into a single binary file for downloading. http://yann.lecun.com/exdb/mnist/ Define Training Model \u00b6 Modules oneflow.nn and oneflow.layers provide the operators to construct the model. def lenet ( data , train = False ): initializer = flow . truncated_normal ( 0.1 ) conv1 = flow . layers . conv2d ( data , 32 , 5 , padding = \"SAME\" , activation = flow . nn . relu , name = \"conv1\" , kernel_initializer = initializer , ) pool1 = flow . nn . max_pool2d ( conv1 , ksize = 2 , strides = 2 , padding = \"SAME\" , name = \"pool1\" , data_format = \"NCHW\" ) conv2 = flow . layers . conv2d ( pool1 , 64 , 5 , padding = \"SAME\" , activation = flow . nn . relu , name = \"conv2\" , kernel_initializer = initializer , ) pool2 = flow . nn . max_pool2d ( conv2 , ksize = 2 , strides = 2 , padding = \"SAME\" , name = \"pool2\" , data_format = \"NCHW\" ) reshape = flow . reshape ( pool2 , [ pool2 . shape [ 0 ], - 1 ]) hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"dense1\" , ) if train : hidden = flow . nn . dropout ( hidden , rate = 0.5 , name = \"dropout\" ) return flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , name = \"dense2\" ) As the code showing above, we build up a LeNet network model. Implementation of Job Function for Training \u00b6 OneFlow provides a decorator named oneflow.global_function by which we can covert a Python function to a OneFlow Job Function . global_function Decorator \u00b6 oneflow.function_config decorator takes a type parameter to specify the type of job function. The type=\"tranining\" means that the job function is for traning and type=\"predict\" is for predicting. There is also a function_config parameter taken by oneflow.global_function decorator. The function_config contains configuration about training. @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 )) -> tp . Numpy : # Implementation of netwrok ... The tp.Numpy.Placeholder is a placeholder. The annotation tp.Numpy on return type means that the job function will return a numpy object. Setup Optimizer \u00b6 We can use oneflow.optimizer to specify the parameters needed by optimization. By this way, in the process of each iteration during training, OneFlow will take the specified object as optimization goal. @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : with flow . scope . placement ( \"gpu\" , \"0:0\" ): logits = lenet ( images , train = True ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.1 ]) flow . optimizer . SGD ( lr_scheduler , momentum = 0 ) . minimize ( loss ) return loss So Far, we use flow.nn.sparse_softmax_cross_entropy_with_logits to calculate the loss and specify it as optimization goal. lr_scheduler sets the learning rate schedule, and [0.1] means learning rate is 0.1. flow.optimizer.SGD means SGD is specified as the optimizer. The loss is the goal of minimization to the optimizer and the return type (not requried). Calling the Job Function and Get Results \u00b6 We can start training by invoking the job function. The return value we get when we call the job function is defined by the annotation of return value type in job function. We can get one or multiple results after each call of job function. Example on Single Return Value \u00b6 The job function in lenet_train.py : @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : with flow . scope . placement ( \"gpu\" , \"0:0\" ): logits = lenet ( images , train = True ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.1 ]) flow . optimizer . SGD ( lr_scheduler , momentum = 0 ) . minimize ( loss ) return loss The return value in job function is a tp.Numpy . When calling job function, we will get a numpy object: for epoch in range ( 20 ): for i , ( images , labels ) in enumerate ( zip ( train_images , train_labels )): loss = train_job ( images , labels ) if i % 20 == 0 : print ( loss . mean ()) We call the train_job and print the loss every 20 iterations. Example on Multiple Return Values \u00b6 In script lenet_eval.py , we define the job function below: @flow . global_function ( type = \"predict\" ) def eval_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> Tuple [ tp . Numpy , tp . Numpy ]: with flow . scope . placement ( \"gpu\" , \"0:0\" ): logits = lenet ( images , train = False ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) return ( labels , logits ) The return value type of this job function is Tuple[tp.Numpy, tp.Numpy] . When we call the job function, we will get a tuple container. There are two numpy objects in it: for i , ( images , labels ) in enumerate ( zip ( test_images , test_labels )): labels , logits = eval_job ( images , labels ) acc ( labels , logits ) We call the job function and get labels and logits then use them to evaluate the model. Synchronous and Asynchronous Call \u00b6 All code in this article only call synchronously to get results from job function. In fact, OneFlow can call job function asynchronously. For more details, please refer to Obtain results from job function . Model Initialization, Saving and Loading \u00b6 Model Initialization and Saving \u00b6 The example of model saved by the flow.checkpoint.save : if __name__ == '__main__' : #data loading and training ... flow . checkpoint . save ( \"./lenet_models_1\" ) When the model is saved, we will get a folder called \"lenet_models_1\". This folder contains directories and files corresponding with the model parameters. Model Loading \u00b6 During the prediction process, we can load the parameter from the file to memory by flow.checkpoint.get and then update the parameter to the model by flow.load_variables . For example: if __name__ == '__main__' : flow . load_variables ( flow . checkpoint . get ( \"./lenet_models_1\" )) #evaluation process ... Evaluation of Model \u00b6 The job function for evaluation is basically same as job function for training. The small difference is that the model we use is already saved in evaluation process. Thus, initialization and update of model during iteration are not needed. Job Function for Evaluation \u00b6 @flow . global_function ( type = \"predict\" ) def eval_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> Tuple [ tp . Numpy , tp . Numpy ]: with flow . scope . placement ( \"gpu\" , \"0:0\" ): logits = lenet ( images , train = False ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) return ( labels , logits ) Code above is the implementation of job function for evaluation and its return type is declared as Tuple[tp.Numpy, tp.Numpy] . Tuple have two numpy in it. We will call the job function and calculate the accuracy according to the return values. Process of Evaluation \u00b6 The acc function is used to count the total number of samples and the number of correct prediction results. We will call the job function to get paramters labels and logits : g_total = 0 g_correct = 0 def acc ( labels , logits ): global g_total global g_correct predictions = np . argmax ( logits , 1 ) right_count = np . sum ( predictions == labels ) g_total += labels . shape [ 0 ] g_correct += right_count Call the job function for evaluation: if __name__ == \"__main__\" : flow . load_variables ( flow . checkpoint . get ( \"./lenet_models_1\" )) ( train_images , train_labels ), ( test_images , test_labels ) = flow . data . load_mnist ( BATCH_SIZE , BATCH_SIZE ) for epoch in range ( 1 ): for i , ( images , labels ) in enumerate ( zip ( test_images , test_labels )): labels , logits = eval_job ( images , labels ) acc ( labels , logits ) print ( \"accuracy: {0:.1f} %\" . format ( g_correct * 100 / g_total )) So far, we call the job function for evaluation looply and print the accuracy of evaluation result on MNIST testing set. Image Prediction \u00b6 After making a few changes to the code above, it will take the data from the raw images rather than existing dataset. Then we can get a model to predict the content from the images. def load_image ( file ): im = Image . open ( file ) . convert ( \"L\" ) im = im . resize (( 28 , 28 ), Image . ANTIALIAS ) im = np . array ( im ) . reshape ( 1 , 1 , 28 , 28 ) . astype ( np . float32 ) im = ( im - 128.0 ) / 255.0 im . reshape (( - 1 , 1 , 1 , im . shape [ 1 ], im . shape [ 2 ])) return im def main (): if len ( sys . argv ) != 2 : usage () return flow . load_variables ( flow . checkpoint . get ( \"./lenet_models_1\" )) image = load_image ( sys . argv [ 1 ]) logits = test_job ( image ) prediction = np . argmax ( logits , 1 ) print ( \"prediction: {} \" . format ( prediction [ 0 ])) if __name__ == \"__main__\" : main () Code \u00b6 Model training \u00b6 Script: lenet_train.py Model evaluation \u00b6 Script: lenet_eval.py Saved model: lenet_models_1.zip Digits prediction \u00b6 Script: lenet_test.py Saved model: lenet_models_1.zip MNIST image dataset mnist_raw_images.zip","title":"Recognition of MNIST Handwritten Digits"},{"location":"quick_start/lenet_mnist.html#introduction-of-mnist-dataset","text":"MNIST is a handwritten digits database including training set and testing set. Training set includes 60000 pictures and their corresponding label. Yann LeCun and others have normalized all the images and packed them into a single binary file for downloading. http://yann.lecun.com/exdb/mnist/","title":"Introduction of MNIST Dataset"},{"location":"quick_start/lenet_mnist.html#define-training-model","text":"Modules oneflow.nn and oneflow.layers provide the operators to construct the model. def lenet ( data , train = False ): initializer = flow . truncated_normal ( 0.1 ) conv1 = flow . layers . conv2d ( data , 32 , 5 , padding = \"SAME\" , activation = flow . nn . relu , name = \"conv1\" , kernel_initializer = initializer , ) pool1 = flow . nn . max_pool2d ( conv1 , ksize = 2 , strides = 2 , padding = \"SAME\" , name = \"pool1\" , data_format = \"NCHW\" ) conv2 = flow . layers . conv2d ( pool1 , 64 , 5 , padding = \"SAME\" , activation = flow . nn . relu , name = \"conv2\" , kernel_initializer = initializer , ) pool2 = flow . nn . max_pool2d ( conv2 , ksize = 2 , strides = 2 , padding = \"SAME\" , name = \"pool2\" , data_format = \"NCHW\" ) reshape = flow . reshape ( pool2 , [ pool2 . shape [ 0 ], - 1 ]) hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"dense1\" , ) if train : hidden = flow . nn . dropout ( hidden , rate = 0.5 , name = \"dropout\" ) return flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , name = \"dense2\" ) As the code showing above, we build up a LeNet network model.","title":"Define Training Model"},{"location":"quick_start/lenet_mnist.html#implementation-of-job-function-for-training","text":"OneFlow provides a decorator named oneflow.global_function by which we can covert a Python function to a OneFlow Job Function .","title":"Implementation of Job Function for Training"},{"location":"quick_start/lenet_mnist.html#global_function-decorator","text":"oneflow.function_config decorator takes a type parameter to specify the type of job function. The type=\"tranining\" means that the job function is for traning and type=\"predict\" is for predicting. There is also a function_config parameter taken by oneflow.global_function decorator. The function_config contains configuration about training. @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 )) -> tp . Numpy : # Implementation of netwrok ... The tp.Numpy.Placeholder is a placeholder. The annotation tp.Numpy on return type means that the job function will return a numpy object.","title":"global_function Decorator"},{"location":"quick_start/lenet_mnist.html#setup-optimizer","text":"We can use oneflow.optimizer to specify the parameters needed by optimization. By this way, in the process of each iteration during training, OneFlow will take the specified object as optimization goal. @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : with flow . scope . placement ( \"gpu\" , \"0:0\" ): logits = lenet ( images , train = True ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.1 ]) flow . optimizer . SGD ( lr_scheduler , momentum = 0 ) . minimize ( loss ) return loss So Far, we use flow.nn.sparse_softmax_cross_entropy_with_logits to calculate the loss and specify it as optimization goal. lr_scheduler sets the learning rate schedule, and [0.1] means learning rate is 0.1. flow.optimizer.SGD means SGD is specified as the optimizer. The loss is the goal of minimization to the optimizer and the return type (not requried).","title":"Setup Optimizer"},{"location":"quick_start/lenet_mnist.html#calling-the-job-function-and-get-results","text":"We can start training by invoking the job function. The return value we get when we call the job function is defined by the annotation of return value type in job function. We can get one or multiple results after each call of job function.","title":"Calling the Job Function and Get Results"},{"location":"quick_start/lenet_mnist.html#example-on-single-return-value","text":"The job function in lenet_train.py : @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : with flow . scope . placement ( \"gpu\" , \"0:0\" ): logits = lenet ( images , train = True ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.1 ]) flow . optimizer . SGD ( lr_scheduler , momentum = 0 ) . minimize ( loss ) return loss The return value in job function is a tp.Numpy . When calling job function, we will get a numpy object: for epoch in range ( 20 ): for i , ( images , labels ) in enumerate ( zip ( train_images , train_labels )): loss = train_job ( images , labels ) if i % 20 == 0 : print ( loss . mean ()) We call the train_job and print the loss every 20 iterations.","title":"Example on Single Return Value"},{"location":"quick_start/lenet_mnist.html#example-on-multiple-return-values","text":"In script lenet_eval.py , we define the job function below: @flow . global_function ( type = \"predict\" ) def eval_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> Tuple [ tp . Numpy , tp . Numpy ]: with flow . scope . placement ( \"gpu\" , \"0:0\" ): logits = lenet ( images , train = False ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) return ( labels , logits ) The return value type of this job function is Tuple[tp.Numpy, tp.Numpy] . When we call the job function, we will get a tuple container. There are two numpy objects in it: for i , ( images , labels ) in enumerate ( zip ( test_images , test_labels )): labels , logits = eval_job ( images , labels ) acc ( labels , logits ) We call the job function and get labels and logits then use them to evaluate the model.","title":"Example on Multiple Return Values"},{"location":"quick_start/lenet_mnist.html#synchronous-and-asynchronous-call","text":"All code in this article only call synchronously to get results from job function. In fact, OneFlow can call job function asynchronously. For more details, please refer to Obtain results from job function .","title":"Synchronous and Asynchronous Call"},{"location":"quick_start/lenet_mnist.html#model-initialization-saving-and-loading","text":"","title":"Model Initialization, Saving and Loading"},{"location":"quick_start/lenet_mnist.html#model-initialization-and-saving","text":"The example of model saved by the flow.checkpoint.save : if __name__ == '__main__' : #data loading and training ... flow . checkpoint . save ( \"./lenet_models_1\" ) When the model is saved, we will get a folder called \"lenet_models_1\". This folder contains directories and files corresponding with the model parameters.","title":"Model Initialization and Saving"},{"location":"quick_start/lenet_mnist.html#model-loading","text":"During the prediction process, we can load the parameter from the file to memory by flow.checkpoint.get and then update the parameter to the model by flow.load_variables . For example: if __name__ == '__main__' : flow . load_variables ( flow . checkpoint . get ( \"./lenet_models_1\" )) #evaluation process ...","title":"Model Loading"},{"location":"quick_start/lenet_mnist.html#evaluation-of-model","text":"The job function for evaluation is basically same as job function for training. The small difference is that the model we use is already saved in evaluation process. Thus, initialization and update of model during iteration are not needed.","title":"Evaluation of Model"},{"location":"quick_start/lenet_mnist.html#job-function-for-evaluation","text":"@flow . global_function ( type = \"predict\" ) def eval_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> Tuple [ tp . Numpy , tp . Numpy ]: with flow . scope . placement ( \"gpu\" , \"0:0\" ): logits = lenet ( images , train = False ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) return ( labels , logits ) Code above is the implementation of job function for evaluation and its return type is declared as Tuple[tp.Numpy, tp.Numpy] . Tuple have two numpy in it. We will call the job function and calculate the accuracy according to the return values.","title":"Job Function for Evaluation"},{"location":"quick_start/lenet_mnist.html#process-of-evaluation","text":"The acc function is used to count the total number of samples and the number of correct prediction results. We will call the job function to get paramters labels and logits : g_total = 0 g_correct = 0 def acc ( labels , logits ): global g_total global g_correct predictions = np . argmax ( logits , 1 ) right_count = np . sum ( predictions == labels ) g_total += labels . shape [ 0 ] g_correct += right_count Call the job function for evaluation: if __name__ == \"__main__\" : flow . load_variables ( flow . checkpoint . get ( \"./lenet_models_1\" )) ( train_images , train_labels ), ( test_images , test_labels ) = flow . data . load_mnist ( BATCH_SIZE , BATCH_SIZE ) for epoch in range ( 1 ): for i , ( images , labels ) in enumerate ( zip ( test_images , test_labels )): labels , logits = eval_job ( images , labels ) acc ( labels , logits ) print ( \"accuracy: {0:.1f} %\" . format ( g_correct * 100 / g_total )) So far, we call the job function for evaluation looply and print the accuracy of evaluation result on MNIST testing set.","title":"Process of Evaluation"},{"location":"quick_start/lenet_mnist.html#image-prediction","text":"After making a few changes to the code above, it will take the data from the raw images rather than existing dataset. Then we can get a model to predict the content from the images. def load_image ( file ): im = Image . open ( file ) . convert ( \"L\" ) im = im . resize (( 28 , 28 ), Image . ANTIALIAS ) im = np . array ( im ) . reshape ( 1 , 1 , 28 , 28 ) . astype ( np . float32 ) im = ( im - 128.0 ) / 255.0 im . reshape (( - 1 , 1 , 1 , im . shape [ 1 ], im . shape [ 2 ])) return im def main (): if len ( sys . argv ) != 2 : usage () return flow . load_variables ( flow . checkpoint . get ( \"./lenet_models_1\" )) image = load_image ( sys . argv [ 1 ]) logits = test_job ( image ) prediction = np . argmax ( logits , 1 ) print ( \"prediction: {} \" . format ( prediction [ 0 ])) if __name__ == \"__main__\" : main ()","title":"Image Prediction"},{"location":"quick_start/lenet_mnist.html#code","text":"","title":"Code"},{"location":"quick_start/lenet_mnist.html#model-training","text":"Script: lenet_train.py","title":"Model training"},{"location":"quick_start/lenet_mnist.html#model-evaluation","text":"Script: lenet_eval.py Saved model: lenet_models_1.zip","title":"Model evaluation"},{"location":"quick_start/lenet_mnist.html#digits-prediction","text":"Script: lenet_test.py Saved model: lenet_models_1.zip MNIST image dataset mnist_raw_images.zip","title":"Digits prediction"},{"location":"quick_start/quickstart_in_3_min.html","text":"This article introduces how to quickly get start with OneFlow. We can complete a full neural network training process just in 3 minutes. Example \u00b6 With OneFlow installed, you can run the following command to download mlp_mnist.py python script from repository and run it. wget https://docs.oneflow.org/en/code/quick_start/mlp_mnist.py python3 mlp_mnist.py The output looks like below: Epoch [1/20], Loss: 2.3155 Epoch [1/20], Loss: 0.7955 Epoch [1/20], Loss: 0.4653 Epoch [1/20], Loss: 0.2064 Epoch [1/20], Loss: 0.2683 Epoch [1/20], Loss: 0.3167 ... The output is a series of numbers representing the loss values while training. The goal of training is to make the loss value as small as possible. So far, you have completed a full neural network training by using OneFlow. Code Explanation \u00b6 The following is the full code. # mlp_mnist.py import oneflow as flow import oneflow.typing as tp import numpy as np BATCH_SIZE = 100 @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : with flow . scope . placement ( \"cpu\" , \"0:0\" ): reshape = flow . reshape ( images , [ images . shape [ 0 ], - 1 ]) initializer1 = flow . random_uniform_initializer ( - 1 / 28.0 , 1 / 28.0 ) hidden = flow . layers . dense ( reshape , 500 , activation = flow . nn . relu , kernel_initializer = initializer1 , bias_initializer = initializer1 , name = \"dense1\" , ) initializer2 = flow . random_uniform_initializer ( - np . sqrt ( 1 / 500.0 ), np . sqrt ( 1 / 500.0 )) logits = flow . layers . dense ( hidden , 10 , kernel_initializer = initializer2 , bias_initializer = initializer2 , name = \"dense2\" ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits ) lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.001 ]) flow . optimizer . Adam ( lr_scheduler ) . minimize ( loss ) return loss if __name__ == \"__main__\" : ( train_images , train_labels ), ( test_images , test_labels ) = flow . data . load_mnist ( BATCH_SIZE , BATCH_SIZE ) for epoch in range ( 20 ): for i , ( images , labels ) in enumerate ( zip ( train_images , train_labels )): loss = train_job ( images , labels ) if i % 20 == 0 : print ( 'Epoch [ {} / {} ], Loss: {:.4f} ' . format ( epoch + 1 , 20 , loss . mean ())) The next section is a brief description of this code. A special feature of OneFlow compares to other deep learning frameworks is: @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : train_job function which decorated by @flow.global_function is called \"job function\". Unless functions are decorated by @flow.global_function , or they can not be recognized by OneFlow. The parameter type is used to specify the type of job: type=\"train\" means it's a training job and type=\"predict\" means evaluation or prediction job. In OneFlow, a neural network training or prediction task needs two pieces of information: One part is the structure of neural network and its related parameters. These are defined in the job function which mentioned above. The other part is the configuration of training to the network. For example, learning rate and type of model optimizer. These are defined by code as below: lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.001 ]) flow . optimizer . Adam ( lr_scheduler ) . minimize ( loss ) Besides the job function definition and configuration which mentioned above, code in this script contains all the points of how to train a neural network. flow.data.load_mnist(BATCH_SIZE,BATCH_SIZE) : Prepare and load training data. train_job(images, labels) : return the loss value for each iteration. print(..., loss.mean()) : print loss values for every 20 iterations. This page is just a simple example on neural network. A more comprehensive and detailed introduction of OneFlow can be found in Convolution Neural Network for Handwriting Recognition . In addition, you can refer to Basic topics to learn more about how to use OneFlow for deep learning. Benchmarks and related scripts for some prevalent networks are also provided in repository OneFlow-Benchmark . FAQ \u00b6 Getting stuck when running this script It may be that the incorrect proxy is set in the environment. You can cancel the proxy by first running the command unset http_proxy unset https_proxy unset HTTP_PROXY unset HTTPS_PROXY Then try again My computer can't connect to the Internet and keeps getting stuck when I run the script. This script will automatically download the required data file from the network. If your computer is not connected to the Internet, you will need to download it manually by clicking here and placing it in the script mlp_ mnist.py in the same path and then try again.","title":"Quick Start in 3 Minutes"},{"location":"quick_start/quickstart_in_3_min.html#example","text":"With OneFlow installed, you can run the following command to download mlp_mnist.py python script from repository and run it. wget https://docs.oneflow.org/en/code/quick_start/mlp_mnist.py python3 mlp_mnist.py The output looks like below: Epoch [1/20], Loss: 2.3155 Epoch [1/20], Loss: 0.7955 Epoch [1/20], Loss: 0.4653 Epoch [1/20], Loss: 0.2064 Epoch [1/20], Loss: 0.2683 Epoch [1/20], Loss: 0.3167 ... The output is a series of numbers representing the loss values while training. The goal of training is to make the loss value as small as possible. So far, you have completed a full neural network training by using OneFlow.","title":"Example"},{"location":"quick_start/quickstart_in_3_min.html#code-explanation","text":"The following is the full code. # mlp_mnist.py import oneflow as flow import oneflow.typing as tp import numpy as np BATCH_SIZE = 100 @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : with flow . scope . placement ( \"cpu\" , \"0:0\" ): reshape = flow . reshape ( images , [ images . shape [ 0 ], - 1 ]) initializer1 = flow . random_uniform_initializer ( - 1 / 28.0 , 1 / 28.0 ) hidden = flow . layers . dense ( reshape , 500 , activation = flow . nn . relu , kernel_initializer = initializer1 , bias_initializer = initializer1 , name = \"dense1\" , ) initializer2 = flow . random_uniform_initializer ( - np . sqrt ( 1 / 500.0 ), np . sqrt ( 1 / 500.0 )) logits = flow . layers . dense ( hidden , 10 , kernel_initializer = initializer2 , bias_initializer = initializer2 , name = \"dense2\" ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits ) lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.001 ]) flow . optimizer . Adam ( lr_scheduler ) . minimize ( loss ) return loss if __name__ == \"__main__\" : ( train_images , train_labels ), ( test_images , test_labels ) = flow . data . load_mnist ( BATCH_SIZE , BATCH_SIZE ) for epoch in range ( 20 ): for i , ( images , labels ) in enumerate ( zip ( train_images , train_labels )): loss = train_job ( images , labels ) if i % 20 == 0 : print ( 'Epoch [ {} / {} ], Loss: {:.4f} ' . format ( epoch + 1 , 20 , loss . mean ())) The next section is a brief description of this code. A special feature of OneFlow compares to other deep learning frameworks is: @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : train_job function which decorated by @flow.global_function is called \"job function\". Unless functions are decorated by @flow.global_function , or they can not be recognized by OneFlow. The parameter type is used to specify the type of job: type=\"train\" means it's a training job and type=\"predict\" means evaluation or prediction job. In OneFlow, a neural network training or prediction task needs two pieces of information: One part is the structure of neural network and its related parameters. These are defined in the job function which mentioned above. The other part is the configuration of training to the network. For example, learning rate and type of model optimizer. These are defined by code as below: lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.001 ]) flow . optimizer . Adam ( lr_scheduler ) . minimize ( loss ) Besides the job function definition and configuration which mentioned above, code in this script contains all the points of how to train a neural network. flow.data.load_mnist(BATCH_SIZE,BATCH_SIZE) : Prepare and load training data. train_job(images, labels) : return the loss value for each iteration. print(..., loss.mean()) : print loss values for every 20 iterations. This page is just a simple example on neural network. A more comprehensive and detailed introduction of OneFlow can be found in Convolution Neural Network for Handwriting Recognition . In addition, you can refer to Basic topics to learn more about how to use OneFlow for deep learning. Benchmarks and related scripts for some prevalent networks are also provided in repository OneFlow-Benchmark .","title":"Code Explanation"},{"location":"quick_start/quickstart_in_3_min.html#faq","text":"Getting stuck when running this script It may be that the incorrect proxy is set in the environment. You can cancel the proxy by first running the command unset http_proxy unset https_proxy unset HTTP_PROXY unset HTTPS_PROXY Then try again My computer can't connect to the Internet and keeps getting stuck when I run the script. This script will automatically download the required data file from the network. If your computer is not connected to the Internet, you will need to download it manually by clicking here and placing it in the script mlp_ mnist.py in the same path and then try again.","title":"FAQ"}]}