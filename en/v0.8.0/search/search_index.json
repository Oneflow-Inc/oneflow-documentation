{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"","title":"Home"},{"location":"basics/01_quickstart.html","text":"QUICKSTART \u00b6 This section will take the training process of FashionMNIST as an example to briefly show how OneFlow can be used to accomplish common tasks in deep learning. Refer to the links in each section to the presentation on each subtask. Let\u2019s start by importing the necessary libraries: import oneflow as flow import oneflow.nn as nn from flowvision import transforms from flowvision import datasets FlowVision is a tool library matching with OneFlow, specific to computer vision tasks. It contains a number of models, data augmentation methods, data transformation operations and datasets. Here we import and use the data transformation module transforms and datasets module datasets provided by FlowVision. Settting batch size and device\uff1a BATCH_SIZE = 64 DEVICE = \"cuda\" if flow . cuda . is_available () else \"cpu\" print ( \"Using {} device\" . format ( DEVICE )) Loading Data \u00b6 OneFlow has two primitives to load data, which are Dataset and DataLoader . The flowvision.datasets module contains a number of real data sets (such as MNIST, CIFAR 10, FashionMNIST). We can use flowvision.datasets.FashionMNIST to get the training set and test set data of FashionMNIST. training_data = datasets . FashionMNIST ( root = \"data\" , train = True , transform = transforms . ToTensor (), download = True , source_url = \"https://oneflow-public.oss-cn-beijing.aliyuncs.com/datasets/mnist/Fashion-MNIST/\" , ) test_data = datasets . FashionMNIST ( root = \"data\" , train = False , transform = transforms . ToTensor (), download = True , source_url = \"https://oneflow-public.oss-cn-beijing.aliyuncs.com/datasets/mnist/Fashion-MNIST/\" , ) Out: Downloading https://oneflow-public.oss-cn-beijing.aliyuncs.com/datasets/mnist/Fashion-MNIST/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz 26422272/? [00:15<00:00, 2940814.54it/s] Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw ... The data will be downloaded and extracted to ./data directory. The oneflow.utils.data.DataLoader wraps an iterable around the dataset . train_dataloader = flow . utils . data . DataLoader ( training_data , BATCH_SIZE , shuffle = True ) test_dataloader = flow . utils . data . DataLoader ( test_data , BATCH_SIZE , shuffle = False ) for x , y in train_dataloader : print ( \"x.shape:\" , x . shape ) print ( \"y.shape:\" , y . shape ) break Out: x.shape: flow.Size([64, 1, 28, 28]) y.shape: flow.Size([64]) Dataset and Dataloader Building Networks \u00b6 To define a neural network in OneFlow, we create a class that inherits from nn.Module . We define the layers of the network in the __init__ function and specify how data will pass through the network in the forward function. class NeuralNetwork ( nn . Module ): def __init__ ( self ): super ( NeuralNetwork , self ) . __init__ () self . flatten = nn . Flatten () self . linear_relu_stack = nn . Sequential ( nn . Linear ( 28 * 28 , 512 ), nn . ReLU (), nn . Linear ( 512 , 512 ), nn . ReLU (), nn . Linear ( 512 , 10 ), ) def forward ( self , x ): x = self . flatten ( x ) logits = self . linear_relu_stack ( x ) return logits model = NeuralNetwork () . to ( DEVICE ) print ( model ) Out: NeuralNetwork( (flatten): Flatten(start_dim=1, end_dim=-1) (linear_relu_stack): Sequential( (0): Linear(in_features=784, out_features=512, bias=True) (1): ReLU() (2): Linear(in_features=512, out_features=512, bias=True) (3): ReLU() (4): Linear(in_features=512, out_features=10, bias=True) ) ) Build Network Training Models \u00b6 To train a model, we need a loss function ( loss_fn ) and an optimizer ( optimizer ). The loss function is used to evaluate the difference between the prediction of the neural network and the real label. The optimizer adjusts the parameters of the neural network to make the prediction closer to the real label (expected answer). Here, we use oneflow.optim.SGD to be our optimizer. This process is called back propagation. loss_fn = nn . CrossEntropyLoss () . to ( DEVICE ) optimizer = flow . optim . SGD ( model . parameters (), lr = 1e-3 ) The train function is defined for training. In a single training loop, the model makes forward propagation, calculates loss, and backpropagates to update the model's parameters. def train ( iter , model , loss_fn , optimizer ): size = len ( iter . dataset ) for batch , ( x , y ) in enumerate ( iter ): x = x . to ( DEVICE ) y = y . to ( DEVICE ) # Compute prediction error pred = model ( x ) loss = loss_fn ( pred , y ) # Backpropagation optimizer . zero_grad () loss . backward () optimizer . step () current = batch * BATCH_SIZE if batch % 100 == 0 : print ( f \"loss: { loss : >7f } [ { current : >5d } / { size : >5d } ]\" ) We also define a test function to verify the accuracy of the model: def test ( iter , model , loss_fn ): size = len ( iter . dataset ) num_batches = len ( iter ) model . eval () test_loss , correct = 0 , 0 with flow . no_grad (): for x , y in iter : x = x . to ( DEVICE ) y = y . to ( DEVICE ) pred = model ( x ) test_loss += loss_fn ( pred , y ) bool_value = ( pred . argmax ( 1 ) . to ( dtype = flow . int64 ) == y ) correct += float ( bool_value . sum () . numpy ()) test_loss /= num_batches print ( \"test_loss\" , test_loss , \"num_batches \" , num_batches ) correct /= size print ( f \"Test Error: \\n Accuracy: { ( 100 * correct ) : >0.1f } , Avg loss: { test_loss : >8f } \" ) We use the train function to begin the train process for several epochs and use the test function to assess the accuracy of the network at the end of each epoch: epochs = 5 for t in range ( epochs ): print ( f \"Epoch { t + 1 } \\n -------------------------------\" ) train ( train_dataloader , model , loss_fn , optimizer ) test ( test_dataloader , model , loss_fn ) print ( \"Done!\" ) Out: Epoch 1 ------------------------------- loss: 2.152148 [ 0/60000] loss: 2.140148 [ 6400/60000] loss: 2.147773 [12800/60000] loss: 2.088032 [19200/60000] loss: 2.074728 [25600/60000] loss: 2.034325 [32000/60000] loss: 1.994112 [38400/60000] loss: 1.984397 [44800/60000] loss: 1.918280 [51200/60000] loss: 1.884574 [57600/60000] test_loss tensor(1.9015, device='cuda:0', dtype=oneflow.float32) num_batches 157 Test Error: Accuracy: 56.3, Avg loss: 1.901461 Epoch 2 ------------------------------- loss: 1.914766 [ 0/60000] loss: 1.817333 [ 6400/60000] loss: 1.835239 [12800/60000] ... Autograd Backpropagation and Optimizer Saving and Loading Models \u00b6 Use oneflow.save to save the model. The saved model can be then loaded by oneflow.load to make predictions. flow . save ( model . state_dict (), \"./model\" ) Model Load and Save QQ Group \u00b6 Any problems encountered during the installation or usage, welcome to join the QQ Group to discuss with OneFlow developers and enthusiasts: Add QQ group by 331883 or scan the QR code below:","title":"Quickstart"},{"location":"basics/01_quickstart.html#quickstart","text":"This section will take the training process of FashionMNIST as an example to briefly show how OneFlow can be used to accomplish common tasks in deep learning. Refer to the links in each section to the presentation on each subtask. Let\u2019s start by importing the necessary libraries: import oneflow as flow import oneflow.nn as nn from flowvision import transforms from flowvision import datasets FlowVision is a tool library matching with OneFlow, specific to computer vision tasks. It contains a number of models, data augmentation methods, data transformation operations and datasets. Here we import and use the data transformation module transforms and datasets module datasets provided by FlowVision. Settting batch size and device\uff1a BATCH_SIZE = 64 DEVICE = \"cuda\" if flow . cuda . is_available () else \"cpu\" print ( \"Using {} device\" . format ( DEVICE ))","title":"QUICKSTART"},{"location":"basics/01_quickstart.html#loading-data","text":"OneFlow has two primitives to load data, which are Dataset and DataLoader . The flowvision.datasets module contains a number of real data sets (such as MNIST, CIFAR 10, FashionMNIST). We can use flowvision.datasets.FashionMNIST to get the training set and test set data of FashionMNIST. training_data = datasets . FashionMNIST ( root = \"data\" , train = True , transform = transforms . ToTensor (), download = True , source_url = \"https://oneflow-public.oss-cn-beijing.aliyuncs.com/datasets/mnist/Fashion-MNIST/\" , ) test_data = datasets . FashionMNIST ( root = \"data\" , train = False , transform = transforms . ToTensor (), download = True , source_url = \"https://oneflow-public.oss-cn-beijing.aliyuncs.com/datasets/mnist/Fashion-MNIST/\" , ) Out: Downloading https://oneflow-public.oss-cn-beijing.aliyuncs.com/datasets/mnist/Fashion-MNIST/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz 26422272/? [00:15<00:00, 2940814.54it/s] Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw ... The data will be downloaded and extracted to ./data directory. The oneflow.utils.data.DataLoader wraps an iterable around the dataset . train_dataloader = flow . utils . data . DataLoader ( training_data , BATCH_SIZE , shuffle = True ) test_dataloader = flow . utils . data . DataLoader ( test_data , BATCH_SIZE , shuffle = False ) for x , y in train_dataloader : print ( \"x.shape:\" , x . shape ) print ( \"y.shape:\" , y . shape ) break Out: x.shape: flow.Size([64, 1, 28, 28]) y.shape: flow.Size([64]) Dataset and Dataloader","title":"Loading Data"},{"location":"basics/01_quickstart.html#building-networks","text":"To define a neural network in OneFlow, we create a class that inherits from nn.Module . We define the layers of the network in the __init__ function and specify how data will pass through the network in the forward function. class NeuralNetwork ( nn . Module ): def __init__ ( self ): super ( NeuralNetwork , self ) . __init__ () self . flatten = nn . Flatten () self . linear_relu_stack = nn . Sequential ( nn . Linear ( 28 * 28 , 512 ), nn . ReLU (), nn . Linear ( 512 , 512 ), nn . ReLU (), nn . Linear ( 512 , 10 ), ) def forward ( self , x ): x = self . flatten ( x ) logits = self . linear_relu_stack ( x ) return logits model = NeuralNetwork () . to ( DEVICE ) print ( model ) Out: NeuralNetwork( (flatten): Flatten(start_dim=1, end_dim=-1) (linear_relu_stack): Sequential( (0): Linear(in_features=784, out_features=512, bias=True) (1): ReLU() (2): Linear(in_features=512, out_features=512, bias=True) (3): ReLU() (4): Linear(in_features=512, out_features=10, bias=True) ) ) Build Network","title":"Building Networks"},{"location":"basics/01_quickstart.html#training-models","text":"To train a model, we need a loss function ( loss_fn ) and an optimizer ( optimizer ). The loss function is used to evaluate the difference between the prediction of the neural network and the real label. The optimizer adjusts the parameters of the neural network to make the prediction closer to the real label (expected answer). Here, we use oneflow.optim.SGD to be our optimizer. This process is called back propagation. loss_fn = nn . CrossEntropyLoss () . to ( DEVICE ) optimizer = flow . optim . SGD ( model . parameters (), lr = 1e-3 ) The train function is defined for training. In a single training loop, the model makes forward propagation, calculates loss, and backpropagates to update the model's parameters. def train ( iter , model , loss_fn , optimizer ): size = len ( iter . dataset ) for batch , ( x , y ) in enumerate ( iter ): x = x . to ( DEVICE ) y = y . to ( DEVICE ) # Compute prediction error pred = model ( x ) loss = loss_fn ( pred , y ) # Backpropagation optimizer . zero_grad () loss . backward () optimizer . step () current = batch * BATCH_SIZE if batch % 100 == 0 : print ( f \"loss: { loss : >7f } [ { current : >5d } / { size : >5d } ]\" ) We also define a test function to verify the accuracy of the model: def test ( iter , model , loss_fn ): size = len ( iter . dataset ) num_batches = len ( iter ) model . eval () test_loss , correct = 0 , 0 with flow . no_grad (): for x , y in iter : x = x . to ( DEVICE ) y = y . to ( DEVICE ) pred = model ( x ) test_loss += loss_fn ( pred , y ) bool_value = ( pred . argmax ( 1 ) . to ( dtype = flow . int64 ) == y ) correct += float ( bool_value . sum () . numpy ()) test_loss /= num_batches print ( \"test_loss\" , test_loss , \"num_batches \" , num_batches ) correct /= size print ( f \"Test Error: \\n Accuracy: { ( 100 * correct ) : >0.1f } , Avg loss: { test_loss : >8f } \" ) We use the train function to begin the train process for several epochs and use the test function to assess the accuracy of the network at the end of each epoch: epochs = 5 for t in range ( epochs ): print ( f \"Epoch { t + 1 } \\n -------------------------------\" ) train ( train_dataloader , model , loss_fn , optimizer ) test ( test_dataloader , model , loss_fn ) print ( \"Done!\" ) Out: Epoch 1 ------------------------------- loss: 2.152148 [ 0/60000] loss: 2.140148 [ 6400/60000] loss: 2.147773 [12800/60000] loss: 2.088032 [19200/60000] loss: 2.074728 [25600/60000] loss: 2.034325 [32000/60000] loss: 1.994112 [38400/60000] loss: 1.984397 [44800/60000] loss: 1.918280 [51200/60000] loss: 1.884574 [57600/60000] test_loss tensor(1.9015, device='cuda:0', dtype=oneflow.float32) num_batches 157 Test Error: Accuracy: 56.3, Avg loss: 1.901461 Epoch 2 ------------------------------- loss: 1.914766 [ 0/60000] loss: 1.817333 [ 6400/60000] loss: 1.835239 [12800/60000] ... Autograd Backpropagation and Optimizer","title":"Training Models"},{"location":"basics/01_quickstart.html#saving-and-loading-models","text":"Use oneflow.save to save the model. The saved model can be then loaded by oneflow.load to make predictions. flow . save ( model . state_dict (), \"./model\" ) Model Load and Save","title":"Saving and Loading Models"},{"location":"basics/01_quickstart.html#qq-group","text":"Any problems encountered during the installation or usage, welcome to join the QQ Group to discuss with OneFlow developers and enthusiasts: Add QQ group by 331883 or scan the QR code below:","title":"QQ Group"},{"location":"basics/02_tensor.html","text":"TENSORS \u00b6 The data in the neural network is stored in tensors, which are similar to arrays and mathematical matrices. OneFlow provides a series of operators on tensors. Tensors, together with operators, build up a neural network. Tensors differ from regular multidimensional arrays in that they can run on AI chips (such as the Nvidia GPU) and CPU, thus increasing computing speed. In addition, OneFlow provides Autograd function, which supports automatic differentiation. import oneflow as flow import numpy as np Creating Tensors \u00b6 There are several ways to create tensors, including: Directly from data From a NumPy array By an operator Directly from data \u00b6 Tensors can be created directly from data: x1 = flow . tensor ([[ 1 , 2 ], [ 3 , 4 ]]) x2 = flow . tensor ([[ 1.0 , 2.0 ], [ 3.0 , 4.0 ]]) print ( x1 ) print ( x2 ) Out: tensor([[1, 2], [3, 4]], dtype=oneflow.int64) tensor([[1., 2.], [3., 4.]], dtype=oneflow.float32) We can see that the tensor x1 and x2 are created, whose data types are int64 and float32 , respectively. From a NumPy array \u00b6 Tensors can be created from NumPy arrays by passing the NumPy array as a parameter when the tensor object is constructed. x3 = flow . tensor ( np . ones (( 2 , 3 ))) x4 = flow . tensor ( np . random . rand ( 2 , 3 )) print ( x3 ) print ( x4 ) Out: tensor([[1., 1., 1.], [1., 1., 1.]], dtype=oneflow.float64) tensor([[0.6213, 0.6142, 0.1592], [0.5539, 0.8453, 0.8576]], dtype=oneflow.float64) By an operator \u00b6 There are also many operators available in OneFlow that can be used to create tensors. For example, ones , zeros and eye , which create the all-ones tensor, zero tensor, and identity tensor, respectively. x5 = flow . ones ( 2 , 3 ) x6 = flow . zeros ( 2 , 3 ) x7 = flow . eye ( 3 ) print ( x5 ) print ( x6 ) print ( x7 ) Out: tensor([[1., 1., 1.], [1., 1., 1.]], dtype=oneflow.float32) tensor([[0., 0., 0.], [0., 0., 0.]], dtype=oneflow.float32) tensor([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]], dtype=oneflow.float32) The randn method creates a random tensor: x8 = flow . randn ( 2 , 3 ) Difference Between Tensor and tensor \u00b6 There are two interfaces ( oneflow.Tensor and oneflow.tensor ) in OneFlow, both of which can be used to create tensors. What\u2019s the difference between them? Briefly speaking, the data type of oneflow.Tensor is limited to float32 by default, while the data type of oneflow.tensor can be changed when the data is created. The following code illustrates the difference: print ( flow . Tensor ([ 1 , 2 , 3 ])) print ( flow . tensor ([ 1 , 2 , 3 ])) print ( flow . tensor ([ 1.0 , 2.0 , 3.0 ])) Out: tensor([1., 2., 3.], dtype=oneflow.float32) tensor([1, 2, 3], dtype=oneflow.int64) tensor([1., 2., 3.], dtype=oneflow.float32) Besides, oneflow.Tensor can be created without specific data: x9 = flow . Tensor ( 2 , 3 ) print ( x9 . shape ) Out: flow.Size([2, 3]) Therefore, use oneflow.Tensor to create a tensor if you do not want to specify an explicit value, otherwise, you should use oneflow.tensor . Attributes of a Tensor \u00b6 The shape , dtype , and device attributes of a tensor describe its shape, data type, and device type respectively. x9 = flow . randn ( 1 , 4 ) print ( x9 . shape ) print ( x9 . dtype ) print ( x9 . device ) Out: flow.Size([1, 4]) oneflow.float32 cpu:0 The output shows the shape, the data type, and the device (on CPU No. 0, CPUs were numbered because OneFlow naturally supports distribution, see Global Tensor ). The shape of the tensor can be changed by the reshape method, and the data type and device of the tensor can be changed by the Tensor.to method: x10 = x9 . reshape ( 2 , 2 ) x11 = x10 . to ( dtype = flow . int32 , device = flow . device ( \"cuda\" )) print ( x10 . shape ) print ( x11 . dtype , x11 . device ) Out: oneflow.Size([2, 2]) oneflow.int32 cuda:0 Operations on Tensors \u00b6 A large number of operators are provided in OneFlow, most of which are in the namespaces of oneflow , oneflow.Tensor , oneflow.nn , and oneflow.nn.functional . Tensors in OneFlow are as easy to use as the NumPy arrays. For example, slicing in NumPy style is supported: tensor = flow . ones ( 4 , 4 ) print ( 'First row: ' , tensor [ 0 ]) print ( 'First column: ' , tensor [:, 0 ]) print ( 'Last column:' , tensor [ ... , - 1 ]) tensor [:, 1 ] = 0 print ( tensor ) Out: First row: tensor([1., 1., 1., 1.], dtype=oneflow.float32) First column: tensor([1., 1., 1., 1.], dtype=oneflow.float32) Last column: tensor([1., 1., 1., 1.], dtype=oneflow.float32) tensor([[1., 0., 1., 1.], [1., 0., 1., 1.], [1., 0., 1., 1.], [1., 0., 1., 1.]], dtype=oneflow.float32) In addition, there are many other operations in OneFlow, such as add , sub , mul , div for arithmetic operations; scatter , gather for positional operations; and activation functions ( relu ), convolution functions ( conv2d ), etc. Click on their links to see detailed API description and find out more about other operators.","title":"Tensor"},{"location":"basics/02_tensor.html#tensors","text":"The data in the neural network is stored in tensors, which are similar to arrays and mathematical matrices. OneFlow provides a series of operators on tensors. Tensors, together with operators, build up a neural network. Tensors differ from regular multidimensional arrays in that they can run on AI chips (such as the Nvidia GPU) and CPU, thus increasing computing speed. In addition, OneFlow provides Autograd function, which supports automatic differentiation. import oneflow as flow import numpy as np","title":"TENSORS"},{"location":"basics/02_tensor.html#creating-tensors","text":"There are several ways to create tensors, including: Directly from data From a NumPy array By an operator","title":"Creating Tensors"},{"location":"basics/02_tensor.html#directly-from-data","text":"Tensors can be created directly from data: x1 = flow . tensor ([[ 1 , 2 ], [ 3 , 4 ]]) x2 = flow . tensor ([[ 1.0 , 2.0 ], [ 3.0 , 4.0 ]]) print ( x1 ) print ( x2 ) Out: tensor([[1, 2], [3, 4]], dtype=oneflow.int64) tensor([[1., 2.], [3., 4.]], dtype=oneflow.float32) We can see that the tensor x1 and x2 are created, whose data types are int64 and float32 , respectively.","title":"Directly from data"},{"location":"basics/02_tensor.html#from-a-numpy-array","text":"Tensors can be created from NumPy arrays by passing the NumPy array as a parameter when the tensor object is constructed. x3 = flow . tensor ( np . ones (( 2 , 3 ))) x4 = flow . tensor ( np . random . rand ( 2 , 3 )) print ( x3 ) print ( x4 ) Out: tensor([[1., 1., 1.], [1., 1., 1.]], dtype=oneflow.float64) tensor([[0.6213, 0.6142, 0.1592], [0.5539, 0.8453, 0.8576]], dtype=oneflow.float64)","title":"From a NumPy array"},{"location":"basics/02_tensor.html#by-an-operator","text":"There are also many operators available in OneFlow that can be used to create tensors. For example, ones , zeros and eye , which create the all-ones tensor, zero tensor, and identity tensor, respectively. x5 = flow . ones ( 2 , 3 ) x6 = flow . zeros ( 2 , 3 ) x7 = flow . eye ( 3 ) print ( x5 ) print ( x6 ) print ( x7 ) Out: tensor([[1., 1., 1.], [1., 1., 1.]], dtype=oneflow.float32) tensor([[0., 0., 0.], [0., 0., 0.]], dtype=oneflow.float32) tensor([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]], dtype=oneflow.float32) The randn method creates a random tensor: x8 = flow . randn ( 2 , 3 )","title":"By an operator"},{"location":"basics/02_tensor.html#difference-between-tensor-and-tensor","text":"There are two interfaces ( oneflow.Tensor and oneflow.tensor ) in OneFlow, both of which can be used to create tensors. What\u2019s the difference between them? Briefly speaking, the data type of oneflow.Tensor is limited to float32 by default, while the data type of oneflow.tensor can be changed when the data is created. The following code illustrates the difference: print ( flow . Tensor ([ 1 , 2 , 3 ])) print ( flow . tensor ([ 1 , 2 , 3 ])) print ( flow . tensor ([ 1.0 , 2.0 , 3.0 ])) Out: tensor([1., 2., 3.], dtype=oneflow.float32) tensor([1, 2, 3], dtype=oneflow.int64) tensor([1., 2., 3.], dtype=oneflow.float32) Besides, oneflow.Tensor can be created without specific data: x9 = flow . Tensor ( 2 , 3 ) print ( x9 . shape ) Out: flow.Size([2, 3]) Therefore, use oneflow.Tensor to create a tensor if you do not want to specify an explicit value, otherwise, you should use oneflow.tensor .","title":"Difference Between Tensor and tensor"},{"location":"basics/02_tensor.html#attributes-of-a-tensor","text":"The shape , dtype , and device attributes of a tensor describe its shape, data type, and device type respectively. x9 = flow . randn ( 1 , 4 ) print ( x9 . shape ) print ( x9 . dtype ) print ( x9 . device ) Out: flow.Size([1, 4]) oneflow.float32 cpu:0 The output shows the shape, the data type, and the device (on CPU No. 0, CPUs were numbered because OneFlow naturally supports distribution, see Global Tensor ). The shape of the tensor can be changed by the reshape method, and the data type and device of the tensor can be changed by the Tensor.to method: x10 = x9 . reshape ( 2 , 2 ) x11 = x10 . to ( dtype = flow . int32 , device = flow . device ( \"cuda\" )) print ( x10 . shape ) print ( x11 . dtype , x11 . device ) Out: oneflow.Size([2, 2]) oneflow.int32 cuda:0","title":"Attributes of a Tensor"},{"location":"basics/02_tensor.html#operations-on-tensors","text":"A large number of operators are provided in OneFlow, most of which are in the namespaces of oneflow , oneflow.Tensor , oneflow.nn , and oneflow.nn.functional . Tensors in OneFlow are as easy to use as the NumPy arrays. For example, slicing in NumPy style is supported: tensor = flow . ones ( 4 , 4 ) print ( 'First row: ' , tensor [ 0 ]) print ( 'First column: ' , tensor [:, 0 ]) print ( 'Last column:' , tensor [ ... , - 1 ]) tensor [:, 1 ] = 0 print ( tensor ) Out: First row: tensor([1., 1., 1., 1.], dtype=oneflow.float32) First column: tensor([1., 1., 1., 1.], dtype=oneflow.float32) Last column: tensor([1., 1., 1., 1.], dtype=oneflow.float32) tensor([[1., 0., 1., 1.], [1., 0., 1., 1.], [1., 0., 1., 1.], [1., 0., 1., 1.]], dtype=oneflow.float32) In addition, there are many other operations in OneFlow, such as add , sub , mul , div for arithmetic operations; scatter , gather for positional operations; and activation functions ( relu ), convolution functions ( conv2d ), etc. Click on their links to see detailed API description and find out more about other operators.","title":"Operations on Tensors"},{"location":"basics/03_dataset_dataloader.html","text":"DATASETS & DATALOADERS \u00b6 The behavior of OneFlow's Dataset and DataLoader is the same as PyTorch . Both Dataset and DataLoader are designed for making dataset management decoupling with model training. Dataset classes are used to define how to read data. For common computer vision datasets (e.g. FashionMNIST), we can use the dataset classes from datasets module of FlowVision library. These dataset classes can help us download and load some prevailing datasets automatically, and all of them inherit the Dataset class indirectly. For other datasets, we can define custom dataset classes through inheriting the Dataset class. DataLoader wraps Dataset into an iterator, for easy iterating and access to samples during training. import matplotlib.pyplot as plt import oneflow as flow import oneflow.nn as nn from oneflow.utils.data import Dataset from flowvision import datasets from flowvision import transforms The flowvision.transforms imported above provides some image data transformation operations (e.g. ToTensor is able to convert PIL images or NumPy ndarrays to tensors), which can be used in dataset classes directly. Loading a Dataset Using FlowVision \u00b6 Here is an example of how to load FashionMNIST dataset by flowvision.datasets . We pass the following parameters to the FashionMNIST class: - root : the path where the train/test data is stored; - train : True for training dataset, False for test dataset; - download=True : downloads the data from the internet if it\u2019s not available at root ; - transforms : the feature and label transformations. training_data = datasets . FashionMNIST ( root = \"data\" , train = True , download = True , transform = transforms . ToTensor (), source_url = \"https://oneflow-public.oss-cn-beijing.aliyuncs.com/datasets/mnist/Fashion-MNIST/\" , ) test_data = datasets . FashionMNIST ( root = \"data\" , train = False , download = True , transform = transforms . ToTensor (), source_url = \"https://oneflow-public.oss-cn-beijing.aliyuncs.com/datasets/mnist/Fashion-MNIST/\" , ) The first time it runs, it will download the data set and output the following: Downloading https://oneflow-public.oss-cn-beijing.aliyuncs.com/datasets/mnist/Fashion-MNIST/train-images-idx3-ubyte.gz Downloading https://oneflow-public.oss-cn-beijing.aliyuncs.com/datasets/mnist/Fashion-MNIST/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz 26422272/? [00:02<00:00, 8090800.72it/s] Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw Downloading https://oneflow-public.oss-cn-beijing.aliyuncs.com/datasets/mnist/Fashion-MNIST/train-labels-idx1-ubyte.gz Downloading https://oneflow-public.oss-cn-beijing.aliyuncs.com/datasets/mnist/Fashion-MNIST/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz 29696/? [00:00<00:00, 806948.09it/s] Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw Downloading https://oneflow-public.oss-cn-beijing.aliyuncs.com/datasets/mnist/Fashion-MNIST/t10k-images-idx3-ubyte.gz Downloading https://oneflow-public.oss-cn-beijing.aliyuncs.com/datasets/mnist/Fashion-MNIST/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz 4422656/? [00:00<00:00, 19237994.98it/s] Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw Downloading https://oneflow-public.oss-cn-beijing.aliyuncs.com/datasets/mnist/Fashion-MNIST/t10k-labels-idx1-ubyte.gz Downloading https://oneflow-public.oss-cn-beijing.aliyuncs.com/datasets/mnist/Fashion-MNIST/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz 6144/? [00:00<00:00, 152710.85it/s] Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw Iterating the Dataset \u00b6 We can index Dataset manually like a list : training_data[index] . The following example randomly accesses 9 pictures in training_data and visualizes them. labels_map = { 0 : \"T-Shirt\" , 1 : \"Trouser\" , 2 : \"Pullover\" , 3 : \"Dress\" , 4 : \"Coat\" , 5 : \"Sandal\" , 6 : \"Shirt\" , 7 : \"Sneaker\" , 8 : \"Bag\" , 9 : \"Ankle Boot\" , } figure = plt . figure ( figsize = ( 8 , 8 )) cols , rows = 3 , 3 from random import randint for i in range ( 1 , cols * rows + 1 ): sample_idx = randint ( 0 , len ( training_data )) img , label = training_data [ sample_idx ] figure . add_subplot ( rows , cols , i ) plt . title ( labels_map [ label ]) plt . axis ( \"off\" ) plt . imshow ( img . squeeze () . numpy (), cmap = \"gray\" ) plt . show () Creating a Custom Dataset for Your Files \u00b6 A custom dataset can be defined by inheriting oneflow.utils.data.Dataset . Custom Dataset can be used with Dataloader introduced in the next section to simplify data processing. Here is an example of how to create a custom Dataset , the key steps are: Inheriting oneflow.utils.data.Dataset Implements the __len__ method that returns the number of samples in our dataset. Implements the __getitem__ method that loads and returns a sample from the dataset when users call dataset_obj[idx] . import numpy as np class CustomDataset ( Dataset ): raw_data_x = np . array ([[ 1 , 2 ], [ 2 , 3 ], [ 4 , 6 ], [ 3 , 1 ]], dtype = np . float32 ) raw_label = np . array ([[ 8 ], [ 13 ], [ 26 ], [ 9 ]], dtype = np . float32 ) def __init__ ( self , transform = None , target_transform = None ): self . transform = transform self . target_transform = target_transform def __len__ ( self ): return len ( raw_label ) def __getitem__ ( self , idx ): x = CustomDataset . raw_data_x [ idx ] label = CustomDataset . raw_label [ idx ] if self . transform : x = self . transform ( x ) if self . target_transform : label = self . target_transform ( label ) return x , label custom_dataset = CustomDataset () print ( custom_dataset [ 0 ]) print ( custom_dataset [ 1 ]) Output\uff1a (array([1., 2.], dtype=float32), array([8.], dtype=float32)) (array([2., 3.], dtype=float32), array([13.], dtype=float32)) Using DataLoader \u00b6 The Dataset retrieves all features of our dataset and labels one sample at a time. While training a model, we typically want to pass samples in \"minibatches\", which means they will load a same amount of data as the batch size at the time, and reshuffle the data at every epoch to reduce model overfitting. At this time, we can use DataLoader . DataLoader can wrap Dataset into an iterator to access data during the training loop. Here is an example: batch_size=64 : the batch size at each iteration shuffle : whether the data is shuffled after we iterate over all batches from oneflow.utils.data import DataLoader train_dataloader = DataLoader ( training_data , batch_size = 64 , shuffle = True ) x , label = next ( iter ( train_dataloader )) print ( f \"shape of x: { x . shape } , shape of label: { label . shape } \" ) Output\uff1a shape of x:oneflow.Size([64, 1, 28, 28]), shape of label: oneflow.Size([64]) img = x [ 0 ] . squeeze () . numpy () label = label [ 0 ] plt . imshow ( img , cmap = \"gray\" ) plt . show () print ( label ) Output\uff1a(output a picture randomly) tensor(9, dtype=oneflow.int64) We can also use the DataLoader iterator during the training loop. for x , label in train_dataloader : print ( x . shape , label . shape ) # training...","title":"Datesets & Dataloaders"},{"location":"basics/03_dataset_dataloader.html#datasets-dataloaders","text":"The behavior of OneFlow's Dataset and DataLoader is the same as PyTorch . Both Dataset and DataLoader are designed for making dataset management decoupling with model training. Dataset classes are used to define how to read data. For common computer vision datasets (e.g. FashionMNIST), we can use the dataset classes from datasets module of FlowVision library. These dataset classes can help us download and load some prevailing datasets automatically, and all of them inherit the Dataset class indirectly. For other datasets, we can define custom dataset classes through inheriting the Dataset class. DataLoader wraps Dataset into an iterator, for easy iterating and access to samples during training. import matplotlib.pyplot as plt import oneflow as flow import oneflow.nn as nn from oneflow.utils.data import Dataset from flowvision import datasets from flowvision import transforms The flowvision.transforms imported above provides some image data transformation operations (e.g. ToTensor is able to convert PIL images or NumPy ndarrays to tensors), which can be used in dataset classes directly.","title":"DATASETS &amp; DATALOADERS"},{"location":"basics/03_dataset_dataloader.html#loading-a-dataset-using-flowvision","text":"Here is an example of how to load FashionMNIST dataset by flowvision.datasets . We pass the following parameters to the FashionMNIST class: - root : the path where the train/test data is stored; - train : True for training dataset, False for test dataset; - download=True : downloads the data from the internet if it\u2019s not available at root ; - transforms : the feature and label transformations. training_data = datasets . FashionMNIST ( root = \"data\" , train = True , download = True , transform = transforms . ToTensor (), source_url = \"https://oneflow-public.oss-cn-beijing.aliyuncs.com/datasets/mnist/Fashion-MNIST/\" , ) test_data = datasets . FashionMNIST ( root = \"data\" , train = False , download = True , transform = transforms . ToTensor (), source_url = \"https://oneflow-public.oss-cn-beijing.aliyuncs.com/datasets/mnist/Fashion-MNIST/\" , ) The first time it runs, it will download the data set and output the following: Downloading https://oneflow-public.oss-cn-beijing.aliyuncs.com/datasets/mnist/Fashion-MNIST/train-images-idx3-ubyte.gz Downloading https://oneflow-public.oss-cn-beijing.aliyuncs.com/datasets/mnist/Fashion-MNIST/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz 26422272/? [00:02<00:00, 8090800.72it/s] Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw Downloading https://oneflow-public.oss-cn-beijing.aliyuncs.com/datasets/mnist/Fashion-MNIST/train-labels-idx1-ubyte.gz Downloading https://oneflow-public.oss-cn-beijing.aliyuncs.com/datasets/mnist/Fashion-MNIST/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz 29696/? [00:00<00:00, 806948.09it/s] Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw Downloading https://oneflow-public.oss-cn-beijing.aliyuncs.com/datasets/mnist/Fashion-MNIST/t10k-images-idx3-ubyte.gz Downloading https://oneflow-public.oss-cn-beijing.aliyuncs.com/datasets/mnist/Fashion-MNIST/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz 4422656/? [00:00<00:00, 19237994.98it/s] Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw Downloading https://oneflow-public.oss-cn-beijing.aliyuncs.com/datasets/mnist/Fashion-MNIST/t10k-labels-idx1-ubyte.gz Downloading https://oneflow-public.oss-cn-beijing.aliyuncs.com/datasets/mnist/Fashion-MNIST/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz 6144/? [00:00<00:00, 152710.85it/s] Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw","title":"Loading a Dataset Using FlowVision"},{"location":"basics/03_dataset_dataloader.html#iterating-the-dataset","text":"We can index Dataset manually like a list : training_data[index] . The following example randomly accesses 9 pictures in training_data and visualizes them. labels_map = { 0 : \"T-Shirt\" , 1 : \"Trouser\" , 2 : \"Pullover\" , 3 : \"Dress\" , 4 : \"Coat\" , 5 : \"Sandal\" , 6 : \"Shirt\" , 7 : \"Sneaker\" , 8 : \"Bag\" , 9 : \"Ankle Boot\" , } figure = plt . figure ( figsize = ( 8 , 8 )) cols , rows = 3 , 3 from random import randint for i in range ( 1 , cols * rows + 1 ): sample_idx = randint ( 0 , len ( training_data )) img , label = training_data [ sample_idx ] figure . add_subplot ( rows , cols , i ) plt . title ( labels_map [ label ]) plt . axis ( \"off\" ) plt . imshow ( img . squeeze () . numpy (), cmap = \"gray\" ) plt . show ()","title":"Iterating the Dataset"},{"location":"basics/03_dataset_dataloader.html#creating-a-custom-dataset-for-your-files","text":"A custom dataset can be defined by inheriting oneflow.utils.data.Dataset . Custom Dataset can be used with Dataloader introduced in the next section to simplify data processing. Here is an example of how to create a custom Dataset , the key steps are: Inheriting oneflow.utils.data.Dataset Implements the __len__ method that returns the number of samples in our dataset. Implements the __getitem__ method that loads and returns a sample from the dataset when users call dataset_obj[idx] . import numpy as np class CustomDataset ( Dataset ): raw_data_x = np . array ([[ 1 , 2 ], [ 2 , 3 ], [ 4 , 6 ], [ 3 , 1 ]], dtype = np . float32 ) raw_label = np . array ([[ 8 ], [ 13 ], [ 26 ], [ 9 ]], dtype = np . float32 ) def __init__ ( self , transform = None , target_transform = None ): self . transform = transform self . target_transform = target_transform def __len__ ( self ): return len ( raw_label ) def __getitem__ ( self , idx ): x = CustomDataset . raw_data_x [ idx ] label = CustomDataset . raw_label [ idx ] if self . transform : x = self . transform ( x ) if self . target_transform : label = self . target_transform ( label ) return x , label custom_dataset = CustomDataset () print ( custom_dataset [ 0 ]) print ( custom_dataset [ 1 ]) Output\uff1a (array([1., 2.], dtype=float32), array([8.], dtype=float32)) (array([2., 3.], dtype=float32), array([13.], dtype=float32))","title":"Creating a Custom Dataset for Your Files"},{"location":"basics/03_dataset_dataloader.html#using-dataloader","text":"The Dataset retrieves all features of our dataset and labels one sample at a time. While training a model, we typically want to pass samples in \"minibatches\", which means they will load a same amount of data as the batch size at the time, and reshuffle the data at every epoch to reduce model overfitting. At this time, we can use DataLoader . DataLoader can wrap Dataset into an iterator to access data during the training loop. Here is an example: batch_size=64 : the batch size at each iteration shuffle : whether the data is shuffled after we iterate over all batches from oneflow.utils.data import DataLoader train_dataloader = DataLoader ( training_data , batch_size = 64 , shuffle = True ) x , label = next ( iter ( train_dataloader )) print ( f \"shape of x: { x . shape } , shape of label: { label . shape } \" ) Output\uff1a shape of x:oneflow.Size([64, 1, 28, 28]), shape of label: oneflow.Size([64]) img = x [ 0 ] . squeeze () . numpy () label = label [ 0 ] plt . imshow ( img , cmap = \"gray\" ) plt . show () print ( label ) Output\uff1a(output a picture randomly) tensor(9, dtype=oneflow.int64) We can also use the DataLoader iterator during the training loop. for x , label in train_dataloader : print ( x . shape , label . shape ) # training...","title":"Using DataLoader"},{"location":"basics/04_build_network.html","text":"BUILD NEURAL NETWORK \u00b6 The layers of a neural network can be built by API in namespace oneflow.nn , It provides common Module (such as oneflow.nn.Conv2d , oneflow.nn.ReLU ). All Module classes inherit from oneflow.nn.Module , and many simple Module can form more complex Module. In this way, users can easily build and manage complex neural networks. import oneflow as flow import oneflow.nn as nn Define Module class \u00b6 oneflow.nn provides common Module classes and we can use them easily. Or we can build a neural network by customizing the Module class on the basis. This method consists of three parts\uff1a Write a class that inherits from oneflow.nn.Module class Write the method of __init__ class, in which we construct the network structure Write the method of forward class, which calculates on the basis of the input of Module class NeuralNetwork ( nn . Module ): def __init__ ( self ): super ( NeuralNetwork , self ) . __init__ () self . flatten = nn . Flatten () self . linear_relu_stack = nn . Sequential ( nn . Linear ( 28 * 28 , 512 ), nn . ReLU (), nn . Linear ( 512 , 512 ), nn . ReLU (), nn . Linear ( 512 , 10 ), nn . ReLU () ) def forward ( self , x ): x = self . flatten ( x ) logits = self . linear_relu_stack ( x ) return logits net = NeuralNetwork () print ( net ) The above code will output the structure of the NeuralNetwork network\uff1a NeuralNetwork( (flatten): Flatten(start_dim=1, end_dim=-1) (linear_relu_stack): Sequential( (0): Linear(in_features=784, out_features=512, bias=True) (1): ReLU() (2): Linear(in_features=512, out_features=512, bias=True) (3): ReLU() (4): Linear(in_features=512, out_features=10, bias=True) (5): ReLU() ) ) Then, call net (notice\uff1aIt is not recommended to explicitly call forward ): X = flow . ones ( 1 , 28 , 28 ) logits = net ( X ) pred_probab = nn . Softmax ( dim = 1 )( logits ) y_pred = pred_probab . argmax ( 1 ) print ( f \"Predicted class: { y_pred } \" ) You will get output similar to the following: Predicted class: tensor([1], dtype=oneflow.int32) The above process of data input, network calculation and the output of reasoning is shown in the figure below. flow.nn.functional \u00b6 In addition to oneflow.nn , oneflow.nn.functional namespace also provides many API. It overlaps with oneflow.nn to some extent. For example, nn.functional.relu and nn.ReLU both can be used for activation in neural network. The main differences between them are: The API under nn is a class. It needs to be instantiated before being called; The API under nn.functional is a function. It is called directly. The API under nn manages network parameters automatically\uff1bBut for the function under NN. Functional , we need to define our own parameters and manually pass them in each call. In fact, most of the Module provided by OneFlow is the result of encapsulating the methods under nn.functional . nn.functional can manage the network more finely. The following example uses the methods in nn.functional to build a Module FunctionalNeuralNetwork equivalent to the NeuralNetwork class above. Readers can appreciate the similarities and differences between the two: class FunctionalNeuralNetwork ( nn . Module ): def __init__ ( self ): super ( FunctionalNeuralNetwork , self ) . __init__ () self . weight1 = nn . Parameter ( flow . randn ( 28 * 28 , 512 )) self . bias1 = nn . Parameter ( flow . randn ( 512 )) self . weight2 = nn . Parameter ( flow . randn ( 512 , 512 )) self . bias2 = nn . Parameter ( flow . randn ( 512 )) self . weight3 = nn . Parameter ( flow . randn ( 512 , 10 )) self . bias3 = nn . Parameter ( flow . randn ( 10 )) def forward ( self , x ): x = x . reshape ( 1 , 28 * 28 ) out = flow . matmul ( x , self . weight1 ) out = out + self . bias1 out = nn . functional . relu ( out ) out = flow . matmul ( out , self . weight2 ) out = out + self . bias2 out = nn . functional . relu ( out ) out = flow . matmul ( out , self . weight3 ) out = out + self . bias3 out = nn . functional . relu ( out ) return out net = FunctionalNeuralNetwork () X = flow . ones ( 1 , 28 , 28 ) logits = net ( X ) pred_probab = nn . Softmax ( dim = 1 )( logits ) y_pred = pred_probab . argmax ( 1 ) print ( f \"Predicted class: { y_pred } \" ) Module container \u00b6 Comparing the similarities and differences between the NeuralNetwork and FunctionalNeuralNetwork ,we can find that nn.Sequential plays an important role in simplifying the code. nn.Sequential is a special container. Any class inherited from nn.Module can be placed in it. Its specialty is that when Sequential propagates forward, Sequential automatically \"concatenates\" the layers contained in the container. Specifically, the output of the previous layer will be automatically transferred as the input of the next layer according to the sequence of Sequential added to each layer until the output of the last layer of the whole Moudle is obtained. The following is an example of building a network without Sequential (not recommended): class MyModel ( nn . Module ): def __init__ ( self ): super ( MyModel , self ) . __init__ () self . conv1 = nn . Conv2d ( 1 , 20 , 5 ) self . relu1 = nn . ReLU () self . conv2 = nn . Conv2d ( 20 , 64 , 5 ) self . relu2 = nn . ReLU () def forward ( self , x ): out = self . conv1 ( x ) out = self . relu1 ( out ) out = self . conv2 ( out ) out = self . relu2 ( out ) return out If sequential is used, it looks like this, which will be more concise. class MySeqModel ( nn . Module ): def __init__ ( self ): super ( MySeqModel , self ) . __init__ () self . seq = nn . Sequential ( nn . Conv2d ( 1 , 20 , 5 ), nn . ReLU (), nn . Conv2d ( 20 , 64 , 5 ), nn . ReLU () ) def forward ( self , x ): return self . seq ( x ) Besides Sequential, there are nn.ModuleList and nn.ModuleDict . They can automatically register parameters to the whole network. But their other behavior is similar to Python list and Python dict, which are just simple containers and do not automatically propagate forward. You need manually traverse to complete the calculation of each layer.","title":"Build Neural Network"},{"location":"basics/04_build_network.html#build-neural-network","text":"The layers of a neural network can be built by API in namespace oneflow.nn , It provides common Module (such as oneflow.nn.Conv2d , oneflow.nn.ReLU ). All Module classes inherit from oneflow.nn.Module , and many simple Module can form more complex Module. In this way, users can easily build and manage complex neural networks. import oneflow as flow import oneflow.nn as nn","title":"BUILD NEURAL NETWORK"},{"location":"basics/04_build_network.html#define-module-class","text":"oneflow.nn provides common Module classes and we can use them easily. Or we can build a neural network by customizing the Module class on the basis. This method consists of three parts\uff1a Write a class that inherits from oneflow.nn.Module class Write the method of __init__ class, in which we construct the network structure Write the method of forward class, which calculates on the basis of the input of Module class NeuralNetwork ( nn . Module ): def __init__ ( self ): super ( NeuralNetwork , self ) . __init__ () self . flatten = nn . Flatten () self . linear_relu_stack = nn . Sequential ( nn . Linear ( 28 * 28 , 512 ), nn . ReLU (), nn . Linear ( 512 , 512 ), nn . ReLU (), nn . Linear ( 512 , 10 ), nn . ReLU () ) def forward ( self , x ): x = self . flatten ( x ) logits = self . linear_relu_stack ( x ) return logits net = NeuralNetwork () print ( net ) The above code will output the structure of the NeuralNetwork network\uff1a NeuralNetwork( (flatten): Flatten(start_dim=1, end_dim=-1) (linear_relu_stack): Sequential( (0): Linear(in_features=784, out_features=512, bias=True) (1): ReLU() (2): Linear(in_features=512, out_features=512, bias=True) (3): ReLU() (4): Linear(in_features=512, out_features=10, bias=True) (5): ReLU() ) ) Then, call net (notice\uff1aIt is not recommended to explicitly call forward ): X = flow . ones ( 1 , 28 , 28 ) logits = net ( X ) pred_probab = nn . Softmax ( dim = 1 )( logits ) y_pred = pred_probab . argmax ( 1 ) print ( f \"Predicted class: { y_pred } \" ) You will get output similar to the following: Predicted class: tensor([1], dtype=oneflow.int32) The above process of data input, network calculation and the output of reasoning is shown in the figure below.","title":"Define Module class"},{"location":"basics/04_build_network.html#flownnfunctional","text":"In addition to oneflow.nn , oneflow.nn.functional namespace also provides many API. It overlaps with oneflow.nn to some extent. For example, nn.functional.relu and nn.ReLU both can be used for activation in neural network. The main differences between them are: The API under nn is a class. It needs to be instantiated before being called; The API under nn.functional is a function. It is called directly. The API under nn manages network parameters automatically\uff1bBut for the function under NN. Functional , we need to define our own parameters and manually pass them in each call. In fact, most of the Module provided by OneFlow is the result of encapsulating the methods under nn.functional . nn.functional can manage the network more finely. The following example uses the methods in nn.functional to build a Module FunctionalNeuralNetwork equivalent to the NeuralNetwork class above. Readers can appreciate the similarities and differences between the two: class FunctionalNeuralNetwork ( nn . Module ): def __init__ ( self ): super ( FunctionalNeuralNetwork , self ) . __init__ () self . weight1 = nn . Parameter ( flow . randn ( 28 * 28 , 512 )) self . bias1 = nn . Parameter ( flow . randn ( 512 )) self . weight2 = nn . Parameter ( flow . randn ( 512 , 512 )) self . bias2 = nn . Parameter ( flow . randn ( 512 )) self . weight3 = nn . Parameter ( flow . randn ( 512 , 10 )) self . bias3 = nn . Parameter ( flow . randn ( 10 )) def forward ( self , x ): x = x . reshape ( 1 , 28 * 28 ) out = flow . matmul ( x , self . weight1 ) out = out + self . bias1 out = nn . functional . relu ( out ) out = flow . matmul ( out , self . weight2 ) out = out + self . bias2 out = nn . functional . relu ( out ) out = flow . matmul ( out , self . weight3 ) out = out + self . bias3 out = nn . functional . relu ( out ) return out net = FunctionalNeuralNetwork () X = flow . ones ( 1 , 28 , 28 ) logits = net ( X ) pred_probab = nn . Softmax ( dim = 1 )( logits ) y_pred = pred_probab . argmax ( 1 ) print ( f \"Predicted class: { y_pred } \" )","title":"flow.nn.functional"},{"location":"basics/04_build_network.html#module-container","text":"Comparing the similarities and differences between the NeuralNetwork and FunctionalNeuralNetwork ,we can find that nn.Sequential plays an important role in simplifying the code. nn.Sequential is a special container. Any class inherited from nn.Module can be placed in it. Its specialty is that when Sequential propagates forward, Sequential automatically \"concatenates\" the layers contained in the container. Specifically, the output of the previous layer will be automatically transferred as the input of the next layer according to the sequence of Sequential added to each layer until the output of the last layer of the whole Moudle is obtained. The following is an example of building a network without Sequential (not recommended): class MyModel ( nn . Module ): def __init__ ( self ): super ( MyModel , self ) . __init__ () self . conv1 = nn . Conv2d ( 1 , 20 , 5 ) self . relu1 = nn . ReLU () self . conv2 = nn . Conv2d ( 20 , 64 , 5 ) self . relu2 = nn . ReLU () def forward ( self , x ): out = self . conv1 ( x ) out = self . relu1 ( out ) out = self . conv2 ( out ) out = self . relu2 ( out ) return out If sequential is used, it looks like this, which will be more concise. class MySeqModel ( nn . Module ): def __init__ ( self ): super ( MySeqModel , self ) . __init__ () self . seq = nn . Sequential ( nn . Conv2d ( 1 , 20 , 5 ), nn . ReLU (), nn . Conv2d ( 20 , 64 , 5 ), nn . ReLU () ) def forward ( self , x ): return self . seq ( x ) Besides Sequential, there are nn.ModuleList and nn.ModuleDict . They can automatically register parameters to the whole network. But their other behavior is similar to Python list and Python dict, which are just simple containers and do not automatically propagate forward. You need manually traverse to complete the calculation of each layer.","title":"Module container"},{"location":"basics/05_autograd.html","text":"AUTOGRAD \u00b6 The training process of a neural network is powered by backpropagation algorithm . In the backpropagation process, we update the parameters by obtaining the gradient of the loss function with respect to the parameters. OneFlow provides an autograd engine, which can calculate the gradient of the parameters in the neural network automatically. We will first introduce the basic concepts of the computation graph, which are conducive to understand the common settings and limitations of Oneflow's automatic differentiation. Then we will introduce OneFlow's common automatic differentiation interfaces. Computation Graph \u00b6 Computation graphs are composed of tensors and operators. We show this in code as below: import oneflow as flow def loss ( y_pred , y ): return flow . sum ( 1 / 2 * ( y_pred - y ) ** 2 ) x = flow . ones ( 1 , 5 ) # input\u3010\u4e0d\u786e\u5b9a\u3011 w = flow . randn ( 5 , 3 , requires_grad = True ) b = flow . randn ( 1 , 3 , requires_grad = True ) z = flow . matmul ( x , w ) + b y = flow . zeros ( 1 , 3 ) # label l = loss ( z , y ) Corresponding computation graph\uff1a In computation graph, the nodes only with output and with no input called leaf node, like x , w , b , and y , the nodes only with output and with no input called root node, like loss . During the backpropagation process, the gradient of l to w and b is required to update w and b . Therefore, we need to set requires_grad as True when creating them. Automatic Gradient \u00b6 backward() and Gradient \u00b6 During the backpropagation process, we need to get the gradients of l to w , b respectively, shown as \\(\\frac{\\partial l}{\\partial w}\\) and \\(\\frac{\\partial l}{\\partial b}\\) . We only need to call the 'backward()' method of l , and then OneFlow will automatically calculate the gradients and store them in the w.grad and b.grad . l . backward () print ( w . grad ) print ( b . grad ) tensor([[0.9397, 2.5428, 2.5377], [0.9397, 2.5428, 2.5377], [0.9397, 2.5428, 2.5377], [0.9397, 2.5428, 2.5377], [0.9397, 2.5428, 2.5377]], dtype=oneflow.float32) tensor([[0.9397, 2.5428, 2.5377]], dtype=oneflow.float32) Gradient for Non-leaf Nodes \u00b6 By default, only gradients of leaf nodes with requires_grad=True will be retained. The 'grad' of a non-leaf node is automatically freed during the calling of 'backward' and cannot be viewed. Tensor.retain_grad() can be called to retain and view the 'grad' of a non-leaf node. from math import pi n1 = flow . tensor ( pi / 2 , requires_grad = True ) n2 = flow . sin ( n1 ) n2 . retain_grad () n3 = flow . pow ( n2 , 2 ) n3 . backward () print ( n1 . grad ) print ( n2 . grad ) we get \\(\\frac{\\partial n_3}{\\partial n_1}\\) and \\(\\frac{\\partial n_3}{\\partial n_2}\\) using the code above. Output: tensor(-8.7423e-08, dtype=oneflow.float32) tensor(2., dtype=oneflow.float32) Call backward() Multiple Times on a Computation Graph \u00b6 By default, we can only call backward() once for each computation graph. For example, the following code will raise an error: n1 = flow . tensor ( 10. , requires_grad = True ) n2 = flow . pow ( n1 , 2 ) n2 . backward () n2 . backward () Error message: Maybe you try to backward through the node a second time. Specify retain_graph=True when calling .backward() or autograd.grad() the first time. If we need to call backward() multiple times on the same computation graph, retain_graph needs to be True . n1 = flow . tensor ( 10. , requires_grad = True ) n2 = flow . pow ( n1 , 2 ) n2 . backward ( retain_graph = True ) print ( n1 . grad ) n2 . backward () print ( n1 . grad ) Output\uff1a tensor(20., dtype=oneflow.float32) tensor(40., dtype=oneflow.float32) The above output shows that OneFlow will accumulate the gradient calculated by backward() multiple times. By calling the zero_() , we can clear the gradient: n1 = flow . tensor ( 10. , requires_grad = True ) n2 = flow . pow ( n1 , 2 ) n2 . backward ( retain_graph = True ) print ( n1 . grad ) n1 . grad . zero_ () n2 . backward () print ( n1 . grad ) Output\uff1a tensor(20., dtype=oneflow.float32) tensor(20., dtype=oneflow.float32) Disabled Gradient Calculation \u00b6 By default, OneFlow will trace and calculate gradients of Tensors with requires_grad = Ture . However, in some cases, we don't need OneFlow to keep tracing gradients such as just wanting the forward pass for inference. Then we can use oneflow.no_grad or oneflow.Tensor.detach to set. z = flow . matmul ( x , w ) + b print ( z . requires_grad ) with flow . no_grad (): z = flow . matmul ( x , w ) + b print ( z . requires_grad ) Output\uff1a True False z_det = z . detach () print ( z_det . requires_grad ) Output\uff1a False Gradients for Non-Scalar Outputs \u00b6 Usually, we call backward() on scalar loss . However, if loss is a tensor, an error will be raised when calling backward() on loss . x = flow . randn ( 1 , 2 , requires_grad = True ) y = 3 * x + 1 y . backward () Error message\uff1a Check failed: IsScalarTensor(*outputs.at(i)) Grad can be implicitly created only for scalar outputs We can get the gradient after y.sum() . x = flow . randn ( 1 , 2 , requires_grad = True ) y = 3 * x + 1 y = y . sum () y . backward () print ( x . grad ) Output\uff1a tensor([[3., 3.]], dtype=oneflow.float32) Please refer to the \"Further Reading\" section below for the analysis of the cause and solution of the error. Further Reading \u00b6 There are two elements \\(x_1\\) and \\(x_2\\) in Tensor x , and two elements \\(y_1\\) and \\(y_2\\) in Tensor y . The relationship between them is: \\[ \\mathbf{x} = [x_1, x_2] \\] \\[ \\mathbf{y} = [y_1, y_2] = [3x_1+1, 3x_2+1] \\] We want to get \\(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}\\) \\[ \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} = \\frac{[3x_1+1, 3x_2+1]}{[x_1, x_2]} \\] It doesn't make sense in mathematics, so of course an error is reported. In fact, when the user calls y.backward() , the result desired is usually: \\[ [\\frac{\\partial y_1}{\\partial x_1}, \\frac{\\partial y_2}{\\partial x_2}] \\] After call sum() on y : \\[ y = y_1 + y_2 = 3x_1 + 3x_2 + 2 \\] At this time, when calling backward() , the gradients of \\(x_1\\) and \\(x_2\\) can be calculated: \\[ \\frac{\\partial y}{\\partial x_1} = \\frac{\\partial 3x_1 + 3x_2 + 2}{\\partial x_1} = 3 \\] \\[ \\frac{\\partial y}{\\partial x_2} = \\frac{\\partial 3x_1 + 3x_2 + 2}{\\partial x_2} = 3 \\] In addition to using sum() , Vector Jacobian Product(VJP) is a more general method to calculate the gradient of the non-scalar root node. Using the above example, OneFlow will generate the Jacobian matrix according to the computation graph during the backpropagation process: \\[ J = \\begin{pmatrix} \\frac{\\partial y_1}{\\partial x_1} & \\frac{\\partial y_1}{\\partial x_2}\\\\ \\frac{\\partial y_2}{\\partial x_1} & \\frac{\\partial y_2}{\\partial x_2} \\end{pmatrix}\\\\ = \\begin{pmatrix} \\frac{\\partial y_1}{\\partial x_1} & 0 \\\\ 0 & \\frac{\\partial y_2}{\\partial x_2} \\end{pmatrix} \\] To calculate VJP, a vector \\(\\mathbf{v}\\) with the same size as \\(\\mathbf{y}\\) needs to be provided: \\[ \\begin{bmatrix} v_1\\\\ v_2 \\end{bmatrix} \\times \\begin{pmatrix} \\frac{\\partial y_1}{\\partial x_1} & 0 \\\\ 0 & \\frac{\\partial y_2}{\\partial x_2} \\end{pmatrix}= \\begin{bmatrix} v_1 \\frac{\\partial y_1}{\\partial x_1}\\\\ v_2 \\frac{\\partial y_2}{\\partial x_2} \\end{bmatrix} \\] If the vector \\(\\mathbf{v}\\) is the gradient of the upper layer in the backpropagation, the result of VJP is exactly the gradient required by the current layer. backward() can accept a tensor as a parameter, when the parameter is \\(\\mathbf{v}\\) in VJP. We can also use the following methods to find the gradient of a tensor: x = flow . randn ( 1 , 2 , requires_grad = True ) y = 3 * x + 1 y . backward ( flow . ones_like ( y )) print ( x . grad ) Output\uff1a tensor([[3., 3.]], dtype=oneflow.float32) External links Automatic Differentiation","title":"Autograd"},{"location":"basics/05_autograd.html#autograd","text":"The training process of a neural network is powered by backpropagation algorithm . In the backpropagation process, we update the parameters by obtaining the gradient of the loss function with respect to the parameters. OneFlow provides an autograd engine, which can calculate the gradient of the parameters in the neural network automatically. We will first introduce the basic concepts of the computation graph, which are conducive to understand the common settings and limitations of Oneflow's automatic differentiation. Then we will introduce OneFlow's common automatic differentiation interfaces.","title":"AUTOGRAD"},{"location":"basics/05_autograd.html#computation-graph","text":"Computation graphs are composed of tensors and operators. We show this in code as below: import oneflow as flow def loss ( y_pred , y ): return flow . sum ( 1 / 2 * ( y_pred - y ) ** 2 ) x = flow . ones ( 1 , 5 ) # input\u3010\u4e0d\u786e\u5b9a\u3011 w = flow . randn ( 5 , 3 , requires_grad = True ) b = flow . randn ( 1 , 3 , requires_grad = True ) z = flow . matmul ( x , w ) + b y = flow . zeros ( 1 , 3 ) # label l = loss ( z , y ) Corresponding computation graph\uff1a In computation graph, the nodes only with output and with no input called leaf node, like x , w , b , and y , the nodes only with output and with no input called root node, like loss . During the backpropagation process, the gradient of l to w and b is required to update w and b . Therefore, we need to set requires_grad as True when creating them.","title":"Computation Graph"},{"location":"basics/05_autograd.html#automatic-gradient","text":"","title":"Automatic Gradient"},{"location":"basics/05_autograd.html#backward-and-gradient","text":"During the backpropagation process, we need to get the gradients of l to w , b respectively, shown as \\(\\frac{\\partial l}{\\partial w}\\) and \\(\\frac{\\partial l}{\\partial b}\\) . We only need to call the 'backward()' method of l , and then OneFlow will automatically calculate the gradients and store them in the w.grad and b.grad . l . backward () print ( w . grad ) print ( b . grad ) tensor([[0.9397, 2.5428, 2.5377], [0.9397, 2.5428, 2.5377], [0.9397, 2.5428, 2.5377], [0.9397, 2.5428, 2.5377], [0.9397, 2.5428, 2.5377]], dtype=oneflow.float32) tensor([[0.9397, 2.5428, 2.5377]], dtype=oneflow.float32)","title":"backward() and Gradient"},{"location":"basics/05_autograd.html#gradient-for-non-leaf-nodes","text":"By default, only gradients of leaf nodes with requires_grad=True will be retained. The 'grad' of a non-leaf node is automatically freed during the calling of 'backward' and cannot be viewed. Tensor.retain_grad() can be called to retain and view the 'grad' of a non-leaf node. from math import pi n1 = flow . tensor ( pi / 2 , requires_grad = True ) n2 = flow . sin ( n1 ) n2 . retain_grad () n3 = flow . pow ( n2 , 2 ) n3 . backward () print ( n1 . grad ) print ( n2 . grad ) we get \\(\\frac{\\partial n_3}{\\partial n_1}\\) and \\(\\frac{\\partial n_3}{\\partial n_2}\\) using the code above. Output: tensor(-8.7423e-08, dtype=oneflow.float32) tensor(2., dtype=oneflow.float32)","title":"Gradient for Non-leaf Nodes"},{"location":"basics/05_autograd.html#call-backward-multiple-times-on-a-computation-graph","text":"By default, we can only call backward() once for each computation graph. For example, the following code will raise an error: n1 = flow . tensor ( 10. , requires_grad = True ) n2 = flow . pow ( n1 , 2 ) n2 . backward () n2 . backward () Error message: Maybe you try to backward through the node a second time. Specify retain_graph=True when calling .backward() or autograd.grad() the first time. If we need to call backward() multiple times on the same computation graph, retain_graph needs to be True . n1 = flow . tensor ( 10. , requires_grad = True ) n2 = flow . pow ( n1 , 2 ) n2 . backward ( retain_graph = True ) print ( n1 . grad ) n2 . backward () print ( n1 . grad ) Output\uff1a tensor(20., dtype=oneflow.float32) tensor(40., dtype=oneflow.float32) The above output shows that OneFlow will accumulate the gradient calculated by backward() multiple times. By calling the zero_() , we can clear the gradient: n1 = flow . tensor ( 10. , requires_grad = True ) n2 = flow . pow ( n1 , 2 ) n2 . backward ( retain_graph = True ) print ( n1 . grad ) n1 . grad . zero_ () n2 . backward () print ( n1 . grad ) Output\uff1a tensor(20., dtype=oneflow.float32) tensor(20., dtype=oneflow.float32)","title":"Call backward() Multiple Times on a Computation Graph"},{"location":"basics/05_autograd.html#disabled-gradient-calculation","text":"By default, OneFlow will trace and calculate gradients of Tensors with requires_grad = Ture . However, in some cases, we don't need OneFlow to keep tracing gradients such as just wanting the forward pass for inference. Then we can use oneflow.no_grad or oneflow.Tensor.detach to set. z = flow . matmul ( x , w ) + b print ( z . requires_grad ) with flow . no_grad (): z = flow . matmul ( x , w ) + b print ( z . requires_grad ) Output\uff1a True False z_det = z . detach () print ( z_det . requires_grad ) Output\uff1a False","title":"Disabled Gradient Calculation"},{"location":"basics/05_autograd.html#gradients-for-non-scalar-outputs","text":"Usually, we call backward() on scalar loss . However, if loss is a tensor, an error will be raised when calling backward() on loss . x = flow . randn ( 1 , 2 , requires_grad = True ) y = 3 * x + 1 y . backward () Error message\uff1a Check failed: IsScalarTensor(*outputs.at(i)) Grad can be implicitly created only for scalar outputs We can get the gradient after y.sum() . x = flow . randn ( 1 , 2 , requires_grad = True ) y = 3 * x + 1 y = y . sum () y . backward () print ( x . grad ) Output\uff1a tensor([[3., 3.]], dtype=oneflow.float32) Please refer to the \"Further Reading\" section below for the analysis of the cause and solution of the error.","title":"Gradients for Non-Scalar Outputs"},{"location":"basics/05_autograd.html#further-reading","text":"There are two elements \\(x_1\\) and \\(x_2\\) in Tensor x , and two elements \\(y_1\\) and \\(y_2\\) in Tensor y . The relationship between them is: \\[ \\mathbf{x} = [x_1, x_2] \\] \\[ \\mathbf{y} = [y_1, y_2] = [3x_1+1, 3x_2+1] \\] We want to get \\(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}\\) \\[ \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} = \\frac{[3x_1+1, 3x_2+1]}{[x_1, x_2]} \\] It doesn't make sense in mathematics, so of course an error is reported. In fact, when the user calls y.backward() , the result desired is usually: \\[ [\\frac{\\partial y_1}{\\partial x_1}, \\frac{\\partial y_2}{\\partial x_2}] \\] After call sum() on y : \\[ y = y_1 + y_2 = 3x_1 + 3x_2 + 2 \\] At this time, when calling backward() , the gradients of \\(x_1\\) and \\(x_2\\) can be calculated: \\[ \\frac{\\partial y}{\\partial x_1} = \\frac{\\partial 3x_1 + 3x_2 + 2}{\\partial x_1} = 3 \\] \\[ \\frac{\\partial y}{\\partial x_2} = \\frac{\\partial 3x_1 + 3x_2 + 2}{\\partial x_2} = 3 \\] In addition to using sum() , Vector Jacobian Product(VJP) is a more general method to calculate the gradient of the non-scalar root node. Using the above example, OneFlow will generate the Jacobian matrix according to the computation graph during the backpropagation process: \\[ J = \\begin{pmatrix} \\frac{\\partial y_1}{\\partial x_1} & \\frac{\\partial y_1}{\\partial x_2}\\\\ \\frac{\\partial y_2}{\\partial x_1} & \\frac{\\partial y_2}{\\partial x_2} \\end{pmatrix}\\\\ = \\begin{pmatrix} \\frac{\\partial y_1}{\\partial x_1} & 0 \\\\ 0 & \\frac{\\partial y_2}{\\partial x_2} \\end{pmatrix} \\] To calculate VJP, a vector \\(\\mathbf{v}\\) with the same size as \\(\\mathbf{y}\\) needs to be provided: \\[ \\begin{bmatrix} v_1\\\\ v_2 \\end{bmatrix} \\times \\begin{pmatrix} \\frac{\\partial y_1}{\\partial x_1} & 0 \\\\ 0 & \\frac{\\partial y_2}{\\partial x_2} \\end{pmatrix}= \\begin{bmatrix} v_1 \\frac{\\partial y_1}{\\partial x_1}\\\\ v_2 \\frac{\\partial y_2}{\\partial x_2} \\end{bmatrix} \\] If the vector \\(\\mathbf{v}\\) is the gradient of the upper layer in the backpropagation, the result of VJP is exactly the gradient required by the current layer. backward() can accept a tensor as a parameter, when the parameter is \\(\\mathbf{v}\\) in VJP. We can also use the following methods to find the gradient of a tensor: x = flow . randn ( 1 , 2 , requires_grad = True ) y = 3 * x + 1 y . backward ( flow . ones_like ( y )) print ( x . grad ) Output\uff1a tensor([[3., 3.]], dtype=oneflow.float32) External links Automatic Differentiation","title":"Further Reading"},{"location":"basics/06_optimization.html","text":"BACKPROPAGATION AND OPTIMIZER \u00b6 So far, we have learned how to use OneFlow to Dataset and DataLoader , Build Models , Autograd , and combine them so that we can train models by using backpropagation algorithms. In oneflow.optim , there are various optimizer s that simplify the code of back propagation. This article will first introduce the basic concepts of back propagation and then show you how to use the oneflow.optim class. Backpropagation by Numpy Code \u00b6 In order to make it easier for readers to understand the relationship between backpropagation and autograd, a training process of a simple model implemented with numpy is provided here: import numpy as np ITER_COUNT = 500 LR = 0.01 # Forward propagation def forward ( x , w ): return np . matmul ( x , w ) # Loss function def loss ( y_pred , y ): return (( y_pred - y ) ** 2 ) . sum () # Calculate gradient def gradient ( x , y , y_pred ): return np . matmul ( x . T , 2 * ( y_pred - y )) if __name__ == \"__main__\" : # Train: Y = 2*X1 + 3*X2 x = np . array ([[ 1 , 2 ], [ 2 , 3 ], [ 4 , 6 ], [ 3 , 1 ]], dtype = np . float32 ) y = np . array ([[ 8 ], [ 13 ], [ 26 ], [ 9 ]], dtype = np . float32 ) w = np . array ([[ 2 ], [ 1 ]], dtype = np . float32 ) # Training cycle for i in range ( 0 , ITER_COUNT ): y_pred = forward ( x , w ) l = loss ( y_pred , y ) if ( i + 1 ) % 50 == 0 : print ( f \" { i + 1 } / { 500 } loss: { l } \" ) grad = gradient ( x , y , y_pred ) w -= LR * grad print ( f \"w: { w } \" ) output\uff1a 50/500 loss:0.0034512376878410578 100/500 loss:1.965487399502308e-06 150/500 loss:1.05524122773204e-09 200/500 loss:3.865352482534945e-12 250/500 loss:3.865352482534945e-12 300/500 loss:3.865352482534945e-12 350/500 loss:3.865352482534945e-12 400/500 loss:3.865352482534945e-12 450/500 loss:3.865352482534945e-12 500/500 loss:3.865352482534945e-12 w:[[2.000001 ] [2.9999993]] Note that the loss function expression we selected is \\(\\sum (y_{p} - y)^2\\) , so the code for gradient of loss to parameter w is\uff1a def gradient ( x , y , y_pred ): return np . matmul ( x . T , 2 * ( y_pred - y )) SGD is used to update parameters\uff1a grad = gradient ( x , y , y_pred ) w -= LR * grad In summary, a complete iteration in the training includes the following steps: The model calculates the predicted value based on the input and parameters ( y_pred ) Calculate loss, which is the error between the predicted value and the label Calculate the gradient of loss to parameter Update parameter(s) 1 and 2 are forward propagation process; 3 and 4 are back propagation process. Hyperparameters \u00b6 Hyperparameters are parameters related to model training settings, which can affect the efficiency and results of model training.As in the above code ITER_COUNT , LR are hyperparameters. Using the optimizer class in oneflow.optim \u00b6 Using the optimizer class in oneflow.optim for back propagation will be more concise. First, prepare the data and model. The convenience of using Module is that you can place the hyperparameters in Module for management. import oneflow as flow x = flow . tensor ([[ 1 , 2 ], [ 2 , 3 ], [ 4 , 6 ], [ 3 , 1 ]], dtype = flow . float32 ) y = flow . tensor ([[ 8 ], [ 13 ], [ 26 ], [ 9 ]], dtype = flow . float32 ) class MyLrModule ( flow . nn . Module ): def __init__ ( self , lr , iter_count ): super () . __init__ () self . w = flow . nn . Parameter ( flow . tensor ([[ 2 ], [ 1 ]], dtype = flow . float32 )) self . lr = lr self . iter_count = iter_count def forward ( self , x ): return flow . matmul ( x , self . w ) model = MyLrModule ( 0.01 , 500 ) Loss function \u00b6 Then, select the loss function. OneFlow comes with a variety of loss functions. We choose MSELoss here\uff1a loss = flow . nn . MSELoss ( reduction = \"sum\" ) Construct Optimizer \u00b6 The logic of back propagation is wrapped in optimizer. We choose SGD here, You can choose other optimization algorithms as needed, such as Adam and AdamW . optimizer = flow . optim . SGD ( model . parameters (), model . lr ) When the optimizer is constructed, the model parameters and learning rate are given to SGD . Then the optimizer.step() is called, and it automatically completes the gradient of the model parameters and updates the model parameters according to the SGD algorithm. Train \u00b6 When the above preparations are completed, we can start training: for i in range ( 0 , model . iter_count ): y_pred = model ( x ) l = loss ( y_pred , y ) if ( i + 1 ) % 50 == 0 : print ( f \" { i + 1 } / { model . iter_count } loss: { l . numpy () } \" ) optimizer . zero_grad () l . backward () optimizer . step () print ( f \" \\n w: { model . w } \" ) output\uff1a 50/500 loss:0.003451163647696376 100/500 loss:1.965773662959691e-06 150/500 loss:1.103217073250562e-09 200/500 loss:3.865352482534945e-12 250/500 loss:3.865352482534945e-12 300/500 loss:3.865352482534945e-12 350/500 loss:3.865352482534945e-12 400/500 loss:3.865352482534945e-12 450/500 loss:3.865352482534945e-12 500/500 loss:3.865352482534945e-12 w: tensor([[2.], [3.]], dtype=oneflow.float32, grad_fn=<accumulate_grad>)","title":"Backpropagation and Optimizer"},{"location":"basics/06_optimization.html#backpropagation-and-optimizer","text":"So far, we have learned how to use OneFlow to Dataset and DataLoader , Build Models , Autograd , and combine them so that we can train models by using backpropagation algorithms. In oneflow.optim , there are various optimizer s that simplify the code of back propagation. This article will first introduce the basic concepts of back propagation and then show you how to use the oneflow.optim class.","title":"BACKPROPAGATION AND OPTIMIZER"},{"location":"basics/06_optimization.html#backpropagation-by-numpy-code","text":"In order to make it easier for readers to understand the relationship between backpropagation and autograd, a training process of a simple model implemented with numpy is provided here: import numpy as np ITER_COUNT = 500 LR = 0.01 # Forward propagation def forward ( x , w ): return np . matmul ( x , w ) # Loss function def loss ( y_pred , y ): return (( y_pred - y ) ** 2 ) . sum () # Calculate gradient def gradient ( x , y , y_pred ): return np . matmul ( x . T , 2 * ( y_pred - y )) if __name__ == \"__main__\" : # Train: Y = 2*X1 + 3*X2 x = np . array ([[ 1 , 2 ], [ 2 , 3 ], [ 4 , 6 ], [ 3 , 1 ]], dtype = np . float32 ) y = np . array ([[ 8 ], [ 13 ], [ 26 ], [ 9 ]], dtype = np . float32 ) w = np . array ([[ 2 ], [ 1 ]], dtype = np . float32 ) # Training cycle for i in range ( 0 , ITER_COUNT ): y_pred = forward ( x , w ) l = loss ( y_pred , y ) if ( i + 1 ) % 50 == 0 : print ( f \" { i + 1 } / { 500 } loss: { l } \" ) grad = gradient ( x , y , y_pred ) w -= LR * grad print ( f \"w: { w } \" ) output\uff1a 50/500 loss:0.0034512376878410578 100/500 loss:1.965487399502308e-06 150/500 loss:1.05524122773204e-09 200/500 loss:3.865352482534945e-12 250/500 loss:3.865352482534945e-12 300/500 loss:3.865352482534945e-12 350/500 loss:3.865352482534945e-12 400/500 loss:3.865352482534945e-12 450/500 loss:3.865352482534945e-12 500/500 loss:3.865352482534945e-12 w:[[2.000001 ] [2.9999993]] Note that the loss function expression we selected is \\(\\sum (y_{p} - y)^2\\) , so the code for gradient of loss to parameter w is\uff1a def gradient ( x , y , y_pred ): return np . matmul ( x . T , 2 * ( y_pred - y )) SGD is used to update parameters\uff1a grad = gradient ( x , y , y_pred ) w -= LR * grad In summary, a complete iteration in the training includes the following steps: The model calculates the predicted value based on the input and parameters ( y_pred ) Calculate loss, which is the error between the predicted value and the label Calculate the gradient of loss to parameter Update parameter(s) 1 and 2 are forward propagation process; 3 and 4 are back propagation process.","title":"Backpropagation by Numpy Code"},{"location":"basics/06_optimization.html#hyperparameters","text":"Hyperparameters are parameters related to model training settings, which can affect the efficiency and results of model training.As in the above code ITER_COUNT , LR are hyperparameters.","title":"Hyperparameters"},{"location":"basics/06_optimization.html#using-the-optimizer-class-in-oneflowoptim","text":"Using the optimizer class in oneflow.optim for back propagation will be more concise. First, prepare the data and model. The convenience of using Module is that you can place the hyperparameters in Module for management. import oneflow as flow x = flow . tensor ([[ 1 , 2 ], [ 2 , 3 ], [ 4 , 6 ], [ 3 , 1 ]], dtype = flow . float32 ) y = flow . tensor ([[ 8 ], [ 13 ], [ 26 ], [ 9 ]], dtype = flow . float32 ) class MyLrModule ( flow . nn . Module ): def __init__ ( self , lr , iter_count ): super () . __init__ () self . w = flow . nn . Parameter ( flow . tensor ([[ 2 ], [ 1 ]], dtype = flow . float32 )) self . lr = lr self . iter_count = iter_count def forward ( self , x ): return flow . matmul ( x , self . w ) model = MyLrModule ( 0.01 , 500 )","title":"Using the optimizer class in oneflow.optim"},{"location":"basics/06_optimization.html#loss-function","text":"Then, select the loss function. OneFlow comes with a variety of loss functions. We choose MSELoss here\uff1a loss = flow . nn . MSELoss ( reduction = \"sum\" )","title":"Loss function"},{"location":"basics/06_optimization.html#construct-optimizer","text":"The logic of back propagation is wrapped in optimizer. We choose SGD here, You can choose other optimization algorithms as needed, such as Adam and AdamW . optimizer = flow . optim . SGD ( model . parameters (), model . lr ) When the optimizer is constructed, the model parameters and learning rate are given to SGD . Then the optimizer.step() is called, and it automatically completes the gradient of the model parameters and updates the model parameters according to the SGD algorithm.","title":"Construct Optimizer"},{"location":"basics/06_optimization.html#train","text":"When the above preparations are completed, we can start training: for i in range ( 0 , model . iter_count ): y_pred = model ( x ) l = loss ( y_pred , y ) if ( i + 1 ) % 50 == 0 : print ( f \" { i + 1 } / { model . iter_count } loss: { l . numpy () } \" ) optimizer . zero_grad () l . backward () optimizer . step () print ( f \" \\n w: { model . w } \" ) output\uff1a 50/500 loss:0.003451163647696376 100/500 loss:1.965773662959691e-06 150/500 loss:1.103217073250562e-09 200/500 loss:3.865352482534945e-12 250/500 loss:3.865352482534945e-12 300/500 loss:3.865352482534945e-12 350/500 loss:3.865352482534945e-12 400/500 loss:3.865352482534945e-12 450/500 loss:3.865352482534945e-12 500/500 loss:3.865352482534945e-12 w: tensor([[2.], [3.]], dtype=oneflow.float32, grad_fn=<accumulate_grad>)","title":"Train"},{"location":"basics/07_model_load_save.html","text":"SAVE AND LOAD THE MODEL \u00b6 There are two common uses for loading and saving models: Save the model that has been trained to continue training next time. Save the trained model for direct prediction in the future. We will introduce how to use save and load to save and load models as follows. Also, we will show how to load a pre-trained model for inference. Saving and Loading Model Parameters \u00b6 Module provided by OneFlow and defined by users provides the state_dict method to obtain all the model parameters, which is stored in a dictionary with the format \"name-value\". import oneflow as flow m = flow . nn . Linear ( 2 , 3 ) print ( m . state_dict ()) The above code first constructs a Linear object, then prints its parameters. OrderedDict([('weight', tensor([[-0.4297, -0.3571], [ 0.6797, -0.5295], [ 0.4918, -0.3039]], dtype=oneflow.float32, requires_grad=True)), ('bias', tensor([ 0.0977, 0.1219, -0.5372], dtype=oneflow.float32, requires_grad=True))]) We can load parameters by calling load_state_dict method of Module , as the following code: myparams = { \"weight\" : flow . ones ( 3 , 2 ), \"bias\" : flow . zeros ( 3 )} m . load_state_dict ( myparams ) print ( m . state_dict ()) The tensor in the dictionary created by us has been loaded into m Module: OrderedDict([('weight', tensor([[1., 1.], [1., 1.], [1., 1.]], dtype=oneflow.float32, requires_grad=True)), ('bias', tensor([0., 0., 0.], dtype=oneflow.float32, requires_grad=True))]) Saving Models \u00b6 We can use oneflow.save to save models. flow . save ( m . state_dict (), \"./model\" ) The first parameter is the Module parameters, and the second is the saved path. The above code saves the parameters of the m Module object to the path ./model . Loading Models \u00b6 Using oneflow.load to load parameters from disk to memory with the specified path, and get the dictionary of the parameters. params = flow . load ( \"./model\" ) Then use load_state_dict to load the dictionary into the model. m2 = flow . nn . Linear ( 2 , 3 ) m2 . load_state_dict ( params ) print ( m2 . state_dict ()) We have created a new Linear Module object m2 , and loaded the parameters saved from the above to m2 . Then we get the output as below: OrderedDict([('weight', tensor([[1., 1.], [1., 1.], [1., 1.]], dtype=oneflow.float32, requires_grad=True)), ('bias', tensor([0., 0., 0.], dtype=oneflow.float32, requires_grad=True))]) Using a Pre-trained Model to Make Predictions \u00b6 OneFlow can directly load PyTorch's pre-trained model for prediction as long as the structure and parameter names of the model are aligned with the PyTorch model. Examples can be found in here . Run commands below for trying how to use the pre-trained model to make predictions: git clone https://github.com/Oneflow-Inc/models.git cd models/Vision/classification/image/shufflenetv2/ bash infer.sh","title":"Model saving and loading"},{"location":"basics/07_model_load_save.html#save-and-load-the-model","text":"There are two common uses for loading and saving models: Save the model that has been trained to continue training next time. Save the trained model for direct prediction in the future. We will introduce how to use save and load to save and load models as follows. Also, we will show how to load a pre-trained model for inference.","title":"SAVE AND LOAD THE MODEL"},{"location":"basics/07_model_load_save.html#saving-and-loading-model-parameters","text":"Module provided by OneFlow and defined by users provides the state_dict method to obtain all the model parameters, which is stored in a dictionary with the format \"name-value\". import oneflow as flow m = flow . nn . Linear ( 2 , 3 ) print ( m . state_dict ()) The above code first constructs a Linear object, then prints its parameters. OrderedDict([('weight', tensor([[-0.4297, -0.3571], [ 0.6797, -0.5295], [ 0.4918, -0.3039]], dtype=oneflow.float32, requires_grad=True)), ('bias', tensor([ 0.0977, 0.1219, -0.5372], dtype=oneflow.float32, requires_grad=True))]) We can load parameters by calling load_state_dict method of Module , as the following code: myparams = { \"weight\" : flow . ones ( 3 , 2 ), \"bias\" : flow . zeros ( 3 )} m . load_state_dict ( myparams ) print ( m . state_dict ()) The tensor in the dictionary created by us has been loaded into m Module: OrderedDict([('weight', tensor([[1., 1.], [1., 1.], [1., 1.]], dtype=oneflow.float32, requires_grad=True)), ('bias', tensor([0., 0., 0.], dtype=oneflow.float32, requires_grad=True))])","title":"Saving and Loading Model Parameters"},{"location":"basics/07_model_load_save.html#saving-models","text":"We can use oneflow.save to save models. flow . save ( m . state_dict (), \"./model\" ) The first parameter is the Module parameters, and the second is the saved path. The above code saves the parameters of the m Module object to the path ./model .","title":"Saving Models"},{"location":"basics/07_model_load_save.html#loading-models","text":"Using oneflow.load to load parameters from disk to memory with the specified path, and get the dictionary of the parameters. params = flow . load ( \"./model\" ) Then use load_state_dict to load the dictionary into the model. m2 = flow . nn . Linear ( 2 , 3 ) m2 . load_state_dict ( params ) print ( m2 . state_dict ()) We have created a new Linear Module object m2 , and loaded the parameters saved from the above to m2 . Then we get the output as below: OrderedDict([('weight', tensor([[1., 1.], [1., 1.], [1., 1.]], dtype=oneflow.float32, requires_grad=True)), ('bias', tensor([0., 0., 0.], dtype=oneflow.float32, requires_grad=True))])","title":"Loading Models"},{"location":"basics/07_model_load_save.html#using-a-pre-trained-model-to-make-predictions","text":"OneFlow can directly load PyTorch's pre-trained model for prediction as long as the structure and parameter names of the model are aligned with the PyTorch model. Examples can be found in here . Run commands below for trying how to use the pre-trained model to make predictions: git clone https://github.com/Oneflow-Inc/models.git cd models/Vision/classification/image/shufflenetv2/ bash infer.sh","title":"Using a Pre-trained Model to Make Predictions"},{"location":"basics/08_nn_graph.html","text":"STATIC GRAPH INTERFACE: NN.GRAPH \u00b6 At present, there are two ways to run models in deep learning framework, Dynamic Graph and Static Graph , which are also called Eager Mode and Graph Mode in OneFlow. There are pros and cons to both approaches, and OneFlow offers support for both, with the Eager Mode by default. If you are reading the tutorials for this basic topic in order, then all the code you have encountered so far is in Eager Mode. In general, dynamic graphs are easier to use and static graphs have better performance. OneFlow offers nn.Graph , so that users can use the eager-like programming style to build static graphs and train the models. Eager Mode in OneFlow \u00b6 OneFlow runs in Eager Mode by default. The following script uses data set CIFAR10 to train model mobilenet_v2 . Code import oneflow as flow import oneflow.nn as nn import flowvision import flowvision.transforms as transforms BATCH_SIZE = 64 EPOCH_NUM = 1 DEVICE = \"cuda\" if flow . cuda . is_available () else \"cpu\" print ( \"Using {} device\" . format ( DEVICE )) training_data = flowvision . datasets . CIFAR10 ( root = \"data\" , train = True , transform = transforms . ToTensor (), download = True , source_url = \"https://oneflow-public.oss-cn-beijing.aliyuncs.com/datasets/cifar/cifar-10-python.tar.gz\" , ) train_dataloader = flow . utils . data . DataLoader ( training_data , BATCH_SIZE , shuffle = True ) model = flowvision . models . mobilenet_v2 () . to ( DEVICE ) model . classifer = nn . Sequential ( nn . Dropout ( 0.2 ), nn . Linear ( model . last_channel , 10 )) model . train () loss_fn = nn . CrossEntropyLoss () . to ( DEVICE ) optimizer = flow . optim . SGD ( model . parameters (), lr = 1e-3 ) for t in range ( EPOCH_NUM ): print ( f \"Epoch { t + 1 } \\n -------------------------------\" ) size = len ( train_dataloader . dataset ) for batch , ( x , y ) in enumerate ( train_dataloader ): x = x . to ( DEVICE ) y = y . to ( DEVICE ) # Compute prediction error pred = model ( x ) loss = loss_fn ( pred , y ) # Backpropagation optimizer . zero_grad () loss . backward () optimizer . step () current = batch * BATCH_SIZE if batch % 5 == 0 : print ( f \"loss: { loss : >7f } [ { current : >5d } / { size : >5d } ]\" ) Out: loss: 6.921304 [ 0/50000] loss: 6.824391 [ 320/50000] loss: 6.688272 [ 640/50000] loss: 6.644351 [ 960/50000] ... Graph Mode in OneFlow \u00b6 Customize a Graph \u00b6 OneFlow provide the base class nn.Graph , which can be inherited to create a customized Graph class. import oneflow as flow import oneflow.nn as nn class ModuleMyLinear ( nn . Module ): def __init__ ( self , in_features , out_features ): super () . __init__ () self . weight = nn . Parameter ( flow . randn ( in_features , out_features )) self . bias = nn . Parameter ( flow . randn ( out_features )) def forward ( self , input ): return flow . matmul ( input , self . weight ) + self . bias linear_model = ModuleMyLinear ( 4 , 3 ) class GraphMyLinear ( nn . Graph ): def __init__ ( self ): super () . __init__ () self . model = linear_model def build ( self , input ): return self . model ( input ) The simple example above contains the important steps needed to customize a Graph: Inherits nn.Graph . Call super().__init__() at the begining of __init__ method to get OneFlow to do the necessary initialization for the Graph. In __init__ , reuse the nn.Module object in Eager mode ( self.model = model ) Describes the computational process in build method. You can then instantiate and call the Graph: graph_mylinear = GraphMyLinear () input = flow . randn ( 1 , 4 ) out = graph_mylinear ( input ) print ( out ) Out: tensor([[-0.3298, -3.7907, 0.1661]], dtype=oneflow.float32) Note that Graph is similar to Module in that the object itself is callable and it is not recommended to explicitly call the build method. Graph can directly reuse a defined Module. Users can refer the content in Build Network to build a neural network. Then, set Module as a member of Graph in __init__ of Graph. For example, use the linear_model above as the network structure: class ModelGraph ( flow . nn . Graph ): def __init__ ( self ): super () . __init__ () self . model = linear_model def build ( self , x , y ): y_pred = self . model ( x ) return loss model_graph = ModelGraph () The major difference between Module and Graph is that Graph uses build method rather than forward method to describe the computation process, because the build method can contain not only forward computation, but also setting loss , optimizer, etc. You will see an example of using Graph for training later. Inference in Graph Mode \u00b6 The following example for inference in Graph Mode directly using the model, which we have already trained in Eager Mode at the beginning of this article. class GraphMobileNetV2 ( flow . nn . Graph ): def __init__ ( self ): super () . __init__ () self . model = model def build ( self , x ): return self . model ( x ) graph_mobile_net_v2 = GraphMobileNetV2 () x , _ = next ( iter ( train_dataloader )) x = x . to ( DEVICE ) y_pred = graph_mobile_net_v2 ( x ) Training in Graph Mode \u00b6 The Graph can be used for training. Click on the \"Code\" below to see the detailed code. Code import oneflow as flow import oneflow.nn as nn import flowvision import flowvision.transforms as transforms BATCH_SIZE = 64 EPOCH_NUM = 1 DEVICE = \"cuda\" if flow . cuda . is_available () else \"cpu\" print ( \"Using {} device\" . format ( DEVICE )) training_data = flowvision . datasets . CIFAR10 ( root = \"data\" , train = True , transform = transforms . ToTensor (), download = True , ) train_dataloader = flow . utils . data . DataLoader ( training_data , BATCH_SIZE , shuffle = True , drop_last = True ) model = flowvision . models . mobilenet_v2 () . to ( DEVICE ) model . classifer = nn . Sequential ( nn . Dropout ( 0.2 ), nn . Linear ( model . last_channel , 10 )) model . train () loss_fn = nn . CrossEntropyLoss () . to ( DEVICE ) optimizer = flow . optim . SGD ( model . parameters (), lr = 1e-3 ) class GraphMobileNetV2 ( flow . nn . Graph ): def __init__ ( self ): super () . __init__ () self . model = model self . loss_fn = loss_fn self . add_optimizer ( optimizer ) def build ( self , x , y ): y_pred = self . model ( x ) loss = self . loss_fn ( y_pred , y ) loss . backward () return loss graph_mobile_net_v2 = GraphMobileNetV2 () # graph_mobile_net_v2.debug() for t in range ( EPOCH_NUM ): print ( f \"Epoch { t + 1 } \\n -------------------------------\" ) size = len ( train_dataloader . dataset ) for batch , ( x , y ) in enumerate ( train_dataloader ): x = x . to ( DEVICE ) y = y . to ( DEVICE ) loss = graph_mobile_net_v2 ( x , y ) current = batch * BATCH_SIZE if batch % 5 == 0 : print ( f \"loss: { loss : >7f } [ { current : >5d } / { size : >5d } ]\" ) Comparing to inference, there are only a few things that are unique to training: # Optimizer optimizer = flow . optim . SGD ( model . parameters (), lr = 1e-3 ) # (1) # The MobileNetV2 Graph class GraphMobileNetV2 ( flow . nn . Graph ): def __init__ ( self ): # ... self . add_optimizer ( optimizer ) # (2) def build ( self , x , y ): # ... loss . backward () # (3) # ... Constructing the optimizer object, which is same to the training in Eager Mode introduced in Backpropagation and Optimizer . Call self.add_optimizer in Graph's __init__ method to add the optimizer object constructed in the previous step to the Graph. Call backward in Graph's build to trigger back propagation. Debugging in Graph Mode \u00b6 There are two ways to show the debug information of the Graph at present. Firstly you can call print to print the Graph object, and show information about it. print ( graph_mobile_net_v2 ) The output is slightly different depending on whether the Graph object has been called : If you use print before the Graph object is called, the output is information about the network structure. The output for print used before graph_mobile_net_v2 is called is like this: (GRAPH:GraphMobileNetV2_0:GraphMobileNetV2): ( (CONFIG:config:GraphConfig(training=True, )) (MODULE:model:MobileNetV2()): ( (MODULE:model.features:Sequential()): ( (MODULE:model.features.0:ConvBNActivation()): ( (MODULE:model.features.0.0:Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)): ( (PARAMETER:model.features.0.0.weight:tensor(..., device='cuda:0', size=(32, 3, 3, 3), dtype=oneflow.float32, requires_grad=True)): () ) ... (MODULE:model.classifer:Sequential()): ( (MODULE:model.classifer.0:Dropout(p=0.2, inplace=False)): () (MODULE:model.classifer.1:Linear(in_features=1280, out_features=10, bias=True)): ( (PARAMETER:model.classifer.1.weight:tensor(..., size=(10, 1280), dtype=oneflow.float32, requires_grad=True)): () (PARAMETER:model.classifer.1.bias:tensor(..., size=(10,), dtype=oneflow.float32, requires_grad=True)): () ) ) ) (MODULE:loss_fn:CrossEntropyLoss()): () ) In the above debug information, it means that based on Sequential model, the network customizes structures such as ConvBNActivation ( Corresponds to the MBConv module ), convolutional layer( including detailed parameter information such as channel , kernel_size and stride ), Dropout and fully connected layer. If you use print after the Graph object is called, in addition to the structure of the network, it will print inputs and outputs of the tensors, the output on the console is like this: (GRAPH:GraphMobileNetV2_0:GraphMobileNetV2): ( (CONFIG:config:GraphConfig(training=True, )) (INPUT:_GraphMobileNetV2_0-input_0:tensor(..., device='cuda:0', size=(64, 3, 32, 32), dtype=oneflow.float32)) (INPUT:_GraphMobileNetV2_0-input_1:tensor(..., device='cuda:0', size=(64,), dtype=oneflow.int64)) (MODULE:model:MobileNetV2()): ( (INPUT:_model-input_0:tensor(..., device='cuda:0', is_lazy='True', size=(64, 3, 32, 32), dtype=oneflow.float32)) (MODULE:model.features:Sequential()): ( (INPUT:_model.features-input_0:tensor(..., device='cuda:0', is_lazy='True', size=(64, 3, 32, 32), dtype=oneflow.float32)) (MODULE:model.features.0:ConvBNActivation()): ( (INPUT:_model.features.0-input_0:tensor(..., device='cuda:0', is_lazy='True', size=(64, 3, 32, 32), dtype=oneflow.float32)) (MODULE:model.features.0.0:Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)): ( (INPUT:_model.features.0.0-input_0:tensor(..., device='cuda:0', is_lazy='True', size=(64, 3, 32, 32), dtype=oneflow.float32)) (PARAMETER:model.features.0.0.weight:tensor(..., device='cuda:0', size=(32, 3, 3, 3), dtype=oneflow.float32, requires_grad=True)): () (OUTPUT:_model.features.0.0-output_0:tensor(..., device='cuda:0', is_lazy='True', size=(64, 32, 16, 16), dtype=oneflow.float32)) ) ... The second way is that by calling the debug method of Graph objects, Graph\u2019s debug mode is turned on. graph_mobile_net_v2 . debug ( v_level = 1 ) # The defalut of v_level is 0. which can also be written in a simplified way: graph_mobile_net_v2 . debug ( 1 ) OneFlow prints debug information when it compiles the computation graph. If the graph_mobile_net_v2.debug() is removed from the example code above, the output on the console is like this: (GRAPH:GraphMobileNetV2_0:GraphMobileNetV2) end building graph. (GRAPH:GraphMobileNetV2_0:GraphMobileNetV2) start compiling plan and init graph runtime. (GRAPH:GraphMobileNetV2_0:GraphMobileNetV2) end compiling plan and init graph rumtime. The advantage of using debug is that the debug information is composed and printed at the same time, which makes it easy to find the problem if there is any error in the graph building process. Use v_level to choose verbose debug info level, default level is 0, max level is 3. v_level=0 will only print basic information of warning and graph building stages, like graph building time. v_level=1 will additionally print graph build info of each nn.Module , the specific content is described in the table below. v_level=2 will additionally print graph build info of each operation in graph building stages, including name, input, device, SBP information, etc. v_level=3 will additionally print more detailed info of each operation, like information about the location of the code, which is convenient for locating problems in file. In addition, in order for developers to have a clearer understanding of the types under the Graph object, the following is an analysis of the output of debug , which basically includes seven categories of tags: GRAPH , CONFIG , MODULE , PARAMETER , BUFFER , INPUT and OUTPUT . Name Info Example GRAPH User-defined Graph information, followed by type: name: construction method. (GRAPH:GraphMobileNetV2_0:GraphMobileNetV2) CONFIG Graph configuration information. training=True indicates that the Graph is in training mode, and if in the evaluation mode, it corresponds to training=False . (CONFIG:config:GraphConfig(training=True, ) MODULE Corresponding to nn.Module , MODULE can be under the Graph tag, and there is also a hierarchical relationship between multiple modules. (MODULE:model:MobileNetV2()) , and MobileNetV2 reuses the Module class name in Eager mode for users. PARAMETER Shows the clearer information of weight and bias. In addition, when building the graph, the data content of the tensor is less important, so it is more important for building network to only display the meta information of the tensor. (PARAMETER:model.features.0.1.weight:tensor(..., device='cuda:0', size=(32,), dtype=oneflow.float32, requires_grad=True)) BUFFER Statistical characteristics and other content generated during training, such as running_mean and running_var. (BUFFER:model.features.0.1.running_mean:tensor(..., device='cuda:0', size=(32,), dtype=oneflow.float32)) INPUT & OUPTUT Tensor information representing input and output. (INPUT:_model_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(16, 3, 32, 32), dtype=oneflow.float32)) In addition to the methods described above, getting the parameters of the gradient during the training process, accessing to the learning rate and other functions are also under development and will come up soon. Save and Load Graph Models \u00b6 When training Graph model, it is often necessary to save the parameters of the model that has been trained for a period of time and other states such as optimizer parameters, so as to facilitate the resume of training after interruption. Graph model objects have state_dict and load_state_dict interfaces that similar to Module. We can save and load graph models with save and load . This is similar to Eagar module introduced in Model Save and Load . A little different from Eager mode is that when calling Graph's state_dict during training, in addition to the parameters of each layer of the internal Module, other states such as training iteration steps and optimizer parameters will also be obtained, so as to resume training later. For example, we hope to save the latest state after each epoch while training graph_mobile_net_v2 above, we need to add the following code: Assume that we want to save into \"GraphMobileNetV2\" under current directory: CHECKPOINT_SAVE_DIR = \"./GraphMobileNetV2\" Insert the following code at the completion of each epoch: import shutil shutil . rmtree ( CHECKPOINT_SAVE_DIR , ignore_errors = True ) # Clear previous state flow . save ( graph_mobile_net_v2 . state_dict (), CHECKPOINT_SAVE_DIR ) Note Don't save in the following way. Because Graph will process members when it is initialized, and graph_mobile_net_v2.model is actually no longer a Module type: flow.save(graph_mobile_net_v2.model.state_dict(), CHECKPOINT_SAVE_DIR) # it will report an error When we need to restore the previously saved state: state_dict = flow . load ( CHECKPOINT_SAVE_DIR ) graph_mobile_net_v2 . load_state_dict ( state_dict ) Graph and Deployment \u00b6 nn.Graph supports saving model parameters and computation graph at the same time, which can easily support model deployment. If there is a need for model deployment, the Graph object should be exported to the format required for deployment through the oneflow.save interface: MODEL_SAVE_DIR = \"./mobile_net_v2_model\" import os if not os . path . exists ( MODEL_SAVE_DIR ): os . makedirs ( MODEL_SAVE_DIR ) flow . save ( graph_mobile_net_v2 , MODEL_SAVE_DIR ) Note Note the difference from the previous section. save interface supports saving state_dict and also Graph objects. When the Graph object is saved, the model parameters and computation graph will be saved at the same time to decouple from the model structure definition code. In this way, both model parameters and computation graph required for deployment will be saved in the ./mobile_net_v2_model directory. For detailed deployment process, refer to Model Deployment . You must export the model via a Graph object to meet the format requirement for deployment. If it is a model trained in Eager mode (i.e. nn.Module object), you need to use Graph to encapsulate the Module and then export it. Next, we take the neural_style_transfer as an example in the flowvision repository to show how to encapsulate and export the nn.Module model. import oneflow as flow import oneflow.nn as nn from flowvision.models.neural_style_transfer.stylenet import neural_style_transfer class MyGraph ( nn . Graph ): def __init__ ( self , model ): super () . __init__ () self . model = model def build ( self , * input ): return self . model ( * input ) if __name__ == \"__main__\" : fake_image = flow . ones (( 1 , 3 , 256 , 256 )) model = neural_style_transfer ( pretrained = True , progress = True ) model . eval () graph = MyGraph ( model ) out = graph ( fake_image ) MODEL_SAVE_DIR = \"./neural_style_transfer_model\" import os if not os . path . exists ( MODEL_SAVE_DIR ): os . makedirs ( MODEL_SAVE_DIR ) flow . save ( graph , MODEL_SAVE_DIR ) The above code: Defines a MyGraph class, and simply encapsulates the nn.Module object ( return self.model(*input) ), which is only used to convert nn.Module into a Graph object Instantiates the model to get the Graph object\uff08 graph = MyGraph(model) ) Calls once the Graph object ( out = graph(fake_image) ). Its internal mechanism is to use \"fake data\" to flow through the model (tracing mechanism) to build a computation graph Exports the model required for deployment: flow.save(graph, \"1/model\") Further Reading: Dynamic Graph vs. Static Graph \u00b6 User-defined neural networks, are transformed by deep learning frameworks into computation graphs, like the example in Autograd : def loss ( y_pred , y ): return flow . sum ( 1 / 2 * ( y_pred - y ) ** 2 ) x = flow . ones ( 1 , 5 ) # the input w = flow . randn ( 5 , 3 , requires_grad = True ) b = flow . randn ( 1 , 3 , requires_grad = True ) z = flow . matmul ( x , w ) + b y = flow . zeros ( 1 , 3 ) # the label l = loss ( z , y ) The corresponding computation graph is: Dynamic Graph The characteristic of dynamic graph is that it is defined by run. The code above is run like this (Note: the figure below merges simple statements): Because the dynamic graph is defined by run, it is very flexible and easy to debug. You can modify the graph structure at any time and get results immediately. However, the deep learning framework can not get the complete graph information(which can be changed at any time and can never be considered as finished), it can not make full global optimization, so its performance is relatively poor. Static Graph Unlike a dynamic graph, a static graph defines a complete computation graph. It requires the user to declare all compute nodes before the framework starts running. This can be understood as the framework acting as a compiler between the user code and the computation graph that ultimately runs. In the case of the OneFlow, the user\u2019s code is first converted to a full computation graph and then run by the OneFlow Runtime module. Static graph, which get the complete network first, then compile and run, can be optimized in a way that dynamic graph can not, so they have an advantage in performance. It is also easier to deploy across platforms after compiling the computation graph. However, when the actual computation takes place in a static graph, it is no longer directly related to the user\u2019s code, so debugging the static graph is not convenient. The two approaches can be summarized as follows: Dynamic Graph Static Graph Computation Mode Eager Mode Graph Mode Pros The code is flexible and easy to debug. Good performance, easy to optimize and deploy. Cons Poor performance and portability. Not easy to debug. The Eager Mode in OneFlow is aligned with the PyTorch, which allows users familiar with the PyTorch to get their hands on easily with no more effert. The Graph Mode in OneFlow is based on the object-oriented programming style, which allows developers familiar with eager programming style to benefit from static graph with minimal code changes. Related Links \u00b6 Building neural network in OneFlow Eager Mode: Build Network","title":"Static Graph Interface"},{"location":"basics/08_nn_graph.html#static-graph-interface-nngraph","text":"At present, there are two ways to run models in deep learning framework, Dynamic Graph and Static Graph , which are also called Eager Mode and Graph Mode in OneFlow. There are pros and cons to both approaches, and OneFlow offers support for both, with the Eager Mode by default. If you are reading the tutorials for this basic topic in order, then all the code you have encountered so far is in Eager Mode. In general, dynamic graphs are easier to use and static graphs have better performance. OneFlow offers nn.Graph , so that users can use the eager-like programming style to build static graphs and train the models.","title":"STATIC GRAPH INTERFACE: NN.GRAPH"},{"location":"basics/08_nn_graph.html#eager-mode-in-oneflow","text":"OneFlow runs in Eager Mode by default. The following script uses data set CIFAR10 to train model mobilenet_v2 . Code import oneflow as flow import oneflow.nn as nn import flowvision import flowvision.transforms as transforms BATCH_SIZE = 64 EPOCH_NUM = 1 DEVICE = \"cuda\" if flow . cuda . is_available () else \"cpu\" print ( \"Using {} device\" . format ( DEVICE )) training_data = flowvision . datasets . CIFAR10 ( root = \"data\" , train = True , transform = transforms . ToTensor (), download = True , source_url = \"https://oneflow-public.oss-cn-beijing.aliyuncs.com/datasets/cifar/cifar-10-python.tar.gz\" , ) train_dataloader = flow . utils . data . DataLoader ( training_data , BATCH_SIZE , shuffle = True ) model = flowvision . models . mobilenet_v2 () . to ( DEVICE ) model . classifer = nn . Sequential ( nn . Dropout ( 0.2 ), nn . Linear ( model . last_channel , 10 )) model . train () loss_fn = nn . CrossEntropyLoss () . to ( DEVICE ) optimizer = flow . optim . SGD ( model . parameters (), lr = 1e-3 ) for t in range ( EPOCH_NUM ): print ( f \"Epoch { t + 1 } \\n -------------------------------\" ) size = len ( train_dataloader . dataset ) for batch , ( x , y ) in enumerate ( train_dataloader ): x = x . to ( DEVICE ) y = y . to ( DEVICE ) # Compute prediction error pred = model ( x ) loss = loss_fn ( pred , y ) # Backpropagation optimizer . zero_grad () loss . backward () optimizer . step () current = batch * BATCH_SIZE if batch % 5 == 0 : print ( f \"loss: { loss : >7f } [ { current : >5d } / { size : >5d } ]\" ) Out: loss: 6.921304 [ 0/50000] loss: 6.824391 [ 320/50000] loss: 6.688272 [ 640/50000] loss: 6.644351 [ 960/50000] ...","title":"Eager Mode in OneFlow"},{"location":"basics/08_nn_graph.html#graph-mode-in-oneflow","text":"","title":"Graph Mode in OneFlow"},{"location":"basics/08_nn_graph.html#customize-a-graph","text":"OneFlow provide the base class nn.Graph , which can be inherited to create a customized Graph class. import oneflow as flow import oneflow.nn as nn class ModuleMyLinear ( nn . Module ): def __init__ ( self , in_features , out_features ): super () . __init__ () self . weight = nn . Parameter ( flow . randn ( in_features , out_features )) self . bias = nn . Parameter ( flow . randn ( out_features )) def forward ( self , input ): return flow . matmul ( input , self . weight ) + self . bias linear_model = ModuleMyLinear ( 4 , 3 ) class GraphMyLinear ( nn . Graph ): def __init__ ( self ): super () . __init__ () self . model = linear_model def build ( self , input ): return self . model ( input ) The simple example above contains the important steps needed to customize a Graph: Inherits nn.Graph . Call super().__init__() at the begining of __init__ method to get OneFlow to do the necessary initialization for the Graph. In __init__ , reuse the nn.Module object in Eager mode ( self.model = model ) Describes the computational process in build method. You can then instantiate and call the Graph: graph_mylinear = GraphMyLinear () input = flow . randn ( 1 , 4 ) out = graph_mylinear ( input ) print ( out ) Out: tensor([[-0.3298, -3.7907, 0.1661]], dtype=oneflow.float32) Note that Graph is similar to Module in that the object itself is callable and it is not recommended to explicitly call the build method. Graph can directly reuse a defined Module. Users can refer the content in Build Network to build a neural network. Then, set Module as a member of Graph in __init__ of Graph. For example, use the linear_model above as the network structure: class ModelGraph ( flow . nn . Graph ): def __init__ ( self ): super () . __init__ () self . model = linear_model def build ( self , x , y ): y_pred = self . model ( x ) return loss model_graph = ModelGraph () The major difference between Module and Graph is that Graph uses build method rather than forward method to describe the computation process, because the build method can contain not only forward computation, but also setting loss , optimizer, etc. You will see an example of using Graph for training later.","title":"Customize a Graph"},{"location":"basics/08_nn_graph.html#inference-in-graph-mode","text":"The following example for inference in Graph Mode directly using the model, which we have already trained in Eager Mode at the beginning of this article. class GraphMobileNetV2 ( flow . nn . Graph ): def __init__ ( self ): super () . __init__ () self . model = model def build ( self , x ): return self . model ( x ) graph_mobile_net_v2 = GraphMobileNetV2 () x , _ = next ( iter ( train_dataloader )) x = x . to ( DEVICE ) y_pred = graph_mobile_net_v2 ( x )","title":"Inference in Graph Mode"},{"location":"basics/08_nn_graph.html#training-in-graph-mode","text":"The Graph can be used for training. Click on the \"Code\" below to see the detailed code. Code import oneflow as flow import oneflow.nn as nn import flowvision import flowvision.transforms as transforms BATCH_SIZE = 64 EPOCH_NUM = 1 DEVICE = \"cuda\" if flow . cuda . is_available () else \"cpu\" print ( \"Using {} device\" . format ( DEVICE )) training_data = flowvision . datasets . CIFAR10 ( root = \"data\" , train = True , transform = transforms . ToTensor (), download = True , ) train_dataloader = flow . utils . data . DataLoader ( training_data , BATCH_SIZE , shuffle = True , drop_last = True ) model = flowvision . models . mobilenet_v2 () . to ( DEVICE ) model . classifer = nn . Sequential ( nn . Dropout ( 0.2 ), nn . Linear ( model . last_channel , 10 )) model . train () loss_fn = nn . CrossEntropyLoss () . to ( DEVICE ) optimizer = flow . optim . SGD ( model . parameters (), lr = 1e-3 ) class GraphMobileNetV2 ( flow . nn . Graph ): def __init__ ( self ): super () . __init__ () self . model = model self . loss_fn = loss_fn self . add_optimizer ( optimizer ) def build ( self , x , y ): y_pred = self . model ( x ) loss = self . loss_fn ( y_pred , y ) loss . backward () return loss graph_mobile_net_v2 = GraphMobileNetV2 () # graph_mobile_net_v2.debug() for t in range ( EPOCH_NUM ): print ( f \"Epoch { t + 1 } \\n -------------------------------\" ) size = len ( train_dataloader . dataset ) for batch , ( x , y ) in enumerate ( train_dataloader ): x = x . to ( DEVICE ) y = y . to ( DEVICE ) loss = graph_mobile_net_v2 ( x , y ) current = batch * BATCH_SIZE if batch % 5 == 0 : print ( f \"loss: { loss : >7f } [ { current : >5d } / { size : >5d } ]\" ) Comparing to inference, there are only a few things that are unique to training: # Optimizer optimizer = flow . optim . SGD ( model . parameters (), lr = 1e-3 ) # (1) # The MobileNetV2 Graph class GraphMobileNetV2 ( flow . nn . Graph ): def __init__ ( self ): # ... self . add_optimizer ( optimizer ) # (2) def build ( self , x , y ): # ... loss . backward () # (3) # ... Constructing the optimizer object, which is same to the training in Eager Mode introduced in Backpropagation and Optimizer . Call self.add_optimizer in Graph's __init__ method to add the optimizer object constructed in the previous step to the Graph. Call backward in Graph's build to trigger back propagation.","title":"Training in Graph Mode"},{"location":"basics/08_nn_graph.html#debugging-in-graph-mode","text":"There are two ways to show the debug information of the Graph at present. Firstly you can call print to print the Graph object, and show information about it. print ( graph_mobile_net_v2 ) The output is slightly different depending on whether the Graph object has been called : If you use print before the Graph object is called, the output is information about the network structure. The output for print used before graph_mobile_net_v2 is called is like this: (GRAPH:GraphMobileNetV2_0:GraphMobileNetV2): ( (CONFIG:config:GraphConfig(training=True, )) (MODULE:model:MobileNetV2()): ( (MODULE:model.features:Sequential()): ( (MODULE:model.features.0:ConvBNActivation()): ( (MODULE:model.features.0.0:Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)): ( (PARAMETER:model.features.0.0.weight:tensor(..., device='cuda:0', size=(32, 3, 3, 3), dtype=oneflow.float32, requires_grad=True)): () ) ... (MODULE:model.classifer:Sequential()): ( (MODULE:model.classifer.0:Dropout(p=0.2, inplace=False)): () (MODULE:model.classifer.1:Linear(in_features=1280, out_features=10, bias=True)): ( (PARAMETER:model.classifer.1.weight:tensor(..., size=(10, 1280), dtype=oneflow.float32, requires_grad=True)): () (PARAMETER:model.classifer.1.bias:tensor(..., size=(10,), dtype=oneflow.float32, requires_grad=True)): () ) ) ) (MODULE:loss_fn:CrossEntropyLoss()): () ) In the above debug information, it means that based on Sequential model, the network customizes structures such as ConvBNActivation ( Corresponds to the MBConv module ), convolutional layer( including detailed parameter information such as channel , kernel_size and stride ), Dropout and fully connected layer. If you use print after the Graph object is called, in addition to the structure of the network, it will print inputs and outputs of the tensors, the output on the console is like this: (GRAPH:GraphMobileNetV2_0:GraphMobileNetV2): ( (CONFIG:config:GraphConfig(training=True, )) (INPUT:_GraphMobileNetV2_0-input_0:tensor(..., device='cuda:0', size=(64, 3, 32, 32), dtype=oneflow.float32)) (INPUT:_GraphMobileNetV2_0-input_1:tensor(..., device='cuda:0', size=(64,), dtype=oneflow.int64)) (MODULE:model:MobileNetV2()): ( (INPUT:_model-input_0:tensor(..., device='cuda:0', is_lazy='True', size=(64, 3, 32, 32), dtype=oneflow.float32)) (MODULE:model.features:Sequential()): ( (INPUT:_model.features-input_0:tensor(..., device='cuda:0', is_lazy='True', size=(64, 3, 32, 32), dtype=oneflow.float32)) (MODULE:model.features.0:ConvBNActivation()): ( (INPUT:_model.features.0-input_0:tensor(..., device='cuda:0', is_lazy='True', size=(64, 3, 32, 32), dtype=oneflow.float32)) (MODULE:model.features.0.0:Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)): ( (INPUT:_model.features.0.0-input_0:tensor(..., device='cuda:0', is_lazy='True', size=(64, 3, 32, 32), dtype=oneflow.float32)) (PARAMETER:model.features.0.0.weight:tensor(..., device='cuda:0', size=(32, 3, 3, 3), dtype=oneflow.float32, requires_grad=True)): () (OUTPUT:_model.features.0.0-output_0:tensor(..., device='cuda:0', is_lazy='True', size=(64, 32, 16, 16), dtype=oneflow.float32)) ) ... The second way is that by calling the debug method of Graph objects, Graph\u2019s debug mode is turned on. graph_mobile_net_v2 . debug ( v_level = 1 ) # The defalut of v_level is 0. which can also be written in a simplified way: graph_mobile_net_v2 . debug ( 1 ) OneFlow prints debug information when it compiles the computation graph. If the graph_mobile_net_v2.debug() is removed from the example code above, the output on the console is like this: (GRAPH:GraphMobileNetV2_0:GraphMobileNetV2) end building graph. (GRAPH:GraphMobileNetV2_0:GraphMobileNetV2) start compiling plan and init graph runtime. (GRAPH:GraphMobileNetV2_0:GraphMobileNetV2) end compiling plan and init graph rumtime. The advantage of using debug is that the debug information is composed and printed at the same time, which makes it easy to find the problem if there is any error in the graph building process. Use v_level to choose verbose debug info level, default level is 0, max level is 3. v_level=0 will only print basic information of warning and graph building stages, like graph building time. v_level=1 will additionally print graph build info of each nn.Module , the specific content is described in the table below. v_level=2 will additionally print graph build info of each operation in graph building stages, including name, input, device, SBP information, etc. v_level=3 will additionally print more detailed info of each operation, like information about the location of the code, which is convenient for locating problems in file. In addition, in order for developers to have a clearer understanding of the types under the Graph object, the following is an analysis of the output of debug , which basically includes seven categories of tags: GRAPH , CONFIG , MODULE , PARAMETER , BUFFER , INPUT and OUTPUT . Name Info Example GRAPH User-defined Graph information, followed by type: name: construction method. (GRAPH:GraphMobileNetV2_0:GraphMobileNetV2) CONFIG Graph configuration information. training=True indicates that the Graph is in training mode, and if in the evaluation mode, it corresponds to training=False . (CONFIG:config:GraphConfig(training=True, ) MODULE Corresponding to nn.Module , MODULE can be under the Graph tag, and there is also a hierarchical relationship between multiple modules. (MODULE:model:MobileNetV2()) , and MobileNetV2 reuses the Module class name in Eager mode for users. PARAMETER Shows the clearer information of weight and bias. In addition, when building the graph, the data content of the tensor is less important, so it is more important for building network to only display the meta information of the tensor. (PARAMETER:model.features.0.1.weight:tensor(..., device='cuda:0', size=(32,), dtype=oneflow.float32, requires_grad=True)) BUFFER Statistical characteristics and other content generated during training, such as running_mean and running_var. (BUFFER:model.features.0.1.running_mean:tensor(..., device='cuda:0', size=(32,), dtype=oneflow.float32)) INPUT & OUPTUT Tensor information representing input and output. (INPUT:_model_input.0.0_2:tensor(..., device='cuda:0', is_lazy='True', size=(16, 3, 32, 32), dtype=oneflow.float32)) In addition to the methods described above, getting the parameters of the gradient during the training process, accessing to the learning rate and other functions are also under development and will come up soon.","title":"Debugging in Graph Mode"},{"location":"basics/08_nn_graph.html#save-and-load-graph-models","text":"When training Graph model, it is often necessary to save the parameters of the model that has been trained for a period of time and other states such as optimizer parameters, so as to facilitate the resume of training after interruption. Graph model objects have state_dict and load_state_dict interfaces that similar to Module. We can save and load graph models with save and load . This is similar to Eagar module introduced in Model Save and Load . A little different from Eager mode is that when calling Graph's state_dict during training, in addition to the parameters of each layer of the internal Module, other states such as training iteration steps and optimizer parameters will also be obtained, so as to resume training later. For example, we hope to save the latest state after each epoch while training graph_mobile_net_v2 above, we need to add the following code: Assume that we want to save into \"GraphMobileNetV2\" under current directory: CHECKPOINT_SAVE_DIR = \"./GraphMobileNetV2\" Insert the following code at the completion of each epoch: import shutil shutil . rmtree ( CHECKPOINT_SAVE_DIR , ignore_errors = True ) # Clear previous state flow . save ( graph_mobile_net_v2 . state_dict (), CHECKPOINT_SAVE_DIR ) Note Don't save in the following way. Because Graph will process members when it is initialized, and graph_mobile_net_v2.model is actually no longer a Module type: flow.save(graph_mobile_net_v2.model.state_dict(), CHECKPOINT_SAVE_DIR) # it will report an error When we need to restore the previously saved state: state_dict = flow . load ( CHECKPOINT_SAVE_DIR ) graph_mobile_net_v2 . load_state_dict ( state_dict )","title":"Save and Load Graph Models"},{"location":"basics/08_nn_graph.html#graph-and-deployment","text":"nn.Graph supports saving model parameters and computation graph at the same time, which can easily support model deployment. If there is a need for model deployment, the Graph object should be exported to the format required for deployment through the oneflow.save interface: MODEL_SAVE_DIR = \"./mobile_net_v2_model\" import os if not os . path . exists ( MODEL_SAVE_DIR ): os . makedirs ( MODEL_SAVE_DIR ) flow . save ( graph_mobile_net_v2 , MODEL_SAVE_DIR ) Note Note the difference from the previous section. save interface supports saving state_dict and also Graph objects. When the Graph object is saved, the model parameters and computation graph will be saved at the same time to decouple from the model structure definition code. In this way, both model parameters and computation graph required for deployment will be saved in the ./mobile_net_v2_model directory. For detailed deployment process, refer to Model Deployment . You must export the model via a Graph object to meet the format requirement for deployment. If it is a model trained in Eager mode (i.e. nn.Module object), you need to use Graph to encapsulate the Module and then export it. Next, we take the neural_style_transfer as an example in the flowvision repository to show how to encapsulate and export the nn.Module model. import oneflow as flow import oneflow.nn as nn from flowvision.models.neural_style_transfer.stylenet import neural_style_transfer class MyGraph ( nn . Graph ): def __init__ ( self , model ): super () . __init__ () self . model = model def build ( self , * input ): return self . model ( * input ) if __name__ == \"__main__\" : fake_image = flow . ones (( 1 , 3 , 256 , 256 )) model = neural_style_transfer ( pretrained = True , progress = True ) model . eval () graph = MyGraph ( model ) out = graph ( fake_image ) MODEL_SAVE_DIR = \"./neural_style_transfer_model\" import os if not os . path . exists ( MODEL_SAVE_DIR ): os . makedirs ( MODEL_SAVE_DIR ) flow . save ( graph , MODEL_SAVE_DIR ) The above code: Defines a MyGraph class, and simply encapsulates the nn.Module object ( return self.model(*input) ), which is only used to convert nn.Module into a Graph object Instantiates the model to get the Graph object\uff08 graph = MyGraph(model) ) Calls once the Graph object ( out = graph(fake_image) ). Its internal mechanism is to use \"fake data\" to flow through the model (tracing mechanism) to build a computation graph Exports the model required for deployment: flow.save(graph, \"1/model\")","title":"Graph and Deployment"},{"location":"basics/08_nn_graph.html#further-reading-dynamic-graph-vs-static-graph","text":"User-defined neural networks, are transformed by deep learning frameworks into computation graphs, like the example in Autograd : def loss ( y_pred , y ): return flow . sum ( 1 / 2 * ( y_pred - y ) ** 2 ) x = flow . ones ( 1 , 5 ) # the input w = flow . randn ( 5 , 3 , requires_grad = True ) b = flow . randn ( 1 , 3 , requires_grad = True ) z = flow . matmul ( x , w ) + b y = flow . zeros ( 1 , 3 ) # the label l = loss ( z , y ) The corresponding computation graph is: Dynamic Graph The characteristic of dynamic graph is that it is defined by run. The code above is run like this (Note: the figure below merges simple statements): Because the dynamic graph is defined by run, it is very flexible and easy to debug. You can modify the graph structure at any time and get results immediately. However, the deep learning framework can not get the complete graph information(which can be changed at any time and can never be considered as finished), it can not make full global optimization, so its performance is relatively poor. Static Graph Unlike a dynamic graph, a static graph defines a complete computation graph. It requires the user to declare all compute nodes before the framework starts running. This can be understood as the framework acting as a compiler between the user code and the computation graph that ultimately runs. In the case of the OneFlow, the user\u2019s code is first converted to a full computation graph and then run by the OneFlow Runtime module. Static graph, which get the complete network first, then compile and run, can be optimized in a way that dynamic graph can not, so they have an advantage in performance. It is also easier to deploy across platforms after compiling the computation graph. However, when the actual computation takes place in a static graph, it is no longer directly related to the user\u2019s code, so debugging the static graph is not convenient. The two approaches can be summarized as follows: Dynamic Graph Static Graph Computation Mode Eager Mode Graph Mode Pros The code is flexible and easy to debug. Good performance, easy to optimize and deploy. Cons Poor performance and portability. Not easy to debug. The Eager Mode in OneFlow is aligned with the PyTorch, which allows users familiar with the PyTorch to get their hands on easily with no more effert. The Graph Mode in OneFlow is based on the object-oriented programming style, which allows developers familiar with eager programming style to benefit from static graph with minimal code changes.","title":"Further Reading: Dynamic Graph vs. Static Graph"},{"location":"basics/08_nn_graph.html#related-links","text":"Building neural network in OneFlow Eager Mode: Build Network","title":"Related Links"},{"location":"cookies/activation_checkpointing.html","text":"Activation Checkpointing \u00b6 Introduction to Activation Checkpointing \u00b6 Activation Checkpointing is a sub-linear memory optimization technique proposed in 2016, by Chen Tianqi's team in their paper Training Deep Nets with Sublinear Memory Cost , aiming to reduce the memory usage during training. The basic principle of Activation Checkpointing is exchange time for space : After the analysis of the computational graph, some intermediate activation features that are not used temporarily in the forward process will be deleted to reduce the memory usage, and they will be restored with additional forward computation when needed in the backward process. OneFlow's static graph module nn.Graph already supports Activation Checkpointing. This article will introduce how to turn on it during training. Example of using Activation Checkpointing \u00b6 First, we define a simple model consist of loss function and optimizer in exactly the same way as before. import oneflow as flow import oneflow.nn as nn DEVICE = \"cuda\" if flow . cuda . is_available () else \"cpu\" print ( \"Using {} device\" . format ( DEVICE )) model_part1 = nn . Sequential ( nn . Linear ( 256 , 128 ), nn . ReLU (), nn . Linear ( 128 , 64 ), nn . ReLU () ) model_part1 = model_part1 . to ( DEVICE ) model_part1 . train () model_part2 = nn . Sequential ( nn . Linear ( 64 , 32 ), nn . ReLU (), nn . Linear ( 32 , 10 ) ) model_part2 = model_part2 . to ( DEVICE ) model_part2 . train () loss_fn = nn . CrossEntropyLoss () . to ( DEVICE ) optimizer = flow . optim . SGD ([{ 'params' : model_part1 . parameters ()}, { 'params' : model_part2 . parameters ()}], lr = 1e-3 ) To turn on activation checkpointing, you only need to specify .config.activation_checkpointing = True on the Eager model member (i.e. the nn.Module object) in the nn.Graph model. For more details of this API, please refer to: activation_checkpointing . For each nn.Module with \"activation checkpointing\" turned on, its input activations will be preserved, while other intermediate activations will be recomputed when used during backpropagation. class CustomGraph ( flow . nn . Graph ): def __init__ ( self ): super () . __init__ () self . model_part1 = model_part1 self . model_part2 = model_part2 # Turn on activation checkpointing on two consecutive nn.Module self . model_part1 . config . activation_checkpointing = True self . model_part2 . config . activation_checkpointing = True self . loss_fn = loss_fn self . add_optimizer ( optimizer ) def build ( self , x , y ): y_pred = self . model_part2 ( self . model_part1 ( x )) loss = self . loss_fn ( y_pred , y ) loss . backward () return y_pred , loss Then, you can start training and other operations as usual. graph_model = CustomGraph () for _ in range ( 100 ): x = flow . randn ( 128 , 256 ) . to ( DEVICE ) y = flow . ones ( 128 , 1 , dtype = flow . int64 ) . to ( DEVICE ) graph_model ( x , y ) # Other codes... Comparative Experiment on BERT Model \u00b6 In order to verify the actual effect of Activation Checkpointing, we can conduct comparative experiments on the model BERT . We can directly use the BERT model provided by libai . To turn on Activation Checkpointing, we just need to set train.activation_checkpoint.enabled to True in the configuration file. First, get data ready according to Prepare the Data and the Vocab . For simplicity, we use a single device for training (the GPU used in the experimental environment is NVIDIA GeForce RTX 3090, and the memory size is 24268 MB): time python tools/train_net.py --config-file configs/bert_large_pretrain.py Add the time command at the beginning of the whole command to measure the time spent in the training process. The experimental results are as follows: Whether to Turn on Activation Checkpointing Average Memory Usage Time Spent No 9141 MB 25 minutes 16 seconds Yes 5978 MB 33 minutes 36 seconds We can see from the above table that Activation Checkpointin significantly reduces the memory usage during training. At the same time, the time spent increases due to the additional forward computation required. Overall, Activation Checkpointing is a very effective solution when there is a lack of video memory.","title":"Activation Checkpointing"},{"location":"cookies/activation_checkpointing.html#activation-checkpointing","text":"","title":"Activation Checkpointing"},{"location":"cookies/activation_checkpointing.html#introduction-to-activation-checkpointing","text":"Activation Checkpointing is a sub-linear memory optimization technique proposed in 2016, by Chen Tianqi's team in their paper Training Deep Nets with Sublinear Memory Cost , aiming to reduce the memory usage during training. The basic principle of Activation Checkpointing is exchange time for space : After the analysis of the computational graph, some intermediate activation features that are not used temporarily in the forward process will be deleted to reduce the memory usage, and they will be restored with additional forward computation when needed in the backward process. OneFlow's static graph module nn.Graph already supports Activation Checkpointing. This article will introduce how to turn on it during training.","title":"Introduction to Activation Checkpointing"},{"location":"cookies/activation_checkpointing.html#example-of-using-activation-checkpointing","text":"First, we define a simple model consist of loss function and optimizer in exactly the same way as before. import oneflow as flow import oneflow.nn as nn DEVICE = \"cuda\" if flow . cuda . is_available () else \"cpu\" print ( \"Using {} device\" . format ( DEVICE )) model_part1 = nn . Sequential ( nn . Linear ( 256 , 128 ), nn . ReLU (), nn . Linear ( 128 , 64 ), nn . ReLU () ) model_part1 = model_part1 . to ( DEVICE ) model_part1 . train () model_part2 = nn . Sequential ( nn . Linear ( 64 , 32 ), nn . ReLU (), nn . Linear ( 32 , 10 ) ) model_part2 = model_part2 . to ( DEVICE ) model_part2 . train () loss_fn = nn . CrossEntropyLoss () . to ( DEVICE ) optimizer = flow . optim . SGD ([{ 'params' : model_part1 . parameters ()}, { 'params' : model_part2 . parameters ()}], lr = 1e-3 ) To turn on activation checkpointing, you only need to specify .config.activation_checkpointing = True on the Eager model member (i.e. the nn.Module object) in the nn.Graph model. For more details of this API, please refer to: activation_checkpointing . For each nn.Module with \"activation checkpointing\" turned on, its input activations will be preserved, while other intermediate activations will be recomputed when used during backpropagation. class CustomGraph ( flow . nn . Graph ): def __init__ ( self ): super () . __init__ () self . model_part1 = model_part1 self . model_part2 = model_part2 # Turn on activation checkpointing on two consecutive nn.Module self . model_part1 . config . activation_checkpointing = True self . model_part2 . config . activation_checkpointing = True self . loss_fn = loss_fn self . add_optimizer ( optimizer ) def build ( self , x , y ): y_pred = self . model_part2 ( self . model_part1 ( x )) loss = self . loss_fn ( y_pred , y ) loss . backward () return y_pred , loss Then, you can start training and other operations as usual. graph_model = CustomGraph () for _ in range ( 100 ): x = flow . randn ( 128 , 256 ) . to ( DEVICE ) y = flow . ones ( 128 , 1 , dtype = flow . int64 ) . to ( DEVICE ) graph_model ( x , y ) # Other codes...","title":"Example of using Activation Checkpointing"},{"location":"cookies/activation_checkpointing.html#comparative-experiment-on-bert-model","text":"In order to verify the actual effect of Activation Checkpointing, we can conduct comparative experiments on the model BERT . We can directly use the BERT model provided by libai . To turn on Activation Checkpointing, we just need to set train.activation_checkpoint.enabled to True in the configuration file. First, get data ready according to Prepare the Data and the Vocab . For simplicity, we use a single device for training (the GPU used in the experimental environment is NVIDIA GeForce RTX 3090, and the memory size is 24268 MB): time python tools/train_net.py --config-file configs/bert_large_pretrain.py Add the time command at the beginning of the whole command to measure the time spent in the training process. The experimental results are as follows: Whether to Turn on Activation Checkpointing Average Memory Usage Time Spent No 9141 MB 25 minutes 16 seconds Yes 5978 MB 33 minutes 36 seconds We can see from the above table that Activation Checkpointin significantly reduces the memory usage during training. At the same time, the time spent increases due to the additional forward computation required. Overall, Activation Checkpointing is a very effective solution when there is a lack of video memory.","title":"Comparative Experiment on BERT Model"},{"location":"cookies/amp.html","text":"Automatic Mixed Precision Training \u00b6 Introduction to AMP \u00b6 When we train deep learning models, we typically use 32-bit single-precision floating point (FP32), while AMP (Automatic Mixed Precision) is a technique that allows both FP32 and FP16 to be used when training models. This can make the memory usage less and the computation faster when training the model. But because the numerical range of FP16 is smaller than that of FP32, it is more prone to numerical overflow problems, and there may be some errors. But lots of practice has proved that many deep learning models can be trained with this technique without loss of accuracy. Example of using AMP \u00b6 First, we define a simple model, loss function and optimizer in exactly the same way as before. import oneflow as flow import oneflow.nn as nn DEVICE = \"cuda\" if flow . cuda . is_available () else \"cpu\" print ( \"Using {} device\" . format ( DEVICE )) model = nn . Sequential ( nn . Linear ( 256 , 128 ), nn . ReLU (), nn . Linear ( 128 , 10 ) ) model = model . to ( DEVICE ) model . train () loss_fn = nn . CrossEntropyLoss () . to ( DEVICE ) optimizer = flow . optim . SGD ( model . parameters (), lr = 1e-3 ) If you want to enable AMP mode, just add self.config.enable_amp(True) to the model nn.Graph . The details of this API is at: enable_amp . class CustomGraph ( flow . nn . Graph ): def __init__ ( self ): super () . __init__ () self . model = model self . loss_fn = loss_fn self . add_optimizer ( optimizer ) self . config . enable_amp ( True ) # \u5f00\u542f AMP \u6a21\u5f0f def build ( self , x , y ): y_pred = self . model ( x ) loss = self . loss_fn ( y_pred , y ) loss . backward () return y_pred Then, you can start training and other operations as usual. graph_model = CustomGraph () for _ in range ( 100 ): x = flow . randn ( 128 , 256 ) . to ( DEVICE ) y = flow . ones ( 128 , 1 , dtype = flow . int64 ) . to ( DEVICE ) graph_model ( x , y ) Gradient Scaling \u00b6 Gradient Scaling is a method for solving the problem that FP16 is prone to numerical overflow. The basic principle is to use a scale factor to scale the loss and gradient in the process of backpropagation to change the magnitude of its value, thereby mitigate numerical overflow problems as much as possible. OneFlow provides GradScaler to use Gradient Scaling in AMP mode. You only need to instantiate a GradScaler object in the __init__ method of the nn.Graph model, and then specify it through the interface set_grad_scaler . nn.Graph will automatically manage the whole process of Gradient Scaling. Taking the CustomGraph above as an example, you need to add the following code to its __init__ method: grad_scaler = flow . amp . GradScaler ( init_scale = 3000 , growth_factor = 2.0 , backoff_factor = 0.5 , growth_interval = 1000 , ) self . set_grad_scaler ( grad_scaler ) The calculation process of the scale factor and the meaning of the GradScaler parameters are as follows: The size of the scale factor is dynamically estimated in the iterative update (the initial value is specified by init_scale ). In order to reduce the numerical underflow as much as possible, the scale factor should be larger; but if it is too large, FP16 is prone to numerical overflow , resulting in an inf or NaN. The process of dynamic estimation is to increase the scale factor as much as possible without occuring inf or NaN. At each iteration, it will check whether there is a gradient of inf or NaN: If there is: this weight update will be ignored and the scale factor will be reduced (multiplied by the backoff_factor ) If not: weight will update normally. Scale factor will be increased (multiplied by growth_factor ) when no inf or NaN occurs in successive iterations (specified by growth_interval )","title":"Automatic Mixed Precision Training"},{"location":"cookies/amp.html#automatic-mixed-precision-training","text":"","title":"Automatic Mixed Precision Training"},{"location":"cookies/amp.html#introduction-to-amp","text":"When we train deep learning models, we typically use 32-bit single-precision floating point (FP32), while AMP (Automatic Mixed Precision) is a technique that allows both FP32 and FP16 to be used when training models. This can make the memory usage less and the computation faster when training the model. But because the numerical range of FP16 is smaller than that of FP32, it is more prone to numerical overflow problems, and there may be some errors. But lots of practice has proved that many deep learning models can be trained with this technique without loss of accuracy.","title":"Introduction to AMP"},{"location":"cookies/amp.html#example-of-using-amp","text":"First, we define a simple model, loss function and optimizer in exactly the same way as before. import oneflow as flow import oneflow.nn as nn DEVICE = \"cuda\" if flow . cuda . is_available () else \"cpu\" print ( \"Using {} device\" . format ( DEVICE )) model = nn . Sequential ( nn . Linear ( 256 , 128 ), nn . ReLU (), nn . Linear ( 128 , 10 ) ) model = model . to ( DEVICE ) model . train () loss_fn = nn . CrossEntropyLoss () . to ( DEVICE ) optimizer = flow . optim . SGD ( model . parameters (), lr = 1e-3 ) If you want to enable AMP mode, just add self.config.enable_amp(True) to the model nn.Graph . The details of this API is at: enable_amp . class CustomGraph ( flow . nn . Graph ): def __init__ ( self ): super () . __init__ () self . model = model self . loss_fn = loss_fn self . add_optimizer ( optimizer ) self . config . enable_amp ( True ) # \u5f00\u542f AMP \u6a21\u5f0f def build ( self , x , y ): y_pred = self . model ( x ) loss = self . loss_fn ( y_pred , y ) loss . backward () return y_pred Then, you can start training and other operations as usual. graph_model = CustomGraph () for _ in range ( 100 ): x = flow . randn ( 128 , 256 ) . to ( DEVICE ) y = flow . ones ( 128 , 1 , dtype = flow . int64 ) . to ( DEVICE ) graph_model ( x , y )","title":"Example of using AMP"},{"location":"cookies/amp.html#gradient-scaling","text":"Gradient Scaling is a method for solving the problem that FP16 is prone to numerical overflow. The basic principle is to use a scale factor to scale the loss and gradient in the process of backpropagation to change the magnitude of its value, thereby mitigate numerical overflow problems as much as possible. OneFlow provides GradScaler to use Gradient Scaling in AMP mode. You only need to instantiate a GradScaler object in the __init__ method of the nn.Graph model, and then specify it through the interface set_grad_scaler . nn.Graph will automatically manage the whole process of Gradient Scaling. Taking the CustomGraph above as an example, you need to add the following code to its __init__ method: grad_scaler = flow . amp . GradScaler ( init_scale = 3000 , growth_factor = 2.0 , backoff_factor = 0.5 , growth_interval = 1000 , ) self . set_grad_scaler ( grad_scaler ) The calculation process of the scale factor and the meaning of the GradScaler parameters are as follows: The size of the scale factor is dynamically estimated in the iterative update (the initial value is specified by init_scale ). In order to reduce the numerical underflow as much as possible, the scale factor should be larger; but if it is too large, FP16 is prone to numerical overflow , resulting in an inf or NaN. The process of dynamic estimation is to increase the scale factor as much as possible without occuring inf or NaN. At each iteration, it will check whether there is a gradient of inf or NaN: If there is: this weight update will be ignored and the scale factor will be reduced (multiplied by the backoff_factor ) If not: weight will update normally. Scale factor will be increased (multiplied by growth_factor ) when no inf or NaN occurs in successive iterations (specified by growth_interval )","title":"Gradient Scaling"},{"location":"cookies/global_tensor.html","text":"Using Global Tensor to Program on Multi-Device Multi-GPU: Basic Operations \u00b6 By YaoChi , Xu Xiaoyu , Zuo Yihao , Guoliang Cheng , Shen Jiali Global tensor can be executed on multi-device multi-GPU, and it\u2019s an interface to implement the Global View programming. Today, most parallel programs adopt the SPMD (Single program, multiple data) programming method, which means the devices will execute the same program but process different parts of the data to realize data parallelism. Take PyTorch\u2019s DDP (Distributed Data Parallel) for example, each process executes the same neural network computing logic, but the difference is that they load different slices of one dataset. But, the defect of SPMD programming is that multiple data makes communications more complicated. In a deep learning scenario, SPMD programming needs to insert communication operations into original computing codes, such as AllReduce for data parallelism and AllGather/ReduceScatter for model parallelism. If the parallel mode is much more complicated or a new mode needs to be experimented with, it will be troublesome to develop and maintain after inserting the communication operations. Global View programming permits users to program from the SPSD view. Different from SPMD programming, SPSD programming is a method that data is also single from the programming interface layer. When we extend a single-process program to a parallelly executed one, the single-process data will also be extended to the multi-process data, so it's natural that the data on different processes corresponds to the same logic data on the originally single-process program. And the logic data is called Global Tensor in OneFlow. Global Tensor supports users to utilize the SPSD interface to program, which means users can program on a single device and OneFlow framework will automatically convert to physical SPMD/MPMD mode and execute the program in a parallel/distributed way. With Global Tensor, a more naturally Global View programming method is available, and users can regard the multi-devices as a single device to implement SPSD programming. Global Tensor \u00b6 In programming languages, \"Global\" usually refers to in-process global visibility, such as Global Variable . Instead, the \"Global\" of the \"Global Tensor\" means inter-process global visibility. So, it\u2019s more accurate to regard the Global Tensor as a tensor that can be seen on all processes. Global Tensor exists on all processes. When the tensor is executed by an operator on all processes, it will be automatically executed on multi-device multi-GPU. At present, the commonly-used tensor is only visible on one process and also exists on a single device. OneFlow calls it the Local Tensor, which means it\u2019s a tensor that can be seen on only one process. Local is relative to Global, so Local Tensor can be considered as Local (on one process) Tensor. Most of OneFlow\u2019s operators are compatible with the execution of Local Tensors and Global Tensors. It\u2019s convenient to convert the Local Tensor to the Global Tensor, so the code originally executed on single-device single-GPU can be smoothly converted to ones that can be executed on multi-device multi-GPU. Global Tensor allows users to easily develop models on multi-device multi-GPU. Compared to utilizing the original communication operators, the efficiency of developing parallelly executed models will be doubled. Creating Global Tensor \u00b6 Let\u2019s try to create a Global Tensor on a machine with two GPUs. Take randn operator for example, a Python file named test_randn_global.py needs to be created and add the following content to it: import oneflow as flow # Place a global tensor on cuda device of rank(process) 0 and 1 placement = flow . placement ( type = \"cuda\" , ranks = [ 0 , 1 ]) # Each rank's local data is a part data as a result of spliting global data on dim 0 sbp = flow . sbp . split ( dim = 0 ) # Create a global tensor by randn x = flow . randn ( 4 , 5 , placement = placement , sbp = sbp ) # Print local data print ( \"Local data of global tensor: \\n \" , x . to_local () . numpy ()) # Print global data print ( \"Global data of global tensor: \\n \" , x . numpy ()) Here are some explanations for some new concepts in the code above: placement refers to the physical device where the Global Tensor locates. The parameter type specifies the type of the physical device, and here we use \"cuda\" to represent the GPU device. The parameter ranks specifies the device ID. For readers who don\u2019t have 2 GPUs, the parameter type can be specified as \"cpu\" to use the CPU to simulate multiple devices, and the following code still works. sbp refers to the distributed way of the Global Tensor. Here, sbp = flow.sbp.split(dim=0) means that the Global Tensor is evenly split along dimension 0. The to_local() method is to acquire the Local Tensor in the present rank from the Global Tensor because the Global Tensor has one Local Tensor in each rank as its practically existing local component. Next, configure the environment variables required by multi-process launching. Here, the machine owns 2 GPUs, which correspond to 2 process launchings. So, we should turn on 2 terminals and respectively configure the following environment variables: Note Clicking the label \"Terminal 0\" or \"Terminal 1\" separately to check its corresponding console\u2019s command/code. Terminal 0 export MASTER_ADDR = 127 .0.0.1 MASTER_PORT = 17789 WORLD_SIZE = 2 RANK = 0 LOCAL_RANK = 0 Terminal 1 export MASTER_ADDR = 127 .0.0.1 MASTER_PORT = 17789 WORLD_SIZE = 2 RANK = 1 LOCAL_RANK = 1 More about detailed explanation of the environment variables above and how to conduct a distributed launching with the help of tools, please refer to Further reading . Finally, launch test_randn_global.py in two terminals respectively and observe the results of creating the Global Tensor: python3 test_randn_global.py In Terminal 0 (rank 0), we can see: Local data of global tensor: [[-0.07157125 -0.92717147 1.5102768 1.4611115 1.014263 ] [-0.1511031 1.570759 0.9416077 0.6184639 2.4420679 ]] Global data of global tensor: [[-0.07157125 -0.92717147 1.5102768 1.4611115 1.014263 ] [-0.1511031 1.570759 0.9416077 0.6184639 2.4420679 ] [-0.38203463 0.453836 0.9136015 2.35773 -0.3279942 ] [-0.8570119 -0.91476554 -0.06646168 0.50022084 -0.4387695 ]] In Terminal 1 (rank 1), we can see: Local data of global tensor: [[-0.38203463 0.453836 0.9136015 2.35773 -0.3279942 ] [-0.8570119 -0.91476554 -0.06646168 0.50022084 -0.4387695 ]] Global data of global tensor: [[-0.07157125 -0.92717147 1.5102768 1.4611115 1.014263 ] [-0.1511031 1.570759 0.9416077 0.6184639 2.4420679 ] [-0.38203463 0.453836 0.9136015 2.35773 -0.3279942 ] [-0.8570119 -0.91476554 -0.06646168 0.50022084 -0.4387695 ]] It\u2019s clear that if we concatenate the Local Tensors in rank 1 and rank 2 on dimension 0, we can get the complete value of the Global Tensor. Converting Local Tensor to Global Tensor \u00b6 We can firstly create a Local Tensor and then utilize the Tensor.to_global method to convert the Local Tensor to a Global Tensor. Create the following program and launch it in the similar way mentioned above: import oneflow as flow x = flow . randn ( 2 , 5 ) . cuda () print ( x . is_local ) # True print ( x . is_global ) # False placement = flow . placement ( type = \"cuda\" , ranks = [ 0 , 1 ]) sbp = flow . sbp . split ( 0 ) x_global = x . to_global ( placement = placement , sbp = sbp ) print ( x_global . shape ) # (4, 5) print ( x . is_local ) # True print ( x_global . is_global ) # True This program separately creates a Local Tensor with the shape of (2,5) on 2 GPUs, and the newly-created tensors are called x. Then, we specify cuda devices in rank 0 and rank 1 as the placement and split(dim=0) as its SBP. After the to_global method, the original Local Tensor is converted to the Global Tensor named x_global . We can see that the shape of x_global has been changed into (4, 5) , which is the same as the (global) shape of the Global Tensor. The relationship between the Global Tensor and the Local Tensor is the total and the component, and the Local Tensor is the component of the total in a certain rank. The specific relationship between the Global Tensor and the Local Tensor is decided by the placement and SBP. For example, in the above case, the relationship is between tensors on GPU 0 and GPU 1, and we split x_global along dimension 0 to get x . Based on the above relationship, the to_global method can infer x_global.shape according to x.shape : it concatenates the Local Tensor x on 2 GPUs along dimension 0 to obtain x_global . Except for shape, the Global Tensor also contains some data. The Global Tensor has a Local Tensor in each rank to symbolize its local component, which is its physical data in every rank. By the way, each rank only stores different parts of the data. Converting Global Tensor to Local Tensor \u00b6 You can utilize the to_local method to obtain the local component of the Global Tensor, just like the following: import oneflow as flow placement = flow . placement ( type = \"cuda\" , ranks = [ 0 , 1 ]) sbp = flow . sbp . split ( 0 ) x = flow . randn ( 4 , 5 , placement = placement , sbp = sbp ) print ( x . to_local ()) When the x.to_local() method is executed, two different ranks will separately obtain a Local Tensor with the shape of (2, 5) . In Terminal 0 (rank 0), we can see: tensor([[-0.2730, 1.8042, 0.0721, -0.5024, -1.2583], [-0.3379, 0.9371, 0.7981, -0.5447, -0.5629]], dtype=oneflow.float32) In Terminal 1 (rank 1), we can see: tensor([[ 0.6829, 0.4849, 2.1611, 1.4059, 0.0934], [-0.0301, -0.6942, -0.8094, -1.3050, -0.1778]], dtype=oneflow.float32) The to_local() has no parameters, because the Global Tensor has already confirmed its local component according to the placement and SBP, and it\u2019s fine to directly acquire the Local Tensor that its local component corresponds to. Converting One Global Tensor to Another Global Tensor \u00b6 Usually, distributed computing requires inserting communication operations into normal computational logic, but OneFlow only needs users to convert the data distribution type of the Global Tensor. In terms of type, the biggest difference between the Global Tensor and the general Local Tensor is that the Global Tensor has global data distribution type, which specifies how the Global Tensor is distributed in each rank, including its placement and SBP. The function of placement in global data distribution type is to specify the device group where data is distributed: The parameter type specifies the physical device type. cuda represents the GPU device memory, and cpu` refers to the CPU device memory. The parameter ranks specifies the process ID set. Because each rank corresponds to one physical device, ranks can also be seen as the device ID set. Actually, ranks is an nd-array composed of rank ID, which supports high-dimensional device arrangement. For more details, please refer to oneflow.placement . The function of SBP in the global data distribution type is to specify the relationship between global data and local data: S, i.e., split(dim), notes that the relationship between global data and local data is split, indicating the global data is evenly split according to the dimension dim and distributed in each rank. B, i.e., broadcast, notes that the relationship between global data and local data is broadcast, indicating the global data is replicated in each rank. P, i.e., partial_sum, notes that the relationship between global data and local data is partial, indicating the value of the global data is the element-wise sum of the local data distributed in each rank. For more details, please refer to oneflow.sbp.sbp . Data re-distribution is commonly seen in parallel computing, i.e., changing the distributed way of data, such as gathering all data slices. In the MPI programming paradigm (SPMD), data re-distribution requires writing explicit communication operations like AllReduce, AllGather, and ReduceScatter. But in OneFlow\u2019s Global View programming paradigm (SPSD), data re-distribution can be achieved by utilizing Global Tensor\u2019s global data distribution type conversion. The conversion of the global data distribution type is similar to (explicit) type conversion in general programming languages. Users only need to specify the targeted type when they convert types, and some implicit operations can be executed automatically. For example, when converting the type from double to int, the system will remove the decimal point automatically. Similarly, it\u2019s only required to specify the new global data distribution type that the Global Tensor will be converted into, and OneFlow will complete implicit communication operations automatically. And the interface to convert the global data distribution type is Tensor.to_global . The to_global method contains two parameters- placement and sbp , which decide the newly-converted global data distribution type. The main implicit operations in converting the global data distribution type are to infer and execute the communications, and these operations are implemented by OneFlow\u2019s Boxing , which is a mechanism to re-distribute data automatically. The following is a case to convert a split-distributed Global Tensor to a broadcast-distributed one: import oneflow as flow x = flow . randn ( 2 , 5 ) . cuda () placement = flow . placement ( type = \"cuda\" , ranks = [ 0 , 1 ]) sbp = flow . sbp . split ( 0 ) x_global = x . to_global ( placement = placement , sbp = sbp ) print ( x_global . shape ) # (4, 5) print ( x_global . to_local ()) sbp_b = flow . sbp . broadcast x_global_b = x_global . to_global ( placement = placement , sbp = sbp_b ) print ( x_global_b . shape ) # (4, 5) print ( x_global_b . to_local ()) When the global data distribution type is converted from x_global to x_global_b , the parameter sbp has changed from flow.sbp.split(0) to flow.sbp.broadcast . Their global shapes have remained (4, 5) , but the local component has turned from a data slice into complete data, and this change can be seen from the printed result of the to_local() . Here, the to_global conversion has merged the Local Tensors. Generally speaking, SPMD programming mode requires users to write an all-gather collective communication to merge the Local Tensors, but in OneFlow Global View programming, the type conversion is enough to complete the merging process. Global Tensor\u2019s type conversion can infer and execute the communication operations automatically. So, algorithm developers can concentrate on thinking in data distribution rather than thinking in data communication operation , and what they imagine is what they obtain, which helps them to develop distributed programs more efficiently. Let\u2019s add by introducing how to apply numpy() to the Global Tensor. For random Global Tensor, such as x_global , x_global.numpy() is equivalent to x_global.to_global(spb=flow.sbp.broadcast).to_local().numpy() , which means x_global.numpy() will firstly convert the original Global Tensor to one, which SBP is flow.sbp.broadcast(), then conduct a to_local operation and finally invoke numpy() for the Local Tensor. Therefore, the x_global.numpy() method can obtain complete data. Global Tensor Participating in Computation \u00b6 This section introduces how the Global Tensor participates in practical computation. Take the Global Tensor participating in matrix multiplication computation for example, please firstly create the following program: import oneflow as flow placement = flow . placement ( type = \"cuda\" , ranks = [ 0 , 1 ]) x = flow . randn ( 4 , 5 , placement = placement , sbp = flow . sbp . split ( dim = 0 )) w = flow . randn ( 5 , 8 , placement = placement , sbp = flow . sbp . broadcast ) y = flow . matmul ( x , w ) print ( y . is_global ) # True print ( y . shape ) # (4, 8) print ( y . sbp ) # (flow.sbp.split(dim=0)) print ( y . to_local () . numpy ()) In the program above, we have created 2 Global Tensors- x and w , and they participate in oneflow.matmul computation and generate y . Most of OneFlow\u2019s operators support computing the Global Tensor. When flow.matmul executes the Global Tensor, there is nothing special about its interface. Arguably, most of OneFlow\u2019s operators are polymorphic, so they can decide how to compute according to the input: If the input of the operator is a Local Tensor, the operator will compute the tensor in normal single-device single-GPU execution mode. If the input of the operator is a Global Tensor, the operator will compute the tensor in global view (multi-device multi-GPU) mode. The operators supporting polymorphic execution are very convenient for users to change the single-GPU code into distributed code: they only need to convert the (Local) Tensor they accept to a Global Tensor. Just like single-device execution requires the data to be input into the same device, in the program above, the premise of the operator being executed successfully is that x and w have the same placement. The result of matrix multiplication- y is also a Global Tensor. When flow.matmul computes x and w , it will automatically infer the placement and SBP of the output data. The following are the principles: Placement: The input data and the output data have the same placement; SBP: The inference principle of the output data's SBP is decided by the operator type, and this principle is built into OneFlow. For more details, please refer to SBP Signature . Here, the multiplied result of flow.sbp.split(0) and flow.sbp.broadcast will be inferred as flow.sbp.split(0) . x is a data slice in each rank, w complete data, and y a data slice. Anyone familiar with common parallel execution approaches will find that a forward computation with data parallelism is conducted here. x is a data slice, and w the complete parameters. Conclusion \u00b6 This article has discussed: Global View offers the SPSD programming view; Global Tensor is visible on all processes when being executed; Global Tensor and Local Tensor are mutually convertible; Global Tensor supports converting the global data distribution type to implement distributed communication; OneFlow operators are polymorphic enough to enable the execution of the Global Tensor; So, this article will come to a close, and it fisrtly introduces how to create a Global Tensor and finally explains the detailed steps for data parallelism computation that is based on a Global Tensor. More about parallelism ways and SBP's inference logic will be discussed in our later articles. Further Reading \u00b6 OneFlow's multi-machine multi-GPU launching and its required environment variables \u00b6 OneFlow's Global Tensors are executed under ** Multi-Client mode**, which means each device corresponds to one process. For example, n Machine m GPU has n * m processes. Besides, each process has its own rank ID, which corresponds to the ranks of the Global Tensor's placement parameter. Take 2 Machines 2 GPUs for example, Machine 0 corresponds to GPU 0 and GPU 1, and Machine 1 corresponds to GPU 2 and GPU 3. So, flow.placement(type=\"cuda\", ranks=[2]) can only identify the GPU 0 on Machine 1. Generally, in the n Machine m GPU environment, flow.placement(type=\"cuda\", ranks=[k]) only identifies the GPU k % m on Machine k / n . Because the Multi-Client mode is adopted , we need to launch different processes corresponding to each device. In OneFlow, all processes need to launch the same scripts, and different processes distinguish process ID and establish communications according to different environment variables. Notes of environment variables: MASTER_ADDR \uff1athe IP of Machine 0 under multi-machine training; MASTER_PORT \uff1athe listening port of Machine 0 under multi-machine training, and this port shouldn\u2019t conflict with the occupied ports; WORLD_SIZE : the number of computing devices in the whole cluster. Because it\u2019s still not feasible to configure different number of GPUs on each device, the WORLD_SIZE equals the machine numbers multiplies the GPU numbers on each machine. In the previous case, we create the Global Tensor in single machine 2 GPUs environment, so the WORLD_SIZE=2 ; RANK \uff1athe process ID of all devices in the whole cluster; LOCAL_RANK \uff1athe process ID of single device; Differences between RANK and LOCAL_RANK : For single machine training, including single-machine single-GPU and single-machine multi-GPU, RANK equals to LOCAL_RANK ; For multi-machine training, the upper limit to LOCAL_RANK for each device is the number of computing devices on each machine; the upper limit to RANK is the sum of computing devices on all machines, and all devices are numbered from 0. (Because these computing devices are numbered from 0, the upper limit doesn\u2019t exist.) Take 2 Machines 2 GPUs for example, the corresponding relationship between LOCAL_RANK and RANK for each GPU is listed as follows: RANK LOCAL_RANK GPU 0 on Machine 0 0 0 GPU 0 on Machine 1 1 1 GPU 1 on Machine 0 2 0 GPU 1 on Machine 1 3 1 Although it is complicated to utilize environment variables launching, this approach is widely applicable because users can adopt random ways to launch the processes. Besides, OneFlow also offers a convenient tool, oneflow.distributed.launch , to help users launch multiple processes in a distributed way and construct environment variables automatically.","title":"Basic Operations for Using Global Tensor to Program on Cluster"},{"location":"cookies/global_tensor.html#using-global-tensor-to-program-on-multi-device-multi-gpu-basic-operations","text":"By YaoChi , Xu Xiaoyu , Zuo Yihao , Guoliang Cheng , Shen Jiali Global tensor can be executed on multi-device multi-GPU, and it\u2019s an interface to implement the Global View programming. Today, most parallel programs adopt the SPMD (Single program, multiple data) programming method, which means the devices will execute the same program but process different parts of the data to realize data parallelism. Take PyTorch\u2019s DDP (Distributed Data Parallel) for example, each process executes the same neural network computing logic, but the difference is that they load different slices of one dataset. But, the defect of SPMD programming is that multiple data makes communications more complicated. In a deep learning scenario, SPMD programming needs to insert communication operations into original computing codes, such as AllReduce for data parallelism and AllGather/ReduceScatter for model parallelism. If the parallel mode is much more complicated or a new mode needs to be experimented with, it will be troublesome to develop and maintain after inserting the communication operations. Global View programming permits users to program from the SPSD view. Different from SPMD programming, SPSD programming is a method that data is also single from the programming interface layer. When we extend a single-process program to a parallelly executed one, the single-process data will also be extended to the multi-process data, so it's natural that the data on different processes corresponds to the same logic data on the originally single-process program. And the logic data is called Global Tensor in OneFlow. Global Tensor supports users to utilize the SPSD interface to program, which means users can program on a single device and OneFlow framework will automatically convert to physical SPMD/MPMD mode and execute the program in a parallel/distributed way. With Global Tensor, a more naturally Global View programming method is available, and users can regard the multi-devices as a single device to implement SPSD programming.","title":"Using Global Tensor to Program on Multi-Device Multi-GPU: Basic Operations"},{"location":"cookies/global_tensor.html#global-tensor","text":"In programming languages, \"Global\" usually refers to in-process global visibility, such as Global Variable . Instead, the \"Global\" of the \"Global Tensor\" means inter-process global visibility. So, it\u2019s more accurate to regard the Global Tensor as a tensor that can be seen on all processes. Global Tensor exists on all processes. When the tensor is executed by an operator on all processes, it will be automatically executed on multi-device multi-GPU. At present, the commonly-used tensor is only visible on one process and also exists on a single device. OneFlow calls it the Local Tensor, which means it\u2019s a tensor that can be seen on only one process. Local is relative to Global, so Local Tensor can be considered as Local (on one process) Tensor. Most of OneFlow\u2019s operators are compatible with the execution of Local Tensors and Global Tensors. It\u2019s convenient to convert the Local Tensor to the Global Tensor, so the code originally executed on single-device single-GPU can be smoothly converted to ones that can be executed on multi-device multi-GPU. Global Tensor allows users to easily develop models on multi-device multi-GPU. Compared to utilizing the original communication operators, the efficiency of developing parallelly executed models will be doubled.","title":"Global Tensor"},{"location":"cookies/global_tensor.html#creating-global-tensor","text":"Let\u2019s try to create a Global Tensor on a machine with two GPUs. Take randn operator for example, a Python file named test_randn_global.py needs to be created and add the following content to it: import oneflow as flow # Place a global tensor on cuda device of rank(process) 0 and 1 placement = flow . placement ( type = \"cuda\" , ranks = [ 0 , 1 ]) # Each rank's local data is a part data as a result of spliting global data on dim 0 sbp = flow . sbp . split ( dim = 0 ) # Create a global tensor by randn x = flow . randn ( 4 , 5 , placement = placement , sbp = sbp ) # Print local data print ( \"Local data of global tensor: \\n \" , x . to_local () . numpy ()) # Print global data print ( \"Global data of global tensor: \\n \" , x . numpy ()) Here are some explanations for some new concepts in the code above: placement refers to the physical device where the Global Tensor locates. The parameter type specifies the type of the physical device, and here we use \"cuda\" to represent the GPU device. The parameter ranks specifies the device ID. For readers who don\u2019t have 2 GPUs, the parameter type can be specified as \"cpu\" to use the CPU to simulate multiple devices, and the following code still works. sbp refers to the distributed way of the Global Tensor. Here, sbp = flow.sbp.split(dim=0) means that the Global Tensor is evenly split along dimension 0. The to_local() method is to acquire the Local Tensor in the present rank from the Global Tensor because the Global Tensor has one Local Tensor in each rank as its practically existing local component. Next, configure the environment variables required by multi-process launching. Here, the machine owns 2 GPUs, which correspond to 2 process launchings. So, we should turn on 2 terminals and respectively configure the following environment variables: Note Clicking the label \"Terminal 0\" or \"Terminal 1\" separately to check its corresponding console\u2019s command/code. Terminal 0 export MASTER_ADDR = 127 .0.0.1 MASTER_PORT = 17789 WORLD_SIZE = 2 RANK = 0 LOCAL_RANK = 0 Terminal 1 export MASTER_ADDR = 127 .0.0.1 MASTER_PORT = 17789 WORLD_SIZE = 2 RANK = 1 LOCAL_RANK = 1 More about detailed explanation of the environment variables above and how to conduct a distributed launching with the help of tools, please refer to Further reading . Finally, launch test_randn_global.py in two terminals respectively and observe the results of creating the Global Tensor: python3 test_randn_global.py In Terminal 0 (rank 0), we can see: Local data of global tensor: [[-0.07157125 -0.92717147 1.5102768 1.4611115 1.014263 ] [-0.1511031 1.570759 0.9416077 0.6184639 2.4420679 ]] Global data of global tensor: [[-0.07157125 -0.92717147 1.5102768 1.4611115 1.014263 ] [-0.1511031 1.570759 0.9416077 0.6184639 2.4420679 ] [-0.38203463 0.453836 0.9136015 2.35773 -0.3279942 ] [-0.8570119 -0.91476554 -0.06646168 0.50022084 -0.4387695 ]] In Terminal 1 (rank 1), we can see: Local data of global tensor: [[-0.38203463 0.453836 0.9136015 2.35773 -0.3279942 ] [-0.8570119 -0.91476554 -0.06646168 0.50022084 -0.4387695 ]] Global data of global tensor: [[-0.07157125 -0.92717147 1.5102768 1.4611115 1.014263 ] [-0.1511031 1.570759 0.9416077 0.6184639 2.4420679 ] [-0.38203463 0.453836 0.9136015 2.35773 -0.3279942 ] [-0.8570119 -0.91476554 -0.06646168 0.50022084 -0.4387695 ]] It\u2019s clear that if we concatenate the Local Tensors in rank 1 and rank 2 on dimension 0, we can get the complete value of the Global Tensor.","title":"Creating Global Tensor"},{"location":"cookies/global_tensor.html#converting-local-tensor-to-global-tensor","text":"We can firstly create a Local Tensor and then utilize the Tensor.to_global method to convert the Local Tensor to a Global Tensor. Create the following program and launch it in the similar way mentioned above: import oneflow as flow x = flow . randn ( 2 , 5 ) . cuda () print ( x . is_local ) # True print ( x . is_global ) # False placement = flow . placement ( type = \"cuda\" , ranks = [ 0 , 1 ]) sbp = flow . sbp . split ( 0 ) x_global = x . to_global ( placement = placement , sbp = sbp ) print ( x_global . shape ) # (4, 5) print ( x . is_local ) # True print ( x_global . is_global ) # True This program separately creates a Local Tensor with the shape of (2,5) on 2 GPUs, and the newly-created tensors are called x. Then, we specify cuda devices in rank 0 and rank 1 as the placement and split(dim=0) as its SBP. After the to_global method, the original Local Tensor is converted to the Global Tensor named x_global . We can see that the shape of x_global has been changed into (4, 5) , which is the same as the (global) shape of the Global Tensor. The relationship between the Global Tensor and the Local Tensor is the total and the component, and the Local Tensor is the component of the total in a certain rank. The specific relationship between the Global Tensor and the Local Tensor is decided by the placement and SBP. For example, in the above case, the relationship is between tensors on GPU 0 and GPU 1, and we split x_global along dimension 0 to get x . Based on the above relationship, the to_global method can infer x_global.shape according to x.shape : it concatenates the Local Tensor x on 2 GPUs along dimension 0 to obtain x_global . Except for shape, the Global Tensor also contains some data. The Global Tensor has a Local Tensor in each rank to symbolize its local component, which is its physical data in every rank. By the way, each rank only stores different parts of the data.","title":"Converting Local Tensor to Global Tensor"},{"location":"cookies/global_tensor.html#converting-global-tensor-to-local-tensor","text":"You can utilize the to_local method to obtain the local component of the Global Tensor, just like the following: import oneflow as flow placement = flow . placement ( type = \"cuda\" , ranks = [ 0 , 1 ]) sbp = flow . sbp . split ( 0 ) x = flow . randn ( 4 , 5 , placement = placement , sbp = sbp ) print ( x . to_local ()) When the x.to_local() method is executed, two different ranks will separately obtain a Local Tensor with the shape of (2, 5) . In Terminal 0 (rank 0), we can see: tensor([[-0.2730, 1.8042, 0.0721, -0.5024, -1.2583], [-0.3379, 0.9371, 0.7981, -0.5447, -0.5629]], dtype=oneflow.float32) In Terminal 1 (rank 1), we can see: tensor([[ 0.6829, 0.4849, 2.1611, 1.4059, 0.0934], [-0.0301, -0.6942, -0.8094, -1.3050, -0.1778]], dtype=oneflow.float32) The to_local() has no parameters, because the Global Tensor has already confirmed its local component according to the placement and SBP, and it\u2019s fine to directly acquire the Local Tensor that its local component corresponds to.","title":"Converting Global Tensor to Local Tensor"},{"location":"cookies/global_tensor.html#converting-one-global-tensor-to-another-global-tensor","text":"Usually, distributed computing requires inserting communication operations into normal computational logic, but OneFlow only needs users to convert the data distribution type of the Global Tensor. In terms of type, the biggest difference between the Global Tensor and the general Local Tensor is that the Global Tensor has global data distribution type, which specifies how the Global Tensor is distributed in each rank, including its placement and SBP. The function of placement in global data distribution type is to specify the device group where data is distributed: The parameter type specifies the physical device type. cuda represents the GPU device memory, and cpu` refers to the CPU device memory. The parameter ranks specifies the process ID set. Because each rank corresponds to one physical device, ranks can also be seen as the device ID set. Actually, ranks is an nd-array composed of rank ID, which supports high-dimensional device arrangement. For more details, please refer to oneflow.placement . The function of SBP in the global data distribution type is to specify the relationship between global data and local data: S, i.e., split(dim), notes that the relationship between global data and local data is split, indicating the global data is evenly split according to the dimension dim and distributed in each rank. B, i.e., broadcast, notes that the relationship between global data and local data is broadcast, indicating the global data is replicated in each rank. P, i.e., partial_sum, notes that the relationship between global data and local data is partial, indicating the value of the global data is the element-wise sum of the local data distributed in each rank. For more details, please refer to oneflow.sbp.sbp . Data re-distribution is commonly seen in parallel computing, i.e., changing the distributed way of data, such as gathering all data slices. In the MPI programming paradigm (SPMD), data re-distribution requires writing explicit communication operations like AllReduce, AllGather, and ReduceScatter. But in OneFlow\u2019s Global View programming paradigm (SPSD), data re-distribution can be achieved by utilizing Global Tensor\u2019s global data distribution type conversion. The conversion of the global data distribution type is similar to (explicit) type conversion in general programming languages. Users only need to specify the targeted type when they convert types, and some implicit operations can be executed automatically. For example, when converting the type from double to int, the system will remove the decimal point automatically. Similarly, it\u2019s only required to specify the new global data distribution type that the Global Tensor will be converted into, and OneFlow will complete implicit communication operations automatically. And the interface to convert the global data distribution type is Tensor.to_global . The to_global method contains two parameters- placement and sbp , which decide the newly-converted global data distribution type. The main implicit operations in converting the global data distribution type are to infer and execute the communications, and these operations are implemented by OneFlow\u2019s Boxing , which is a mechanism to re-distribute data automatically. The following is a case to convert a split-distributed Global Tensor to a broadcast-distributed one: import oneflow as flow x = flow . randn ( 2 , 5 ) . cuda () placement = flow . placement ( type = \"cuda\" , ranks = [ 0 , 1 ]) sbp = flow . sbp . split ( 0 ) x_global = x . to_global ( placement = placement , sbp = sbp ) print ( x_global . shape ) # (4, 5) print ( x_global . to_local ()) sbp_b = flow . sbp . broadcast x_global_b = x_global . to_global ( placement = placement , sbp = sbp_b ) print ( x_global_b . shape ) # (4, 5) print ( x_global_b . to_local ()) When the global data distribution type is converted from x_global to x_global_b , the parameter sbp has changed from flow.sbp.split(0) to flow.sbp.broadcast . Their global shapes have remained (4, 5) , but the local component has turned from a data slice into complete data, and this change can be seen from the printed result of the to_local() . Here, the to_global conversion has merged the Local Tensors. Generally speaking, SPMD programming mode requires users to write an all-gather collective communication to merge the Local Tensors, but in OneFlow Global View programming, the type conversion is enough to complete the merging process. Global Tensor\u2019s type conversion can infer and execute the communication operations automatically. So, algorithm developers can concentrate on thinking in data distribution rather than thinking in data communication operation , and what they imagine is what they obtain, which helps them to develop distributed programs more efficiently. Let\u2019s add by introducing how to apply numpy() to the Global Tensor. For random Global Tensor, such as x_global , x_global.numpy() is equivalent to x_global.to_global(spb=flow.sbp.broadcast).to_local().numpy() , which means x_global.numpy() will firstly convert the original Global Tensor to one, which SBP is flow.sbp.broadcast(), then conduct a to_local operation and finally invoke numpy() for the Local Tensor. Therefore, the x_global.numpy() method can obtain complete data.","title":"Converting One Global Tensor to Another Global Tensor"},{"location":"cookies/global_tensor.html#global-tensor-participating-in-computation","text":"This section introduces how the Global Tensor participates in practical computation. Take the Global Tensor participating in matrix multiplication computation for example, please firstly create the following program: import oneflow as flow placement = flow . placement ( type = \"cuda\" , ranks = [ 0 , 1 ]) x = flow . randn ( 4 , 5 , placement = placement , sbp = flow . sbp . split ( dim = 0 )) w = flow . randn ( 5 , 8 , placement = placement , sbp = flow . sbp . broadcast ) y = flow . matmul ( x , w ) print ( y . is_global ) # True print ( y . shape ) # (4, 8) print ( y . sbp ) # (flow.sbp.split(dim=0)) print ( y . to_local () . numpy ()) In the program above, we have created 2 Global Tensors- x and w , and they participate in oneflow.matmul computation and generate y . Most of OneFlow\u2019s operators support computing the Global Tensor. When flow.matmul executes the Global Tensor, there is nothing special about its interface. Arguably, most of OneFlow\u2019s operators are polymorphic, so they can decide how to compute according to the input: If the input of the operator is a Local Tensor, the operator will compute the tensor in normal single-device single-GPU execution mode. If the input of the operator is a Global Tensor, the operator will compute the tensor in global view (multi-device multi-GPU) mode. The operators supporting polymorphic execution are very convenient for users to change the single-GPU code into distributed code: they only need to convert the (Local) Tensor they accept to a Global Tensor. Just like single-device execution requires the data to be input into the same device, in the program above, the premise of the operator being executed successfully is that x and w have the same placement. The result of matrix multiplication- y is also a Global Tensor. When flow.matmul computes x and w , it will automatically infer the placement and SBP of the output data. The following are the principles: Placement: The input data and the output data have the same placement; SBP: The inference principle of the output data's SBP is decided by the operator type, and this principle is built into OneFlow. For more details, please refer to SBP Signature . Here, the multiplied result of flow.sbp.split(0) and flow.sbp.broadcast will be inferred as flow.sbp.split(0) . x is a data slice in each rank, w complete data, and y a data slice. Anyone familiar with common parallel execution approaches will find that a forward computation with data parallelism is conducted here. x is a data slice, and w the complete parameters.","title":"Global Tensor Participating in Computation"},{"location":"cookies/global_tensor.html#conclusion","text":"This article has discussed: Global View offers the SPSD programming view; Global Tensor is visible on all processes when being executed; Global Tensor and Local Tensor are mutually convertible; Global Tensor supports converting the global data distribution type to implement distributed communication; OneFlow operators are polymorphic enough to enable the execution of the Global Tensor; So, this article will come to a close, and it fisrtly introduces how to create a Global Tensor and finally explains the detailed steps for data parallelism computation that is based on a Global Tensor. More about parallelism ways and SBP's inference logic will be discussed in our later articles.","title":"Conclusion"},{"location":"cookies/global_tensor.html#further-reading","text":"","title":"Further Reading"},{"location":"cookies/global_tensor.html#oneflows-multi-machine-multi-gpu-launching-and-its-required-environment-variables","text":"OneFlow's Global Tensors are executed under ** Multi-Client mode**, which means each device corresponds to one process. For example, n Machine m GPU has n * m processes. Besides, each process has its own rank ID, which corresponds to the ranks of the Global Tensor's placement parameter. Take 2 Machines 2 GPUs for example, Machine 0 corresponds to GPU 0 and GPU 1, and Machine 1 corresponds to GPU 2 and GPU 3. So, flow.placement(type=\"cuda\", ranks=[2]) can only identify the GPU 0 on Machine 1. Generally, in the n Machine m GPU environment, flow.placement(type=\"cuda\", ranks=[k]) only identifies the GPU k % m on Machine k / n . Because the Multi-Client mode is adopted , we need to launch different processes corresponding to each device. In OneFlow, all processes need to launch the same scripts, and different processes distinguish process ID and establish communications according to different environment variables. Notes of environment variables: MASTER_ADDR \uff1athe IP of Machine 0 under multi-machine training; MASTER_PORT \uff1athe listening port of Machine 0 under multi-machine training, and this port shouldn\u2019t conflict with the occupied ports; WORLD_SIZE : the number of computing devices in the whole cluster. Because it\u2019s still not feasible to configure different number of GPUs on each device, the WORLD_SIZE equals the machine numbers multiplies the GPU numbers on each machine. In the previous case, we create the Global Tensor in single machine 2 GPUs environment, so the WORLD_SIZE=2 ; RANK \uff1athe process ID of all devices in the whole cluster; LOCAL_RANK \uff1athe process ID of single device; Differences between RANK and LOCAL_RANK : For single machine training, including single-machine single-GPU and single-machine multi-GPU, RANK equals to LOCAL_RANK ; For multi-machine training, the upper limit to LOCAL_RANK for each device is the number of computing devices on each machine; the upper limit to RANK is the sum of computing devices on all machines, and all devices are numbered from 0. (Because these computing devices are numbered from 0, the upper limit doesn\u2019t exist.) Take 2 Machines 2 GPUs for example, the corresponding relationship between LOCAL_RANK and RANK for each GPU is listed as follows: RANK LOCAL_RANK GPU 0 on Machine 0 0 0 GPU 0 on Machine 1 1 1 GPU 1 on Machine 0 2 0 GPU 1 on Machine 1 3 1 Although it is complicated to utilize environment variables launching, this approach is widely applicable because users can adopt random ways to launch the processes. Besides, OneFlow also offers a convenient tool, oneflow.distributed.launch , to help users launch multiple processes in a distributed way and construct environment variables automatically.","title":"OneFlow's multi-machine multi-GPU launching and its required environment variables"},{"location":"cookies/one_embedding.html","text":"Large-Scale Embedding Solution: OneEmbedding \u00b6 Embedding is an important component of recommender system, and it has also spread to many fields outside recommender systems. Each framework provides basic operators for Embedding, for example, flow.nn.Embedding in OneFlow: import numpy as np import oneflow as flow indices = flow . tensor ([[ 1 , 2 , 4 , 5 ], [ 4 , 3 , 2 , 9 ]], dtype = flow . int ) embedding = flow . nn . Embedding ( 10 , 3 ) y = embedding ( indices ) OneEmbedding is the large-scale Embedding solution that OneFlow provides to solve the problem of large-scale deep recommender systems. OneEmbedding has the following advantages compared to ordionary opeartors: With Flexible hierarchical storage, OneEmbedding can place the Embedding table on GPU memory, CPU memory or SSD, and allow high-speed devices to be used as caches for low-speed devices to achieve both speed and capacity. OneEmbedding supports dynamic expansion. Get Start to OneEmbedding Quickly \u00b6 The following steps is an example of getting started with OneEmbeeding quickly: Configure Embedding table with make_table_options Configure the storage attribute of the Embedding table Instantiate Embedding Construct Graph for training Configure Embedding Table with make_table_options \u00b6 By importing relevant package and the following codes, you can configure Embedding table with make_table_options .OneEmbedding supports simultaneous creation of multiple Embedding table. The following codes configured three Embedding table. import oneflow as flow import oneflow.nn as nn import numpy as np tables = [ flow . one_embedding . make_table_options ( flow . one_embedding . make_uniform_initializer ( low =- 0.1 , high = 0.1 ) ), flow . one_embedding . make_table_options ( flow . one_embedding . make_uniform_initializer ( low =- 0.05 , high = 0.05 ) ), flow . one_embedding . make_table_options ( flow . one_embedding . make_uniform_initializer ( low =- 0.15 , high = 0.15 ) ), ] When configuring the Embedding table, you need to specify the initialization method. The above Embedding tables are initialized in the uniform method. The result of configuring the Embedding table is stored in the tables variable Click make_table_options and make_uniform_initializer to check more detailed information. Configure the Storage Attribute of the Embedding Table \u00b6 Then run the following codes to configure the storage attribute of the Embedding table: store_options = flow . one_embedding . make_cached_ssd_store_options ( cache_budget_mb = 8142 , persistent_path = \"/your_path_to_ssd\" , capacity = 40000000 , size_factor = 1 , physical_block_size = 512 ) By calling make_cached_ssd_store_options here, you can store Embedding table on SSD and use GPU as cache. For the meaning of specific parameters, please refer to make_cached_ssd_store_options API \u6587\u6863 . In addition, you can use pure GPU as storage; or use CPU memory to store Embedding table, but use GPU as cache. For more details, please refer to make_device_mem_store_options and make_cached_host_mem_store_option . Instantiate Embedding \u00b6 After the above configuration is completed, you can use MultiTableEmbedding to get the instantiated Embedding layer. embedding_size = 128 embedding = flow . one_embedding . MultiTableEmbedding ( name = \"my_embedding\" , embedding_dim = embedding_size , dtype = flow . float , key_type = flow . int64 , tables = tables , store_options = store_options , ) embedding . to ( \"cuda\" ) Among them, tables is the Embedding table attribute previously configured by make_table_options , store_options is the previously configured storage attribute, embedding_dim is the feature dimension, dtype is the data type of the feature vector, key_type is the data type of feature ID. If two OneEmbeddings are created at the same time, different name and persistent path parameters are needed to be set during instantiation. For more detailes, please refer to one_embedding.MultiTableEmbedding . Construct Graph for Training \u00b6 Currently, OneEmbedding is only supported in Graph mode. In the following example, we construct a simple Graph class that includes embedding and mlp layers. num_tables = 3 mlp = flow . nn . FusedMLP ( in_features = embedding_size * num_tables , hidden_features = [ 512 , 256 , 128 ], out_features = 1 , skip_final_activation = True , ) mlp . to ( \"cuda\" ) class TrainGraph ( flow . nn . Graph ): def __init__ ( self ,): super () . __init__ () self . embedding_lookup = embedding self . mlp = mlp self . add_optimizer ( flow . optim . SGD ( self . embedding_lookup . parameters (), lr = 0.1 , momentum = 0.0 ) ) self . add_optimizer ( flow . optim . SGD ( self . mlp . parameters (), lr = 0.1 , momentum = 0.0 ) ) def build ( self , ids ): embedding = self . embedding_lookup ( ids ) loss = self . mlp ( flow . reshape ( embedding , ( - 1 , num_tables * embedding_size ))) loss = loss . sum () loss . backward () return loss Then you can instantiate the Graph and start training. ids = np . random . randint ( 0 , 1000 , ( 100 , num_tables ), dtype = np . int64 ) ids_tensor = flow . tensor ( ids , requires_grad = False ) . to ( \"cuda\" ) graph = TrainGraph () loss = graph ( ids_tensor ) print ( loss ) For the detailed information on using Graph, please refer to \u9759\u6001\u56fe\u6a21\u5757 nn.Graph . The Features of OneEmbedding \u00b6 Feature ID and Dynamic Insertion \u00b6 OneEmbedding supports dynamic insertion of new feature ID. As long as the storage medium has sufficient capacity, there is no upper limit on the number of feature IDs. This is why when you use make_table_options , you only need to specify the initialization method, not the total number of feature IDs (Embedding table lines). Feature ID and Multi-Table Query \u00b6 Feature ID cannot be repeated OneEmbedding users who make datasets need to pay special attention: When using MultiTableEmbedding to create multiple tables at the same time,multiple Embedding Tables only have different initialization parameters, and other parameters are the same,at this time, feature IDs in multiple tables cannot be repeated . Multi-table query The query method is no different from the normal Embedding query method if you only use MultiTableEmbedding to configure one table. You can call it directly and pass the feature ID, such as embedding_lookup(ids) . If you use MultiTableEmbedding to configure more than one tables, then you need to specify in which to query for a feature ID with the following two methods: Method 1: Pass an ids of shape (batch_size, number of Embedding table) for query, then the column of this ids corresponds to a Embedding table in turn. For example: ids = np . array ([[ 488 , 333 , 220 ], [ 18 , 568 , 508 ]], dtype = np . int64 ) # This means to query `[[488], [18]]` in the zeroth table, `[[333], [568]]` in the first table, and the corresponding feature vector of `[[220], [508]]` in the second table. embedding_lookup ( ids ) Method 2:When passing the ids parameter, pass a table_ids parameter, which has the exact same shape as ids , and specifies the ordinal number of the table in table_ids . For example: ids = np . array ([ 488 , 333 , 220 , 18 , 568 , 508 ], dtype = np . int64 ) # table_ids has the exact same shape as `ids` table_ids = np . array ([ 0 , 1 , 2 , 0 , 1 , 2 ]) # This means to query `488, 18` in the zeroth table, `333, 568` in the first table, and the corresponding feature vector of `220, 508` in the second table. embedding_lookup ( ids , table_ids ) For more details, please refer to MultiTableEmbedding.forward . How to Choose the Proper Storage Configuration \u00b6 OneEmbedding provides three storage options configurations,they are pure GPU storage, use CPU memory to store and GPU memory as cache and use SSD to store and GPU memory as cache. Pure GPU storage When the size of Embedding table is smaller than the GPU memory, it is the fastest to place all the Embedding table on the GPU memory. In this case, it is recommended to select the pure GPU storage configuration. Use CPU memory to store and GPU memory as cache When the size of Embedding table is larger than the GPU memory, but smaller than the CPU memory, it is recommended to store the Embedding table in the CPU memory and use the GPU memory as cache. Use SSD to store and GPU memory as cache When the size of Embedding table is larger than both the GPU memory and the system memory, if you have a high-speed SSD, you can choose to store the Embedding table in the SSD and use the GPU memory as a cache. In this case, frequent data reading and writing will be performed on the stored vocabulary during the training process, so the random reading and writing speed of files under the path set by persistent_path has a great impact on the overall performance. It is strongly recommended to use a high-performance SSD. If you use a normal disk, it will have a great negative impact on the overall performance. Distributed Training \u00b6 Similar to other modules of OneFlow, OneEmbedding supports distributed expansion natively. Users can refer to the README in #dlrm to start DLRM distributed training. You can also refer to Global Tensor for necessary prerequisites. When using the OneEmbedding module for distributed expansion, please be careful: Currently, OneEmbedding only supports placement on all devices, and the parallelism must be the same as the world size. For example, when training with 4 cards in parallel, the parallelism of the Embedding table must be 4. It is not supported when the network is trained with 4 cards but the Embedding table parallelism is 2. The persistent_path parameter in the store_options configuration specifies the path of the storage. In parallel scenarios, it can be either a string representing a path or a list . If configured as a string representing a path, it represents the root directory under each rank in distributed parallelism. OneFlow will create a storage path based on the number of each rank under this root path, and the name format is rank_id-num_rank . If persistent_path is a list , rank will be configured individually for each item in the list. In parallel scenarios, the capacity in the store_options configuration represents the capacity of total Embedding table, but not the capacity of each rank. cache_budget_mb represents the video memory per GPU device. Extended Reading: DLRM \u00b6 This article shows how to get started with OneEmbedding quickly. Practical examples of OneEmbedding in DLRM tasks are prepared in the OneFlow model repository, please refer to https://github.com/Oneflow-Inc/models/tree/main/RecommenderSystems/dlrm","title":"Large-Scale Embedding Solution OneEmbedding"},{"location":"cookies/one_embedding.html#large-scale-embedding-solution-oneembedding","text":"Embedding is an important component of recommender system, and it has also spread to many fields outside recommender systems. Each framework provides basic operators for Embedding, for example, flow.nn.Embedding in OneFlow: import numpy as np import oneflow as flow indices = flow . tensor ([[ 1 , 2 , 4 , 5 ], [ 4 , 3 , 2 , 9 ]], dtype = flow . int ) embedding = flow . nn . Embedding ( 10 , 3 ) y = embedding ( indices ) OneEmbedding is the large-scale Embedding solution that OneFlow provides to solve the problem of large-scale deep recommender systems. OneEmbedding has the following advantages compared to ordionary opeartors: With Flexible hierarchical storage, OneEmbedding can place the Embedding table on GPU memory, CPU memory or SSD, and allow high-speed devices to be used as caches for low-speed devices to achieve both speed and capacity. OneEmbedding supports dynamic expansion.","title":"Large-Scale Embedding Solution: OneEmbedding"},{"location":"cookies/one_embedding.html#get-start-to-oneembedding-quickly","text":"The following steps is an example of getting started with OneEmbeeding quickly: Configure Embedding table with make_table_options Configure the storage attribute of the Embedding table Instantiate Embedding Construct Graph for training","title":"Get Start to OneEmbedding Quickly"},{"location":"cookies/one_embedding.html#configure-embedding-table-with-make_table_options","text":"By importing relevant package and the following codes, you can configure Embedding table with make_table_options .OneEmbedding supports simultaneous creation of multiple Embedding table. The following codes configured three Embedding table. import oneflow as flow import oneflow.nn as nn import numpy as np tables = [ flow . one_embedding . make_table_options ( flow . one_embedding . make_uniform_initializer ( low =- 0.1 , high = 0.1 ) ), flow . one_embedding . make_table_options ( flow . one_embedding . make_uniform_initializer ( low =- 0.05 , high = 0.05 ) ), flow . one_embedding . make_table_options ( flow . one_embedding . make_uniform_initializer ( low =- 0.15 , high = 0.15 ) ), ] When configuring the Embedding table, you need to specify the initialization method. The above Embedding tables are initialized in the uniform method. The result of configuring the Embedding table is stored in the tables variable Click make_table_options and make_uniform_initializer to check more detailed information.","title":"Configure Embedding Table with make_table_options"},{"location":"cookies/one_embedding.html#configure-the-storage-attribute-of-the-embedding-table","text":"Then run the following codes to configure the storage attribute of the Embedding table: store_options = flow . one_embedding . make_cached_ssd_store_options ( cache_budget_mb = 8142 , persistent_path = \"/your_path_to_ssd\" , capacity = 40000000 , size_factor = 1 , physical_block_size = 512 ) By calling make_cached_ssd_store_options here, you can store Embedding table on SSD and use GPU as cache. For the meaning of specific parameters, please refer to make_cached_ssd_store_options API \u6587\u6863 . In addition, you can use pure GPU as storage; or use CPU memory to store Embedding table, but use GPU as cache. For more details, please refer to make_device_mem_store_options and make_cached_host_mem_store_option .","title":"Configure the Storage Attribute of the Embedding Table"},{"location":"cookies/one_embedding.html#instantiate-embedding","text":"After the above configuration is completed, you can use MultiTableEmbedding to get the instantiated Embedding layer. embedding_size = 128 embedding = flow . one_embedding . MultiTableEmbedding ( name = \"my_embedding\" , embedding_dim = embedding_size , dtype = flow . float , key_type = flow . int64 , tables = tables , store_options = store_options , ) embedding . to ( \"cuda\" ) Among them, tables is the Embedding table attribute previously configured by make_table_options , store_options is the previously configured storage attribute, embedding_dim is the feature dimension, dtype is the data type of the feature vector, key_type is the data type of feature ID. If two OneEmbeddings are created at the same time, different name and persistent path parameters are needed to be set during instantiation. For more detailes, please refer to one_embedding.MultiTableEmbedding .","title":"Instantiate Embedding"},{"location":"cookies/one_embedding.html#construct-graph-for-training","text":"Currently, OneEmbedding is only supported in Graph mode. In the following example, we construct a simple Graph class that includes embedding and mlp layers. num_tables = 3 mlp = flow . nn . FusedMLP ( in_features = embedding_size * num_tables , hidden_features = [ 512 , 256 , 128 ], out_features = 1 , skip_final_activation = True , ) mlp . to ( \"cuda\" ) class TrainGraph ( flow . nn . Graph ): def __init__ ( self ,): super () . __init__ () self . embedding_lookup = embedding self . mlp = mlp self . add_optimizer ( flow . optim . SGD ( self . embedding_lookup . parameters (), lr = 0.1 , momentum = 0.0 ) ) self . add_optimizer ( flow . optim . SGD ( self . mlp . parameters (), lr = 0.1 , momentum = 0.0 ) ) def build ( self , ids ): embedding = self . embedding_lookup ( ids ) loss = self . mlp ( flow . reshape ( embedding , ( - 1 , num_tables * embedding_size ))) loss = loss . sum () loss . backward () return loss Then you can instantiate the Graph and start training. ids = np . random . randint ( 0 , 1000 , ( 100 , num_tables ), dtype = np . int64 ) ids_tensor = flow . tensor ( ids , requires_grad = False ) . to ( \"cuda\" ) graph = TrainGraph () loss = graph ( ids_tensor ) print ( loss ) For the detailed information on using Graph, please refer to \u9759\u6001\u56fe\u6a21\u5757 nn.Graph .","title":"Construct Graph for Training"},{"location":"cookies/one_embedding.html#the-features-of-oneembedding","text":"","title":"The Features of OneEmbedding"},{"location":"cookies/one_embedding.html#feature-id-and-dynamic-insertion","text":"OneEmbedding supports dynamic insertion of new feature ID. As long as the storage medium has sufficient capacity, there is no upper limit on the number of feature IDs. This is why when you use make_table_options , you only need to specify the initialization method, not the total number of feature IDs (Embedding table lines).","title":"Feature ID and Dynamic Insertion"},{"location":"cookies/one_embedding.html#feature-id-and-multi-table-query","text":"Feature ID cannot be repeated OneEmbedding users who make datasets need to pay special attention: When using MultiTableEmbedding to create multiple tables at the same time,multiple Embedding Tables only have different initialization parameters, and other parameters are the same,at this time, feature IDs in multiple tables cannot be repeated . Multi-table query The query method is no different from the normal Embedding query method if you only use MultiTableEmbedding to configure one table. You can call it directly and pass the feature ID, such as embedding_lookup(ids) . If you use MultiTableEmbedding to configure more than one tables, then you need to specify in which to query for a feature ID with the following two methods: Method 1: Pass an ids of shape (batch_size, number of Embedding table) for query, then the column of this ids corresponds to a Embedding table in turn. For example: ids = np . array ([[ 488 , 333 , 220 ], [ 18 , 568 , 508 ]], dtype = np . int64 ) # This means to query `[[488], [18]]` in the zeroth table, `[[333], [568]]` in the first table, and the corresponding feature vector of `[[220], [508]]` in the second table. embedding_lookup ( ids ) Method 2:When passing the ids parameter, pass a table_ids parameter, which has the exact same shape as ids , and specifies the ordinal number of the table in table_ids . For example: ids = np . array ([ 488 , 333 , 220 , 18 , 568 , 508 ], dtype = np . int64 ) # table_ids has the exact same shape as `ids` table_ids = np . array ([ 0 , 1 , 2 , 0 , 1 , 2 ]) # This means to query `488, 18` in the zeroth table, `333, 568` in the first table, and the corresponding feature vector of `220, 508` in the second table. embedding_lookup ( ids , table_ids ) For more details, please refer to MultiTableEmbedding.forward .","title":"Feature ID and Multi-Table Query"},{"location":"cookies/one_embedding.html#how-to-choose-the-proper-storage-configuration","text":"OneEmbedding provides three storage options configurations,they are pure GPU storage, use CPU memory to store and GPU memory as cache and use SSD to store and GPU memory as cache. Pure GPU storage When the size of Embedding table is smaller than the GPU memory, it is the fastest to place all the Embedding table on the GPU memory. In this case, it is recommended to select the pure GPU storage configuration. Use CPU memory to store and GPU memory as cache When the size of Embedding table is larger than the GPU memory, but smaller than the CPU memory, it is recommended to store the Embedding table in the CPU memory and use the GPU memory as cache. Use SSD to store and GPU memory as cache When the size of Embedding table is larger than both the GPU memory and the system memory, if you have a high-speed SSD, you can choose to store the Embedding table in the SSD and use the GPU memory as a cache. In this case, frequent data reading and writing will be performed on the stored vocabulary during the training process, so the random reading and writing speed of files under the path set by persistent_path has a great impact on the overall performance. It is strongly recommended to use a high-performance SSD. If you use a normal disk, it will have a great negative impact on the overall performance.","title":"How to Choose the Proper Storage Configuration"},{"location":"cookies/one_embedding.html#distributed-training","text":"Similar to other modules of OneFlow, OneEmbedding supports distributed expansion natively. Users can refer to the README in #dlrm to start DLRM distributed training. You can also refer to Global Tensor for necessary prerequisites. When using the OneEmbedding module for distributed expansion, please be careful: Currently, OneEmbedding only supports placement on all devices, and the parallelism must be the same as the world size. For example, when training with 4 cards in parallel, the parallelism of the Embedding table must be 4. It is not supported when the network is trained with 4 cards but the Embedding table parallelism is 2. The persistent_path parameter in the store_options configuration specifies the path of the storage. In parallel scenarios, it can be either a string representing a path or a list . If configured as a string representing a path, it represents the root directory under each rank in distributed parallelism. OneFlow will create a storage path based on the number of each rank under this root path, and the name format is rank_id-num_rank . If persistent_path is a list , rank will be configured individually for each item in the list. In parallel scenarios, the capacity in the store_options configuration represents the capacity of total Embedding table, but not the capacity of each rank. cache_budget_mb represents the video memory per GPU device.","title":"Distributed Training"},{"location":"cookies/one_embedding.html#extended-reading-dlrm","text":"This article shows how to get started with OneEmbedding quickly. Practical examples of OneEmbedding in DLRM tasks are prepared in the OneFlow model repository, please refer to https://github.com/Oneflow-Inc/models/tree/main/RecommenderSystems/dlrm","title":"Extended Reading: DLRM"},{"location":"cookies/oneflow2onnnx.html","text":"OneFlow with ONNX \u00b6 This document introduces the usage of OneFlow interacting with ONNX, including how to export OneFlow models to ONNX, and how to use ONNX models for inference. Introduction to ONNX \u00b6 ONNX , known as Open Neural Network Exchange, is an open file format standard designed for machine learning algorithms to store trained algorithmic models. Many major deep learning frameworks (e.g., OneFlow, PyTorch, TensorFlow, MXNet) support exporting models to ONNX models, which allows different deep learning frameworks to store and interact with model data in a uniform format. In addition, ONNX has a corresponding Runtime - ONNX Runtime - that facilitates model deployment and reasoning on multiple platforms (Linux, Windows, Mac OS, Android, iOS, etc.) and multiple hardware (CPU, GPU, etc.). Related Packages \u00b6 There are several ONNX-related libraries, and the features of several common libraries are described below. The onnxruntime-gpu involved in this tutorial can be installed via pip install onnxruntime-gpu . onnx : ONNX model format standard onnxruntime & onnxruntime-gpu : ONNX runtime that is used to load the ONNX model for inference. onnxruntime and onnxruntime-gpu support CPU inference and GPU inference respectively. onnx-simplifier : for simplifying ONNX models, e.g. eliminating operators with constant results onnxoptimizer : it is used to optimize ONNX model by graph transformations Export OneFlow Models to ONNX Models \u00b6 oneflow-onnx is a model conversion tool provided by OneFlow team to support exporting OneFlow static graph models to ONNX models. At present oneflow-onnx supports more than 80 kinds of OneFlow OPs exported as ONNX OPs. For detalis, refer to list of OP supported by OneFlow2ONNX \u3002 Install oneflow-onnx \u00b6 oneflow-onnx is independent of OneFlow and needs to be installed separately via pip: pip install oneflow-onnx How to Use oneflow-onnx \u00b6 To export OneFlow static graph model as ONNX model, just call export_ onnx_ Model function. from oneflow_onnx.oneflow2onnx.util import export_onnx_model export_onnx_model ( graph , external_data = False , opset = None , flow_weight_dir = None , onnx_model_path = \"/tmp\" , dynamic_batch_size = False ) The meaning of each parameter is as follows: graph: the graph need to be converted ( Graph object) external_data: whether to save the weights as external data of the ONNX model. When it is True , it is usually to avoid the 2GB file size limit of protobuf. opset: specify the version of the conversion model (int, default is 10) flow_weight_dir: path to save OneFlow model weights onnx_model_path: save path for exported ONNX models dynamic_batch_size: whether the exported ONNX model supports dynamic batch, default is False In addition, oneflow-onnx provides a function called convert_to_onnx_and_check to convert and meanwhile check the converted ONNX model. The check process will pass the same input to the original OneFlow model and the converted ONNX model respectively, and then compare the difference between each value in the two outputs to see if they are same within a relative range. from oneflow_onnx.oneflow2onnx.util import convert_to_onnx_and_check convert_to_onnx_and_check ( ... ) The parameters of the convert_to_onnx_and_check are almost the same as those of export_onnx_model , besides you can pass print_outlier parameter additionally. When print_outlier=True , it will output any abnormal values found during the check process that exceed the reasonable error range. Considerations when Exporting Models \u00b6 Before exporting the model, it need be set to eval mode because operations such as Dropout and Batch Normalization have different behaviors under the training and evaluation mode. When building a static graph model, you need to specify an input. The value of the input can be random, but make sure the data type and shape is correct. The ONNX model accepts a fixed shape of input, and a varied size of the batch dimension, so by setting the dynamic_batch_size parameter to be True can make the exported ONNX model support dynamic batch size. Oneflow-onnx must use a static graph model (Graph mode) as an parameter to export function. For dynamic graph models (Eager mode), the dynamic graph model needs to be constructed as a static graph model. Refer to the example below. Examples of Usage \u00b6 In this section, the process of exporting a OneFlow model to an ONNX model and performing inference is introduced, using the common ResNet-34 model as an example. The following code uses FlowVision , a library built on OneFlow for computer vision tasks, which contains many models, data enhancement methods, data transformation operations, datasets, and so on. Here we directly use the ResNet-34 model provided by the FlowVision library and use its weight trained on the ImageNet dataset. Export as ONNX Model \u00b6 Import related dependencies and the saved resnet34 model will be used later: import oneflow as flow from oneflow import nn from flowvision.models import resnet34 from oneflow_onnx.oneflow2onnx.util import convert_to_onnx_and_check # Model parameter storage directory MODEL_PARAMS = 'checkpoints/resnet34' # Load & save pretrained model model = resnet34 ( pretrained = True ) flow . save ( model . state_dict (), MODEL_PARAMS ) To build a static graph model using a dynamic graph model. For details, refer to: Static Graph Interface: nn.Graph class ResNet34Graph ( nn . Graph ): def __init__ ( self , eager_model ): super () . __init__ () self . model = eager_model def build ( self , x ): return self . model ( x ) Export OneFlow static graph models to ONNX models: params = flow . load ( MODEL_PARAMS ) model = resnet34 () model . load_state_dict ( params ) # Set the model to eval mode model . eval () resnet34_graph = ResNet34Graph ( model ) # Build the static graph model resnet34_graph . _compile ( flow . randn ( 1 , 3 , 224 , 224 )) # Export as ONNX model and check convert_to_onnx_and_check ( resnet34_graph , flow_weight_dir = MODEL_PARAMS , onnx_model_path = \"./\" , print_outlier = True , dynamic_batch_size = True ) After running, a file named model.onnx is in the current directory, which is the exported ONNX model. Inference with ONNX models \u00b6 Before performing inference, ensure that the ONNX Runtime is installed, that is onnxruntime or onnxruntime-gpu. In the experimental environment of this tutorial, onnxruntime-gpu is installed to invoke the GPU for computation, but if there is no GPU on the machine, you can specify the CPU for calculation. See below for details. We use the following image as input to the model: Import related dependencies: import numpy as np import cv2 from onnxruntime import InferenceSession Define a function to pre-process the image to a format and size accepted by the ONNX model: def preprocess_image ( img , input_hw = ( 224 , 224 )): h , w , _ = img . shape # Use the longer side of the image to determine the scaling factor is_wider = True if h <= w else False scale = input_hw [ 1 ] / w if is_wider else input_hw [ 0 ] / h # Scale the image equally processed_img = cv2 . resize ( img , ( 0 , 0 ), fx = scale , fy = scale , interpolation = cv2 . INTER_LINEAR ) # Normalization processed_img = np . array ( processed_img , dtype = np . float32 ) / 255 # Fill images to ONNX model and preset sizes temp_img = np . zeros (( input_hw [ 0 ], input_hw [ 1 ], 3 ), dtype = np . float32 ) temp_img [: processed_img . shape [ 0 ], : processed_img . shape [ 1 ], :] = processed_img processed_img = temp_img # Adjust the order of axes and add batch axes at the front processed_img = np . expand_dims ( processed_img . transpose ( 2 , 0 , 1 ), axis = 0 ) return processed_img The next step is to use the ONNX model for inference, which consists of creating an InferenceSession object and calling run to perform inference. In onnxruntime(-gpu) 1.9 and above, the providers parameter needs to be explicitly specified when creating an InferenceSession object to select the hardware to use. For onnxruntime-gpu, the values that can be specified include TensorrtExecutionProvider , CUDAExecutionProvider , and CPUExecutionProvider . If there is no GPU on the running machine, you can specify the providers parameter as ['CPUExecutionProvider'] to use the CPU for computation. The type of input data of an ONNX model is a dict. Its keys are input names when exporting the ONNX model, and the values are the actual input data of NumPy array type. You can get \"input names\" through the get_inputs method of the InferenceSession object, which returns a list of objects of onnxruntime.NodeArg type. For NodeArg object, you can use its name property to get a name of str type. In this tutorial, the input is only the image data, so you can get the \"input names\" corresponding to the input by calling .get_inputs()[0].name on the InferenceSession object. The value is _ResNet34Graph_0-input_0/out , which is used as the key to construct the dict of the ONNX model input. Of course, it can also be obtained dynamically at runtime without specifying it in advance. # Read the category name of the ImageNet dataset from the file with open ( 'ImageNet-Class-Names.txt' ) as f : CLASS_NAMES = f . readlines () # Read the image file and preprocess it with the `preprocess_image` function img = cv2 . imread ( 'cat.jpg' , cv2 . IMREAD_COLOR ) img = preprocess_image ( img ) # Create an InferenceSession object ort_sess = InferenceSession ( 'model.onnx' , providers = [ 'TensorrtExecutionProvider' , 'CUDAExecutionProvider' , 'CPUExecutionProvider' ]) # Call the `run` method of the InferenceSession object to perform inference results = ort_sess . run ( None , { \"_ResNet34Graph_0-input_0/out\" : img }) # Output inference results print ( CLASS_NAMES [ np . argmax ( results [ 0 ])]) The output of the run method of the InferenceSession object is a list of NumPy arrays, and each NumPy array corresponds to a set of outputs. Since there is only one set of inputs, the element with index 0 is the output, and the shape of it is (1, 1000) , which corresponds to the probability of 1000 categories (if n images are input as a batch, the shape of them will be (n, 1000) ). After obtaining the index corresponding to the category with the highest probability via np.argmax , the index is mapped to the category name. Run the code and get the result: (base) root@training-notebook-654c6f-654c6f-jupyter-master-0:/workspace# python infer.py 285: 'Egyptian cat', The above inference is done in a Python environment using GPU or CPU. In practice, you can use the exported ONNX model with a different ONNX Runtime depending on the deployment environment.","title":"OneFlow with ONNX"},{"location":"cookies/oneflow2onnnx.html#oneflow-with-onnx","text":"This document introduces the usage of OneFlow interacting with ONNX, including how to export OneFlow models to ONNX, and how to use ONNX models for inference.","title":"OneFlow with ONNX"},{"location":"cookies/oneflow2onnnx.html#introduction-to-onnx","text":"ONNX , known as Open Neural Network Exchange, is an open file format standard designed for machine learning algorithms to store trained algorithmic models. Many major deep learning frameworks (e.g., OneFlow, PyTorch, TensorFlow, MXNet) support exporting models to ONNX models, which allows different deep learning frameworks to store and interact with model data in a uniform format. In addition, ONNX has a corresponding Runtime - ONNX Runtime - that facilitates model deployment and reasoning on multiple platforms (Linux, Windows, Mac OS, Android, iOS, etc.) and multiple hardware (CPU, GPU, etc.).","title":"Introduction to ONNX"},{"location":"cookies/oneflow2onnnx.html#related-packages","text":"There are several ONNX-related libraries, and the features of several common libraries are described below. The onnxruntime-gpu involved in this tutorial can be installed via pip install onnxruntime-gpu . onnx : ONNX model format standard onnxruntime & onnxruntime-gpu : ONNX runtime that is used to load the ONNX model for inference. onnxruntime and onnxruntime-gpu support CPU inference and GPU inference respectively. onnx-simplifier : for simplifying ONNX models, e.g. eliminating operators with constant results onnxoptimizer : it is used to optimize ONNX model by graph transformations","title":"Related Packages"},{"location":"cookies/oneflow2onnnx.html#export-oneflow-models-to-onnx-models","text":"oneflow-onnx is a model conversion tool provided by OneFlow team to support exporting OneFlow static graph models to ONNX models. At present oneflow-onnx supports more than 80 kinds of OneFlow OPs exported as ONNX OPs. For detalis, refer to list of OP supported by OneFlow2ONNX \u3002","title":"Export OneFlow Models to ONNX Models"},{"location":"cookies/oneflow2onnnx.html#install-oneflow-onnx","text":"oneflow-onnx is independent of OneFlow and needs to be installed separately via pip: pip install oneflow-onnx","title":"Install oneflow-onnx"},{"location":"cookies/oneflow2onnnx.html#how-to-use-oneflow-onnx","text":"To export OneFlow static graph model as ONNX model, just call export_ onnx_ Model function. from oneflow_onnx.oneflow2onnx.util import export_onnx_model export_onnx_model ( graph , external_data = False , opset = None , flow_weight_dir = None , onnx_model_path = \"/tmp\" , dynamic_batch_size = False ) The meaning of each parameter is as follows: graph: the graph need to be converted ( Graph object) external_data: whether to save the weights as external data of the ONNX model. When it is True , it is usually to avoid the 2GB file size limit of protobuf. opset: specify the version of the conversion model (int, default is 10) flow_weight_dir: path to save OneFlow model weights onnx_model_path: save path for exported ONNX models dynamic_batch_size: whether the exported ONNX model supports dynamic batch, default is False In addition, oneflow-onnx provides a function called convert_to_onnx_and_check to convert and meanwhile check the converted ONNX model. The check process will pass the same input to the original OneFlow model and the converted ONNX model respectively, and then compare the difference between each value in the two outputs to see if they are same within a relative range. from oneflow_onnx.oneflow2onnx.util import convert_to_onnx_and_check convert_to_onnx_and_check ( ... ) The parameters of the convert_to_onnx_and_check are almost the same as those of export_onnx_model , besides you can pass print_outlier parameter additionally. When print_outlier=True , it will output any abnormal values found during the check process that exceed the reasonable error range.","title":"How to Use oneflow-onnx"},{"location":"cookies/oneflow2onnnx.html#considerations-when-exporting-models","text":"Before exporting the model, it need be set to eval mode because operations such as Dropout and Batch Normalization have different behaviors under the training and evaluation mode. When building a static graph model, you need to specify an input. The value of the input can be random, but make sure the data type and shape is correct. The ONNX model accepts a fixed shape of input, and a varied size of the batch dimension, so by setting the dynamic_batch_size parameter to be True can make the exported ONNX model support dynamic batch size. Oneflow-onnx must use a static graph model (Graph mode) as an parameter to export function. For dynamic graph models (Eager mode), the dynamic graph model needs to be constructed as a static graph model. Refer to the example below.","title":"Considerations when Exporting Models"},{"location":"cookies/oneflow2onnnx.html#examples-of-usage","text":"In this section, the process of exporting a OneFlow model to an ONNX model and performing inference is introduced, using the common ResNet-34 model as an example. The following code uses FlowVision , a library built on OneFlow for computer vision tasks, which contains many models, data enhancement methods, data transformation operations, datasets, and so on. Here we directly use the ResNet-34 model provided by the FlowVision library and use its weight trained on the ImageNet dataset.","title":"Examples of Usage"},{"location":"cookies/oneflow2onnnx.html#export-as-onnx-model","text":"Import related dependencies and the saved resnet34 model will be used later: import oneflow as flow from oneflow import nn from flowvision.models import resnet34 from oneflow_onnx.oneflow2onnx.util import convert_to_onnx_and_check # Model parameter storage directory MODEL_PARAMS = 'checkpoints/resnet34' # Load & save pretrained model model = resnet34 ( pretrained = True ) flow . save ( model . state_dict (), MODEL_PARAMS ) To build a static graph model using a dynamic graph model. For details, refer to: Static Graph Interface: nn.Graph class ResNet34Graph ( nn . Graph ): def __init__ ( self , eager_model ): super () . __init__ () self . model = eager_model def build ( self , x ): return self . model ( x ) Export OneFlow static graph models to ONNX models: params = flow . load ( MODEL_PARAMS ) model = resnet34 () model . load_state_dict ( params ) # Set the model to eval mode model . eval () resnet34_graph = ResNet34Graph ( model ) # Build the static graph model resnet34_graph . _compile ( flow . randn ( 1 , 3 , 224 , 224 )) # Export as ONNX model and check convert_to_onnx_and_check ( resnet34_graph , flow_weight_dir = MODEL_PARAMS , onnx_model_path = \"./\" , print_outlier = True , dynamic_batch_size = True ) After running, a file named model.onnx is in the current directory, which is the exported ONNX model.","title":"Export as ONNX Model"},{"location":"cookies/oneflow2onnnx.html#inference-with-onnx-models","text":"Before performing inference, ensure that the ONNX Runtime is installed, that is onnxruntime or onnxruntime-gpu. In the experimental environment of this tutorial, onnxruntime-gpu is installed to invoke the GPU for computation, but if there is no GPU on the machine, you can specify the CPU for calculation. See below for details. We use the following image as input to the model: Import related dependencies: import numpy as np import cv2 from onnxruntime import InferenceSession Define a function to pre-process the image to a format and size accepted by the ONNX model: def preprocess_image ( img , input_hw = ( 224 , 224 )): h , w , _ = img . shape # Use the longer side of the image to determine the scaling factor is_wider = True if h <= w else False scale = input_hw [ 1 ] / w if is_wider else input_hw [ 0 ] / h # Scale the image equally processed_img = cv2 . resize ( img , ( 0 , 0 ), fx = scale , fy = scale , interpolation = cv2 . INTER_LINEAR ) # Normalization processed_img = np . array ( processed_img , dtype = np . float32 ) / 255 # Fill images to ONNX model and preset sizes temp_img = np . zeros (( input_hw [ 0 ], input_hw [ 1 ], 3 ), dtype = np . float32 ) temp_img [: processed_img . shape [ 0 ], : processed_img . shape [ 1 ], :] = processed_img processed_img = temp_img # Adjust the order of axes and add batch axes at the front processed_img = np . expand_dims ( processed_img . transpose ( 2 , 0 , 1 ), axis = 0 ) return processed_img The next step is to use the ONNX model for inference, which consists of creating an InferenceSession object and calling run to perform inference. In onnxruntime(-gpu) 1.9 and above, the providers parameter needs to be explicitly specified when creating an InferenceSession object to select the hardware to use. For onnxruntime-gpu, the values that can be specified include TensorrtExecutionProvider , CUDAExecutionProvider , and CPUExecutionProvider . If there is no GPU on the running machine, you can specify the providers parameter as ['CPUExecutionProvider'] to use the CPU for computation. The type of input data of an ONNX model is a dict. Its keys are input names when exporting the ONNX model, and the values are the actual input data of NumPy array type. You can get \"input names\" through the get_inputs method of the InferenceSession object, which returns a list of objects of onnxruntime.NodeArg type. For NodeArg object, you can use its name property to get a name of str type. In this tutorial, the input is only the image data, so you can get the \"input names\" corresponding to the input by calling .get_inputs()[0].name on the InferenceSession object. The value is _ResNet34Graph_0-input_0/out , which is used as the key to construct the dict of the ONNX model input. Of course, it can also be obtained dynamically at runtime without specifying it in advance. # Read the category name of the ImageNet dataset from the file with open ( 'ImageNet-Class-Names.txt' ) as f : CLASS_NAMES = f . readlines () # Read the image file and preprocess it with the `preprocess_image` function img = cv2 . imread ( 'cat.jpg' , cv2 . IMREAD_COLOR ) img = preprocess_image ( img ) # Create an InferenceSession object ort_sess = InferenceSession ( 'model.onnx' , providers = [ 'TensorrtExecutionProvider' , 'CUDAExecutionProvider' , 'CPUExecutionProvider' ]) # Call the `run` method of the InferenceSession object to perform inference results = ort_sess . run ( None , { \"_ResNet34Graph_0-input_0/out\" : img }) # Output inference results print ( CLASS_NAMES [ np . argmax ( results [ 0 ])]) The output of the run method of the InferenceSession object is a list of NumPy arrays, and each NumPy array corresponds to a set of outputs. Since there is only one set of inputs, the element with index 0 is the output, and the shape of it is (1, 1000) , which corresponds to the probability of 1000 categories (if n images are input as a batch, the shape of them will be (n, 1000) ). After obtaining the index corresponding to the category with the highest probability via np.argmax , the index is mapped to the category name. Run the code and get the result: (base) root@training-notebook-654c6f-654c6f-jupyter-master-0:/workspace# python infer.py 285: 'Egyptian cat', The above inference is done in a Python environment using GPU or CPU. In practice, you can use the exported ONNX model with a different ONNX Runtime depending on the deployment environment.","title":"Inference with ONNX models"},{"location":"cookies/serving.html","text":"Model Deployment \u00b6 Trained model needs to go through \"Model Deployment\" before it can be integrated into the product and launched. Because the software and hardware environment and the connection method between models and business modules may change when the product is launched, the deployed solutions are also varied. For example, some solutions convert the trained model to other formats (such as ONNX), and then rely on a specific runtime deployment; some solutions will directly use C/C++ or other languages that can generate native code to re-implement the model, and introduce code optimization in pursuit of hardware adaptation or deployment performance. OneFlow provides services for the model by docking with the Triton Inference Server . After training the model, OneFlow's users can deploy the model directly through Triton, use the rich features of Triton, such as Dynamic batching, Model Pipelines, and HTTP/gRPC interface to integrate it into online products quickly and efficiently. This document is divided into the following three sections: Quick Start Introduction to OneFlow Serving Architecture Process from Model Training to Deployment in OneFlow Quick Start \u00b6 OneFlow Serving: Neural Style Transfer is available on OneFlow Cloud. By referring to the project description, you can deploy the project and see the running result with just one click. Analyzing the code, we can find the following key points: Triton server and WEB application server are started in run_cloud.sh : /opt/tritonserver/bin/tritonserver --model-store $( pwd ) /model_repo > 1 .txt & python3 server.py There are simple and normal URL routings in server.py file and stylize in infer.py for inference work. Its result is obtained inside the stylize function through interacting with the HTTP and Triton server. def stylize ( content_path , output_path , style = 'udnie' ): triton_client = httpclient . InferenceServerClient ( url = '127.0.0.1:8000' ) ... inputs . append ( httpclient . InferInput ( 'INPUT_0' , image . shape , 'FP32' )) ... outputs . append ( httpclient . InferRequestedOutput ( 'OUTPUT_0' , binary_data = True )) ... Pretrained models are placed under model_repo , whose hierarchy is organized according to Triton's conventions. This simple online example illustrates how OneFlow models can be deployed through Triton and how business modules interact with the Triton server to obtain inference results. If you want to run this example locally, download demo.zip , then unzip it and run the file run.sh . bash run.sh Next we will introduce the detailed process from training to deployment in OneFlow. Process from Model Training to Deployment in OneFlow \u00b6 The following figure gives you a general description of the relationship between OneFlow and Triton. It can be seen that Triton is in the position of connecting the client and OneFlow: it provides HTTP, gRPC, and C interfaces, so that users can flexibly make an inference request and get the result. In Triton's architecture, OneFlow and Model Repository provide Triton with backend inference capabilities. OneFlow provides a corresponding interface to export the trained model that is under Triton's rule. In addition, Triton also provides built-in features such as task scheduling to ensure better performance. For details, refer to Triton's official documentation . After understanding these basic concepts, let's analyze the process from model training to deployment in OneFlow: Model saving Model deployment Start service Client request Model Saving \u00b6 The model trained in Graph mode can be directly exported in the required format for deployment through oneflow.save ; if it is trained in Eager mode, after simple conversion, it can be exported in the required format. For details, refer to Graph and Deployment . Model Deployment \u00b6 Triton has certain requirements for the layout of the model, so we need follow Triton's convention to organize the model layout and write related configuration files. Layout In this example program, the model files are placed in the model_repository directory, and its layout conforms to Triton's conventions. Let's see how it is organized: $ tree -L 3 model_repository/ model_repository/ \u2514\u2500\u2500 fast_neural_style \u251c\u2500\u2500 1 \u2502 \u2514\u2500\u2500 model \u2514\u2500\u2500 config.pbtxt model_repository is the root directory of the model repository. When starting Triton, you can specify it through the --model-repository option. fast_neural_style is a model in the repository. There can be multiple models in a repository, and each first-level sub-directory is a model. Here we only have the fast_neural_style model. The 1/model directory is the model we saved earlier through flow.save(graph, \"1/model\") . 1 is the version number. In Triton, there can be multiple model versions in a model directory, and the folder name of the model version must be number . Under the model version folder, you need to place a folder named model , which saves model parameters and computation graphs. config.pbtxt is a plain text file used to configure the basic information of the model repository, explained as follows. Model repository configuration config.pbtxt is a configuration file in protobuf text format. By writing this file, you can configure model services, such as specified hardware, input, and output. The example is as follows: name: \"fast_neural_style\" backend: \"oneflow\" max_batch_size: 1 input [ { name: \"INPUT_0\" data_type: TYPE_FP32 dims: [ 3, 256, 256 ] } ] output [ { name: \"OUTPUT_0\" data_type: TYPE_FP32 dims: [ 3, 256, 256 ] } ] instance_group [ { count: 1 kind: KIND_GPU gpus: [ 0 ] } ] Next we explain the configuration items one by one. name : \"fast_neural_style\" The name field is used to specify the model. This line indicates that we use the fast_neural_style model, whose name needs to be the same as the model's folder name mentioned above. backend: \"oneflow\" backend specifies the Triton backend. If you deploy with Oneflow, this field must be specified as oneflow . Next we need to define the shapes of input and output. For the name field, we need to follow the input and output order of the model and the format is INPUT_<index> and OUTPUT_<index> , where <index> indicates the order of model's input or output. Start at 0 by default. The data_type field defines the data type, and dims defines the shape of the tensor. input [ { name: \"INPUT_0\" data_type: TYPE_FP32 dims: [ 3, 256, 256 ] } ] output [ { name: \"OUTPUT_0\" data_type: TYPE_FP32 dims: [ 3, 256, 256 ] } ] The above model name, inference backend, and input and output configuration are the most basic configurations. Once configured, OneFlow can start working. instance_group is used to configure hardware information. instance_group [ { count: 1 kind: KIND_GPU gpus: [ 0 ] } ] It means we instantiate one model and place it on GPU 0. For more configuration options, refer to Model Configuration Documentation for Triton Inference Server . Start Service \u00b6 OneFlow Serving provides Docker images with which you can start model service. After organizing the files according to the above layout, you can map the path to the container and start the service. docker run --rm --runtime=nvidia --network=host -v$(pwd)/model_repository:/models \\ oneflowinc/oneflow-serving /opt/tritonserver/bin/tritonserver --model-store /models Run the command below to check whether the model service is starting. When you see the http 200 status code, the service has started. curl -v localhost:8000/v2/health/ready Request to Triton Server \u00b6 In this example, we use tritonclient to interact with Triton Server. We need to install a python package first. pip3 install tritonclient[all] Actually, clients can interact with Triton Server via HTTP, gRPC or C API etc. . The following code is the core part of image stylization, which can stylize the images passed from the command. You can view the complete code on Cloud Platform , or download demo.zip . #... import tritonclient.http as httpclient if __name__ == '__main__' : parser = argparse . ArgumentParser () parser . add_argument ( '--image' , required = True , help = 'the image to transfer style' ) FLAGS = parser . parse_args () triton_client = httpclient . InferenceServerClient ( url = '127.0.0.1:8000' ) image , w , h = load_image ( FLAGS . image , 256 , 256 ) inputs = [] inputs . append ( httpclient . InferInput ( 'INPUT_0' , image . shape , 'FP32' )) inputs [ 0 ] . set_data_from_numpy ( image , binary_data = True ) outputs = [] outputs . append ( httpclient . InferRequestedOutput ( 'OUTPUT_0' , binary_data = True )) results = triton_client . infer ( 'fast_neural_style' , inputs = inputs , outputs = outputs ) output0_data = results . as_numpy ( 'OUTPUT_0' ) image = recover_image ( output0_data , h , w ) cv2 . imwrite ( 'result.jpg' , image ) First create a triton_client where 127.0.0.1:8000 is the default port for the Triton service. triton_client = httpclient . InferenceServerClient ( url = '127.0.0.1:8000' ) Then through the triton_client.infer interface, you can send an inference request to the Triton Server and get the output. A Tirton inference request needs to specify the model, input and output. The following code is mainly constructing input and output objects. The configuration is consistent with that in the config.pbtxt . And the inference request is sent through triton_client.infer('fast_neural_style', inputs=inputs, outputs=outputs) . The fast_neural_style is also the same as the one in config.pbtxt . inputs = [] inputs . append ( httpclient . InferInput ( 'INPUT_0' , image . shape , 'FP32' )) inputs [ 0 ] . set_data_from_numpy ( image , binary_data = True ) outputs = [] outputs . append ( httpclient . InferRequestedOutput ( 'OUTPUT_0' , binary_data = True )) results = triton_client . infer ( 'fast_neural_style' , inputs = inputs , outputs = outputs ) Convert the format of the obtained inference result and save the result as the output image: output0_data = results . as_numpy ( 'OUTPUT_0' ) image = recover_image ( output0_data , h , w ) cv2 . imwrite ( 'result.jpg' , image ) You can use the following command to infer and stylize images, and the result will be saved in result.jpg . $ curl -o cat.jpg https://images.pexels.com/photos/156934/pexels-photo-156934.jpeg $ python infer.py --image cat.jpg","title":"Model Deployment"},{"location":"cookies/serving.html#model-deployment","text":"Trained model needs to go through \"Model Deployment\" before it can be integrated into the product and launched. Because the software and hardware environment and the connection method between models and business modules may change when the product is launched, the deployed solutions are also varied. For example, some solutions convert the trained model to other formats (such as ONNX), and then rely on a specific runtime deployment; some solutions will directly use C/C++ or other languages that can generate native code to re-implement the model, and introduce code optimization in pursuit of hardware adaptation or deployment performance. OneFlow provides services for the model by docking with the Triton Inference Server . After training the model, OneFlow's users can deploy the model directly through Triton, use the rich features of Triton, such as Dynamic batching, Model Pipelines, and HTTP/gRPC interface to integrate it into online products quickly and efficiently. This document is divided into the following three sections: Quick Start Introduction to OneFlow Serving Architecture Process from Model Training to Deployment in OneFlow","title":"Model Deployment"},{"location":"cookies/serving.html#quick-start","text":"OneFlow Serving: Neural Style Transfer is available on OneFlow Cloud. By referring to the project description, you can deploy the project and see the running result with just one click. Analyzing the code, we can find the following key points: Triton server and WEB application server are started in run_cloud.sh : /opt/tritonserver/bin/tritonserver --model-store $( pwd ) /model_repo > 1 .txt & python3 server.py There are simple and normal URL routings in server.py file and stylize in infer.py for inference work. Its result is obtained inside the stylize function through interacting with the HTTP and Triton server. def stylize ( content_path , output_path , style = 'udnie' ): triton_client = httpclient . InferenceServerClient ( url = '127.0.0.1:8000' ) ... inputs . append ( httpclient . InferInput ( 'INPUT_0' , image . shape , 'FP32' )) ... outputs . append ( httpclient . InferRequestedOutput ( 'OUTPUT_0' , binary_data = True )) ... Pretrained models are placed under model_repo , whose hierarchy is organized according to Triton's conventions. This simple online example illustrates how OneFlow models can be deployed through Triton and how business modules interact with the Triton server to obtain inference results. If you want to run this example locally, download demo.zip , then unzip it and run the file run.sh . bash run.sh Next we will introduce the detailed process from training to deployment in OneFlow.","title":"Quick Start"},{"location":"cookies/serving.html#process-from-model-training-to-deployment-in-oneflow","text":"The following figure gives you a general description of the relationship between OneFlow and Triton. It can be seen that Triton is in the position of connecting the client and OneFlow: it provides HTTP, gRPC, and C interfaces, so that users can flexibly make an inference request and get the result. In Triton's architecture, OneFlow and Model Repository provide Triton with backend inference capabilities. OneFlow provides a corresponding interface to export the trained model that is under Triton's rule. In addition, Triton also provides built-in features such as task scheduling to ensure better performance. For details, refer to Triton's official documentation . After understanding these basic concepts, let's analyze the process from model training to deployment in OneFlow: Model saving Model deployment Start service Client request","title":"Process from Model Training to Deployment in OneFlow"},{"location":"cookies/serving.html#model-saving","text":"The model trained in Graph mode can be directly exported in the required format for deployment through oneflow.save ; if it is trained in Eager mode, after simple conversion, it can be exported in the required format. For details, refer to Graph and Deployment .","title":"Model Saving"},{"location":"cookies/serving.html#model-deployment_1","text":"Triton has certain requirements for the layout of the model, so we need follow Triton's convention to organize the model layout and write related configuration files. Layout In this example program, the model files are placed in the model_repository directory, and its layout conforms to Triton's conventions. Let's see how it is organized: $ tree -L 3 model_repository/ model_repository/ \u2514\u2500\u2500 fast_neural_style \u251c\u2500\u2500 1 \u2502 \u2514\u2500\u2500 model \u2514\u2500\u2500 config.pbtxt model_repository is the root directory of the model repository. When starting Triton, you can specify it through the --model-repository option. fast_neural_style is a model in the repository. There can be multiple models in a repository, and each first-level sub-directory is a model. Here we only have the fast_neural_style model. The 1/model directory is the model we saved earlier through flow.save(graph, \"1/model\") . 1 is the version number. In Triton, there can be multiple model versions in a model directory, and the folder name of the model version must be number . Under the model version folder, you need to place a folder named model , which saves model parameters and computation graphs. config.pbtxt is a plain text file used to configure the basic information of the model repository, explained as follows. Model repository configuration config.pbtxt is a configuration file in protobuf text format. By writing this file, you can configure model services, such as specified hardware, input, and output. The example is as follows: name: \"fast_neural_style\" backend: \"oneflow\" max_batch_size: 1 input [ { name: \"INPUT_0\" data_type: TYPE_FP32 dims: [ 3, 256, 256 ] } ] output [ { name: \"OUTPUT_0\" data_type: TYPE_FP32 dims: [ 3, 256, 256 ] } ] instance_group [ { count: 1 kind: KIND_GPU gpus: [ 0 ] } ] Next we explain the configuration items one by one. name : \"fast_neural_style\" The name field is used to specify the model. This line indicates that we use the fast_neural_style model, whose name needs to be the same as the model's folder name mentioned above. backend: \"oneflow\" backend specifies the Triton backend. If you deploy with Oneflow, this field must be specified as oneflow . Next we need to define the shapes of input and output. For the name field, we need to follow the input and output order of the model and the format is INPUT_<index> and OUTPUT_<index> , where <index> indicates the order of model's input or output. Start at 0 by default. The data_type field defines the data type, and dims defines the shape of the tensor. input [ { name: \"INPUT_0\" data_type: TYPE_FP32 dims: [ 3, 256, 256 ] } ] output [ { name: \"OUTPUT_0\" data_type: TYPE_FP32 dims: [ 3, 256, 256 ] } ] The above model name, inference backend, and input and output configuration are the most basic configurations. Once configured, OneFlow can start working. instance_group is used to configure hardware information. instance_group [ { count: 1 kind: KIND_GPU gpus: [ 0 ] } ] It means we instantiate one model and place it on GPU 0. For more configuration options, refer to Model Configuration Documentation for Triton Inference Server .","title":"Model Deployment"},{"location":"cookies/serving.html#start-service","text":"OneFlow Serving provides Docker images with which you can start model service. After organizing the files according to the above layout, you can map the path to the container and start the service. docker run --rm --runtime=nvidia --network=host -v$(pwd)/model_repository:/models \\ oneflowinc/oneflow-serving /opt/tritonserver/bin/tritonserver --model-store /models Run the command below to check whether the model service is starting. When you see the http 200 status code, the service has started. curl -v localhost:8000/v2/health/ready","title":"Start Service"},{"location":"cookies/serving.html#request-to-triton-server","text":"In this example, we use tritonclient to interact with Triton Server. We need to install a python package first. pip3 install tritonclient[all] Actually, clients can interact with Triton Server via HTTP, gRPC or C API etc. . The following code is the core part of image stylization, which can stylize the images passed from the command. You can view the complete code on Cloud Platform , or download demo.zip . #... import tritonclient.http as httpclient if __name__ == '__main__' : parser = argparse . ArgumentParser () parser . add_argument ( '--image' , required = True , help = 'the image to transfer style' ) FLAGS = parser . parse_args () triton_client = httpclient . InferenceServerClient ( url = '127.0.0.1:8000' ) image , w , h = load_image ( FLAGS . image , 256 , 256 ) inputs = [] inputs . append ( httpclient . InferInput ( 'INPUT_0' , image . shape , 'FP32' )) inputs [ 0 ] . set_data_from_numpy ( image , binary_data = True ) outputs = [] outputs . append ( httpclient . InferRequestedOutput ( 'OUTPUT_0' , binary_data = True )) results = triton_client . infer ( 'fast_neural_style' , inputs = inputs , outputs = outputs ) output0_data = results . as_numpy ( 'OUTPUT_0' ) image = recover_image ( output0_data , h , w ) cv2 . imwrite ( 'result.jpg' , image ) First create a triton_client where 127.0.0.1:8000 is the default port for the Triton service. triton_client = httpclient . InferenceServerClient ( url = '127.0.0.1:8000' ) Then through the triton_client.infer interface, you can send an inference request to the Triton Server and get the output. A Tirton inference request needs to specify the model, input and output. The following code is mainly constructing input and output objects. The configuration is consistent with that in the config.pbtxt . And the inference request is sent through triton_client.infer('fast_neural_style', inputs=inputs, outputs=outputs) . The fast_neural_style is also the same as the one in config.pbtxt . inputs = [] inputs . append ( httpclient . InferInput ( 'INPUT_0' , image . shape , 'FP32' )) inputs [ 0 ] . set_data_from_numpy ( image , binary_data = True ) outputs = [] outputs . append ( httpclient . InferRequestedOutput ( 'OUTPUT_0' , binary_data = True )) results = triton_client . infer ( 'fast_neural_style' , inputs = inputs , outputs = outputs ) Convert the format of the obtained inference result and save the result as the output image: output0_data = results . as_numpy ( 'OUTPUT_0' ) image = recover_image ( output0_data , h , w ) cv2 . imwrite ( 'result.jpg' , image ) You can use the following command to infer and stylize images, and the result will be saved in result.jpg . $ curl -o cat.jpg https://images.pexels.com/photos/156934/pexels-photo-156934.jpeg $ python infer.py --image cat.jpg","title":"Request to Triton Server"},{"location":"cookies/torch2flow.html","text":"Converting Pre-trained Model from PyTorch to OneFlow \u00b6 Since interfaces of OneFlow and PyTorch are compatible, we can convert a pre-trained model from PyTorch to OneFlow when it's needed to use a PyTorch pre-trained model. Example of Model Conversion \u00b6 In the following code, we define and save a PyTorch model and then convert it to a OneFlow model. import torch import torch.nn as nn save_file = 'model.pth' model_torch = nn . Sequential ( nn . Linear ( 128 , 2 ), nn . Softmax () ) torch . save ( model_torch , save_file ) After running the above code, we get a model.pth file of PyTorch model. Then, the following two steps enable us to covert a PyTorch model to a OneFlow model\uff1a Defining a OneFlow model with the same structure Loading the model.pth file and initializing model parameters into OneFlow model Code is shown below: import oneflow as flow import oneflow.nn as nn import torch model_flow = nn . Sequential ( nn . Linear ( 128 , 2 ), nn . Softmax () ) parameters = torch . load ( save_file ) . state_dict () for key , value in parameters . items (): val = value . detach () . cpu () . numpy () parameters [ key ] = val model_flow . load_state_dict ( parameters ) .state_dict() enables us to obtain model parameters defined by key-value . Then, we use .detach().cpu().numpy() to convert parameter whose gradients are blocked into Numpy. Lastly, .load_state_dict(parameters) allows to pass model parameters to OneFlow model. With the simple example described above, we can find that the approach to convert PyTorch model into OneFlow is to use Numpy as a bridge . Therefore, provided the models defined by PyTorch and by OneFlow whose structures are consistent, even complicated models can still be smoothly converted. More Information about FlowVision \u00b6 Same as torchvision, flowvision also provides many pre-trained models, and the models in flowvision are compatible with those in torchvision. Taking AlexNet in flowvision for example, we will show how to convert complicate PyTorch pre-trained models into OneFlow by running the following code: import torchvision.models as models_torch import flowvision.models as models_flow alexnet_torch = models_torch . alexnet ( pretrained = True ) alexnet_flow = models_flow . alexnet () parameters = alexnet_torch . state_dict () for key , value in parameters . items (): val = value . detach () . cpu () . numpy () parameters [ key ] = val alexnet_flow . load_state_dict ( parameters ) We can also use pre-trained models provided in flowvision by importing the following code: alexnet_flow = models_flow . alexnet ( pretrained = True ) For more information about flowvision, please visit flowvision documentation .","title":"Converting Pre-trained Model from PyTorch to OneFlow"},{"location":"cookies/torch2flow.html#converting-pre-trained-model-from-pytorch-to-oneflow","text":"Since interfaces of OneFlow and PyTorch are compatible, we can convert a pre-trained model from PyTorch to OneFlow when it's needed to use a PyTorch pre-trained model.","title":"Converting Pre-trained Model from PyTorch to OneFlow"},{"location":"cookies/torch2flow.html#example-of-model-conversion","text":"In the following code, we define and save a PyTorch model and then convert it to a OneFlow model. import torch import torch.nn as nn save_file = 'model.pth' model_torch = nn . Sequential ( nn . Linear ( 128 , 2 ), nn . Softmax () ) torch . save ( model_torch , save_file ) After running the above code, we get a model.pth file of PyTorch model. Then, the following two steps enable us to covert a PyTorch model to a OneFlow model\uff1a Defining a OneFlow model with the same structure Loading the model.pth file and initializing model parameters into OneFlow model Code is shown below: import oneflow as flow import oneflow.nn as nn import torch model_flow = nn . Sequential ( nn . Linear ( 128 , 2 ), nn . Softmax () ) parameters = torch . load ( save_file ) . state_dict () for key , value in parameters . items (): val = value . detach () . cpu () . numpy () parameters [ key ] = val model_flow . load_state_dict ( parameters ) .state_dict() enables us to obtain model parameters defined by key-value . Then, we use .detach().cpu().numpy() to convert parameter whose gradients are blocked into Numpy. Lastly, .load_state_dict(parameters) allows to pass model parameters to OneFlow model. With the simple example described above, we can find that the approach to convert PyTorch model into OneFlow is to use Numpy as a bridge . Therefore, provided the models defined by PyTorch and by OneFlow whose structures are consistent, even complicated models can still be smoothly converted.","title":"Example of Model Conversion"},{"location":"cookies/torch2flow.html#more-information-about-flowvision","text":"Same as torchvision, flowvision also provides many pre-trained models, and the models in flowvision are compatible with those in torchvision. Taking AlexNet in flowvision for example, we will show how to convert complicate PyTorch pre-trained models into OneFlow by running the following code: import torchvision.models as models_torch import flowvision.models as models_flow alexnet_torch = models_torch . alexnet ( pretrained = True ) alexnet_flow = models_flow . alexnet () parameters = alexnet_torch . state_dict () for key , value in parameters . items (): val = value . detach () . cpu () . numpy () parameters [ key ] = val alexnet_flow . load_state_dict ( parameters ) We can also use pre-trained models provided in flowvision by importing the following code: alexnet_flow = models_flow . alexnet ( pretrained = True ) For more information about flowvision, please visit flowvision documentation .","title":"More Information about FlowVision"},{"location":"cookies/transfer_learning.html","text":"Transfer Learning in Computer Vision \u00b6 This tutorial introduces the fundamentals of transfer learning and shows an example of the use of transfer learning in the field of computer vision. Introduction of Fundamentals \u00b6 Transfer Learning , is a method of transferring knowledge learned from a source dataset to a target dataset. As we all know, supervised learning is a fairly common training method for deep learning models, but it requires a large amount of labeled data to achieve good results. When we want to apply a model to a specific task, we usually cannot obtain a large amount of labeled data due to the cost. If we directly train on such small-scale data, it is easy to cause overfitting. Therefore, transfer learning is one of the ways to solve this problem. For example, in the common image classification task in the field of computer vision, the general image classification model can be divided into two parts: feature extractor (or called backbone network) and classifier (or called output layer). Feature extractors are generally multi-layer networks such as Convolutional Neural Networks, and classifiers are generally single-layer networks such as fully connected layers. Since the categories of different classification tasks are generally different, classifiers are usually not reusable, while feature extractors are usually reusable. Although objects in the source dataset may be very different, or even have no connection at all with the target dataset, models pretrained on large-scale data may have the ability to extract more general image features (such as edges, shapes, and textures), which can help effectively identify objects in the target dataset. Suppose we already have a pretrained model, which can be used in roughly three ways. Initialize the feature extractor with the parameters of the pretrained model, then train the entire model. For deep learning models, the method of parameter initialization is very important to maintain numerical stability. Improper initialization methods may lead to the problem of gradient explosion or gradient disappearance during training. If a pretrained model is used for initialization, the rationality of the initial values of the model parameters can be guaranteed to a large extent, allowing the model to get a head start. Train the entire model, but use a smaller learning rate for the feature extractor and a larger learning rate for the classifier. The pretrained feature extractor has been fully trained, so only a small learning rate is required; while the parameters of the classifier are usually initialized randomly, and it needs to be learned from scratch, so a large learning rate is required. Fix the parameters of the feature extractor and only train the classifier. This is generally efficient and fast if the categories of the target dataset happen to be a subset of the source dataset. The Example of Transfer Learning \u00b6 In this section, ResNet-18 is used as the feature extractor for image classification task on CIFAR-10 Dataset Pretrained models for ResNet-18 (trained on ImageNet dataset), and CIFAR-10 dataset are both conveniently available through FlowVision . First import the required dependencies: import oneflow as flow from oneflow import nn from oneflow.utils.data import DataLoader from flowvision.models import resnet18 from flowvision.datasets import CIFAR10 import flowvision.transforms as transforms Define epoch, batch size, and the computing device used\uff1a NUM_EPOCHS = 3 BATCH_SIZE = 64 DEVICE = 'cuda' if flow . cuda . is_available () else 'cpu' Data Loading and Preprocessing \u00b6 Define Dataset and DataLoader: train_transform = transforms . Compose ([ transforms . RandomHorizontalFlip (), transforms . RandomVerticalFlip (), transforms . Resize ( 224 ), transforms . ToTensor (), transforms . Normalize ([ 0.485 , 0.456 , 0.406 ], [ 0.229 , 0.224 , 0.225 ]) ]) test_transform = transforms . Compose ([ transforms . Resize ( 224 ), transforms . ToTensor (), transforms . Normalize ([ 0.485 , 0.456 , 0.406 ], [ 0.229 , 0.224 , 0.225 ]) ]) train_dataset = CIFAR10 ( root = './data' , train = True , transform = train_transform , download = True ) test_dataset = CIFAR10 ( root = './data' , train = False , transform = test_transform , download = True ) train_data_loader = DataLoader ( train_dataset , batch_size = BATCH_SIZE , shuffle = True , num_workers = 4 ) test_data_loader = DataLoader ( test_dataset , batch_size = BATCH_SIZE , shuffle = False , num_workers = 4 ) Define the Model \u00b6 model = resnet18 ( pretrained = True ) Here, we get the ResNet-18 model loaded with pretrained weights by setting the pretrained parameter to True . If we output model.fc , we will get \"Linear(in_features=512, out_features=1000, bias=True)\". We can see that this classifier has 1000 output neurons, corresponding to 1000 categories of ImageNet. The CIFAR-10 dataset has 10 classes, so we need to replace this fully connected layer classifier: model . fc = nn . Linear ( model . fc . in_features , 10 ) And load the model to the computing device: model = model . to ( DEVICE ) Train the Model \u00b6 Define the training function: def train_model ( model , train_data_loader , test_data_loader , loss_func , optimizer ): dataset_size = len ( train_data_loader . dataset ) model . train () for epoch in range ( NUM_EPOCHS ): for batch , ( images , labels ) in enumerate ( train_data_loader ): images , labels = images . to ( DEVICE ), labels . to ( DEVICE ) preds = model ( images ) loss = loss_func ( preds , labels ) optimizer . zero_grad () loss . backward () optimizer . step () if batch % 100 == 0 : print ( f 'loss: { loss : >7f } [epoch: { epoch } { batch * BATCH_SIZE : >5d } / { dataset_size : >5d } ]' ) evaluate ( model , test_data_loader ) Define the evaluation function with the accuracy rate as the evaluation metric: def evaluate ( model , data_loader ): dataset_size = len ( data_loader . dataset ) model . eval () num_corrects = 0 for images , labels in data_loader : images , labels = images . to ( DEVICE ), labels . to ( DEVICE ) preds = model ( images ) num_corrects += flow . sum ( flow . argmax ( preds , dim = 1 ) == labels ) print ( 'Accuracy: ' , num_corrects . item () / dataset_size ) The three methods mentioned above can be implemented by passing in the corresponding parameters that need to be optimized to the optimizer. Method 1: Train the entire model optimizer = flow . optim . SGD ( model . parameters (), lr = 0.001 , momentum = 0.9 , weight_decay = 5e-4 ) Method 2: Use a smaller learning rate for the feature extractor and a larger learning rate for the classifier fc_params = list ( map ( id , model . fc . parameters ())) backbone_params = filter ( lambda p : id ( p ) not in fc_params , model . parameters ()) optimizer = flow . optim . SGD ([{ 'params' : backbone_params , 'lr' : 0.0001 }, { 'params' : model . fc . parameters (), 'lr' : 0.001 }], momentum = 0.9 , weight_decay = 5e-4 ) Method 3: Fix the parameters of the feature extractor and only train the classifier optimizer = flow . optim . SGD ( model . fc . parameters (), lr = 0.001 , momentum = 0.9 , weight_decay = 5e-4 ) Start training: loss_func = nn . CrossEntropyLoss () train_model ( model , train_data_loader , test_data_loader , loss_func , optimizer ) Comparison of Results \u00b6 In the case of using transfer learning (the first method is used here), the accuracy of the model on the test set after training for 3 epochs reaches 0.9017 ; If we train from scratch without transfer learning, the accuracy is only 0.4957 after the same 3 epochs of training. This shows that transfer learning can indeed play a significant role.","title":"Transfer Learning in Computer Vision"},{"location":"cookies/transfer_learning.html#transfer-learning-in-computer-vision","text":"This tutorial introduces the fundamentals of transfer learning and shows an example of the use of transfer learning in the field of computer vision.","title":"Transfer Learning in Computer Vision"},{"location":"cookies/transfer_learning.html#introduction-of-fundamentals","text":"Transfer Learning , is a method of transferring knowledge learned from a source dataset to a target dataset. As we all know, supervised learning is a fairly common training method for deep learning models, but it requires a large amount of labeled data to achieve good results. When we want to apply a model to a specific task, we usually cannot obtain a large amount of labeled data due to the cost. If we directly train on such small-scale data, it is easy to cause overfitting. Therefore, transfer learning is one of the ways to solve this problem. For example, in the common image classification task in the field of computer vision, the general image classification model can be divided into two parts: feature extractor (or called backbone network) and classifier (or called output layer). Feature extractors are generally multi-layer networks such as Convolutional Neural Networks, and classifiers are generally single-layer networks such as fully connected layers. Since the categories of different classification tasks are generally different, classifiers are usually not reusable, while feature extractors are usually reusable. Although objects in the source dataset may be very different, or even have no connection at all with the target dataset, models pretrained on large-scale data may have the ability to extract more general image features (such as edges, shapes, and textures), which can help effectively identify objects in the target dataset. Suppose we already have a pretrained model, which can be used in roughly three ways. Initialize the feature extractor with the parameters of the pretrained model, then train the entire model. For deep learning models, the method of parameter initialization is very important to maintain numerical stability. Improper initialization methods may lead to the problem of gradient explosion or gradient disappearance during training. If a pretrained model is used for initialization, the rationality of the initial values of the model parameters can be guaranteed to a large extent, allowing the model to get a head start. Train the entire model, but use a smaller learning rate for the feature extractor and a larger learning rate for the classifier. The pretrained feature extractor has been fully trained, so only a small learning rate is required; while the parameters of the classifier are usually initialized randomly, and it needs to be learned from scratch, so a large learning rate is required. Fix the parameters of the feature extractor and only train the classifier. This is generally efficient and fast if the categories of the target dataset happen to be a subset of the source dataset.","title":"Introduction of Fundamentals"},{"location":"cookies/transfer_learning.html#the-example-of-transfer-learning","text":"In this section, ResNet-18 is used as the feature extractor for image classification task on CIFAR-10 Dataset Pretrained models for ResNet-18 (trained on ImageNet dataset), and CIFAR-10 dataset are both conveniently available through FlowVision . First import the required dependencies: import oneflow as flow from oneflow import nn from oneflow.utils.data import DataLoader from flowvision.models import resnet18 from flowvision.datasets import CIFAR10 import flowvision.transforms as transforms Define epoch, batch size, and the computing device used\uff1a NUM_EPOCHS = 3 BATCH_SIZE = 64 DEVICE = 'cuda' if flow . cuda . is_available () else 'cpu'","title":"The Example of Transfer Learning"},{"location":"cookies/transfer_learning.html#data-loading-and-preprocessing","text":"Define Dataset and DataLoader: train_transform = transforms . Compose ([ transforms . RandomHorizontalFlip (), transforms . RandomVerticalFlip (), transforms . Resize ( 224 ), transforms . ToTensor (), transforms . Normalize ([ 0.485 , 0.456 , 0.406 ], [ 0.229 , 0.224 , 0.225 ]) ]) test_transform = transforms . Compose ([ transforms . Resize ( 224 ), transforms . ToTensor (), transforms . Normalize ([ 0.485 , 0.456 , 0.406 ], [ 0.229 , 0.224 , 0.225 ]) ]) train_dataset = CIFAR10 ( root = './data' , train = True , transform = train_transform , download = True ) test_dataset = CIFAR10 ( root = './data' , train = False , transform = test_transform , download = True ) train_data_loader = DataLoader ( train_dataset , batch_size = BATCH_SIZE , shuffle = True , num_workers = 4 ) test_data_loader = DataLoader ( test_dataset , batch_size = BATCH_SIZE , shuffle = False , num_workers = 4 )","title":"Data Loading and Preprocessing"},{"location":"cookies/transfer_learning.html#define-the-model","text":"model = resnet18 ( pretrained = True ) Here, we get the ResNet-18 model loaded with pretrained weights by setting the pretrained parameter to True . If we output model.fc , we will get \"Linear(in_features=512, out_features=1000, bias=True)\". We can see that this classifier has 1000 output neurons, corresponding to 1000 categories of ImageNet. The CIFAR-10 dataset has 10 classes, so we need to replace this fully connected layer classifier: model . fc = nn . Linear ( model . fc . in_features , 10 ) And load the model to the computing device: model = model . to ( DEVICE )","title":"Define the Model"},{"location":"cookies/transfer_learning.html#train-the-model","text":"Define the training function: def train_model ( model , train_data_loader , test_data_loader , loss_func , optimizer ): dataset_size = len ( train_data_loader . dataset ) model . train () for epoch in range ( NUM_EPOCHS ): for batch , ( images , labels ) in enumerate ( train_data_loader ): images , labels = images . to ( DEVICE ), labels . to ( DEVICE ) preds = model ( images ) loss = loss_func ( preds , labels ) optimizer . zero_grad () loss . backward () optimizer . step () if batch % 100 == 0 : print ( f 'loss: { loss : >7f } [epoch: { epoch } { batch * BATCH_SIZE : >5d } / { dataset_size : >5d } ]' ) evaluate ( model , test_data_loader ) Define the evaluation function with the accuracy rate as the evaluation metric: def evaluate ( model , data_loader ): dataset_size = len ( data_loader . dataset ) model . eval () num_corrects = 0 for images , labels in data_loader : images , labels = images . to ( DEVICE ), labels . to ( DEVICE ) preds = model ( images ) num_corrects += flow . sum ( flow . argmax ( preds , dim = 1 ) == labels ) print ( 'Accuracy: ' , num_corrects . item () / dataset_size ) The three methods mentioned above can be implemented by passing in the corresponding parameters that need to be optimized to the optimizer. Method 1: Train the entire model optimizer = flow . optim . SGD ( model . parameters (), lr = 0.001 , momentum = 0.9 , weight_decay = 5e-4 ) Method 2: Use a smaller learning rate for the feature extractor and a larger learning rate for the classifier fc_params = list ( map ( id , model . fc . parameters ())) backbone_params = filter ( lambda p : id ( p ) not in fc_params , model . parameters ()) optimizer = flow . optim . SGD ([{ 'params' : backbone_params , 'lr' : 0.0001 }, { 'params' : model . fc . parameters (), 'lr' : 0.001 }], momentum = 0.9 , weight_decay = 5e-4 ) Method 3: Fix the parameters of the feature extractor and only train the classifier optimizer = flow . optim . SGD ( model . fc . parameters (), lr = 0.001 , momentum = 0.9 , weight_decay = 5e-4 ) Start training: loss_func = nn . CrossEntropyLoss () train_model ( model , train_data_loader , test_data_loader , loss_func , optimizer )","title":"Train the Model"},{"location":"cookies/transfer_learning.html#comparison-of-results","text":"In the case of using transfer learning (the first method is used here), the accuracy of the model on the test set after training for 3 epochs reaches 0.9017 ; If we train from scratch without transfer learning, the accuracy is only 0.4957 after the same 3 epochs of training. This shows that transfer learning can indeed play a significant role.","title":"Comparison of Results"},{"location":"cookies/zero.html","text":"Zero Redundancy Optimizer (ZeRO) \u00b6 Introduction to ZeRO \u00b6 Zero Redundancy Optimizer (ZeRO) is a method proposed in paper ZeRO: Memory Optimization Towards Training A Trillion Parameter Models , aiming to reduce the memory usage under the data parallelism strategy. In common data parallelism strategy, each GPU independently maintains a complete set of model parameters, which is efficient in computation and communication, but inefficient in memory. This problem is especially acute when training large models. ZeRO consists of ZeRO-DP and ZeRO-R, which can effectively reduce the consumption of video memory. This means that larger models can be trained with the same amount of memory. It also means that it is possible to use data parallelism for large models that could only be trained with model parallelism strategies in the past. The memory consumption when training a deep learning model can be divided into two parts: Model States . For large models, most of the memory consumption is occupied by the model state, which mainly includes three parts: Optimizer States, Gradients, and Parameters. The three parts are abbreviated as OPG . Residual States . It includes activation functions, temporary buffers, and unusable memory fragments. ZeRO-DP can be divided into three stages, eliminating memory redundancy by partitioning the OPG state rather than copying it directly, and each GPU only saves part of the OPG. Specifically, ZeRO-DP has three main optimization stages, corresponding to O, P, and G respectively. The three stages increase step by step: Optimizer states partition\uff08P os \uff09: This state is 4x less memory consumption and the same amount of traffic as data parallel. Add gradients partition optimizer (P os+g ): At this stage, the memory consumption is reduced by 8 times, and the traffic is the same as the data parallel. Add parameter partition optimizer (P os+g+p ): At this stage, the memory occupied by the model is evenly distributed among each GPU. Memory consumption is linearly inversely proportional to the degree of data parallel, but there will be a slight increase in traffic. The distribution of the memory consumption of the three stages can be seen in the following figure (from the original ZeRO paper Figure 1): ZeRo Usage Example \u00b6 First, import OneFlow\uff1a import oneflow as flow from oneflow import nn Definine the Training Process of Data Parallelism \u00b6 We define a training process under a data parallellelism strategy, similar to that described in Conduct data parallel training by setting SBP . Note ZeRO can be applied for all the cases where data parallel groups exist. For example, in 2D/3D parallel, ZeRO can be turned on as long as there is a data parallel group. After the definition, we will use placement, SBP, etc: P = flow . placement ( \"cuda\" , ranks = [ 0 , 1 ]) B = flow . sbp . broadcast S0 = flow . sbp . split ( 0 ) DEVICE = \"cuda\" For demonstration purposes, we define a simple model and broadcast it to the cluster: model = nn . Sequential ( nn . Linear ( 256 , 128 ), nn . ReLU (), nn . Linear ( 128 , 10 )) model = model . to ( DEVICE ) model . train () model = model . to_global ( placement = P , sbp = B ) loss_fn = nn . CrossEntropyLoss () . to ( DEVICE ) optimizer = flow . optim . SGD ( model . parameters (), lr = 1e-3 ) ZeRO is set in the graph compiler of nn.Graph , so the dynamic graph model needs to be converted to nn.Graph: class CustomGraph ( flow . nn . Graph ): def __init__ ( self ): super () . __init__ () self . model = model self . loss_fn = loss_fn self . add_optimizer ( optimizer ) # TODO: Set ZeRO def build ( self , x , y ): preds = self . model ( x ) loss = self . loss_fn ( preds , y ) loss . backward () return preds Definine the Training Process graph_model = CustomGraph () for _ in range ( 100 ): x = flow . randn ( 128 , 256 ) . to ( DEVICE ) y = flow . ones ( 128 , 1 , dtype = flow . int64 ) . to ( DEVICE ) global_x = x . to_global ( placement = P , sbp = S0 ) global_y = y . to_global ( placement = P , sbp = S0 ) graph_model ( global_x , global_y ) Then start training through launch Module Enable ZeRO in nn.Graph \u00b6 ZeRO can be enabled through the interface config.set_zero_redundancy_optimizer_mode . Enable Stage 1 of ZeRO \u00b6 class CustomGraph ( flow . nn . Graph ): def __init__ ( self ): super () . __init__ () ... # Enable stage 1 of ZeRO self . config . enable_zero ( True , stage = 1 ) ... Note When using the model for continuous training and prediction: After the training is performed once, ZeRO will automatically change the SBP parameter of the model from Broadcast to Split; when performing prediction, Split will be used for automatic inference without configuring ZeRO. Enable Stage 2 of ZeRO \u00b6 class CustomGraph ( flow . nn . Graph ): def __init__ ( self ): super () . __init__ () ... # Enable stage 2 of ZeRO self . config . enable_zero ( True , stage = 2 ) ... Generally speaking, the optimization of stage 2 has large optimization of memory and small speed impact, so it is recommended to use stage 2 optimization. It can be enabled in a simpler way: class CustomGraph ( flow . nn . Graph ): def __init__ ( self ): super () . __init__ () ... # Enable stage 2 of ZeRO self . config . enable_zero () ... Enable Stage 3 of ZeRO \u00b6 class CustomGraph ( flow . nn . Graph ): def __init__ ( self ): super () . __init__ () ... # Enable stage 3 of ZeRO self . config . enable_zero ( True , stage = 3 ) ... Although enabling the third stage can minimize the memory consumption, it will increase the communication cost which will bring lower speed.","title":"Zero Redundancy Optimizer (ZeRO)"},{"location":"cookies/zero.html#zero-redundancy-optimizer-zero","text":"","title":"Zero Redundancy Optimizer (ZeRO)"},{"location":"cookies/zero.html#introduction-to-zero","text":"Zero Redundancy Optimizer (ZeRO) is a method proposed in paper ZeRO: Memory Optimization Towards Training A Trillion Parameter Models , aiming to reduce the memory usage under the data parallelism strategy. In common data parallelism strategy, each GPU independently maintains a complete set of model parameters, which is efficient in computation and communication, but inefficient in memory. This problem is especially acute when training large models. ZeRO consists of ZeRO-DP and ZeRO-R, which can effectively reduce the consumption of video memory. This means that larger models can be trained with the same amount of memory. It also means that it is possible to use data parallelism for large models that could only be trained with model parallelism strategies in the past. The memory consumption when training a deep learning model can be divided into two parts: Model States . For large models, most of the memory consumption is occupied by the model state, which mainly includes three parts: Optimizer States, Gradients, and Parameters. The three parts are abbreviated as OPG . Residual States . It includes activation functions, temporary buffers, and unusable memory fragments. ZeRO-DP can be divided into three stages, eliminating memory redundancy by partitioning the OPG state rather than copying it directly, and each GPU only saves part of the OPG. Specifically, ZeRO-DP has three main optimization stages, corresponding to O, P, and G respectively. The three stages increase step by step: Optimizer states partition\uff08P os \uff09: This state is 4x less memory consumption and the same amount of traffic as data parallel. Add gradients partition optimizer (P os+g ): At this stage, the memory consumption is reduced by 8 times, and the traffic is the same as the data parallel. Add parameter partition optimizer (P os+g+p ): At this stage, the memory occupied by the model is evenly distributed among each GPU. Memory consumption is linearly inversely proportional to the degree of data parallel, but there will be a slight increase in traffic. The distribution of the memory consumption of the three stages can be seen in the following figure (from the original ZeRO paper Figure 1):","title":"Introduction to ZeRO"},{"location":"cookies/zero.html#zero-usage-example","text":"First, import OneFlow\uff1a import oneflow as flow from oneflow import nn","title":"ZeRo Usage Example"},{"location":"cookies/zero.html#definine-the-training-process-of-data-parallelism","text":"We define a training process under a data parallellelism strategy, similar to that described in Conduct data parallel training by setting SBP . Note ZeRO can be applied for all the cases where data parallel groups exist. For example, in 2D/3D parallel, ZeRO can be turned on as long as there is a data parallel group. After the definition, we will use placement, SBP, etc: P = flow . placement ( \"cuda\" , ranks = [ 0 , 1 ]) B = flow . sbp . broadcast S0 = flow . sbp . split ( 0 ) DEVICE = \"cuda\" For demonstration purposes, we define a simple model and broadcast it to the cluster: model = nn . Sequential ( nn . Linear ( 256 , 128 ), nn . ReLU (), nn . Linear ( 128 , 10 )) model = model . to ( DEVICE ) model . train () model = model . to_global ( placement = P , sbp = B ) loss_fn = nn . CrossEntropyLoss () . to ( DEVICE ) optimizer = flow . optim . SGD ( model . parameters (), lr = 1e-3 ) ZeRO is set in the graph compiler of nn.Graph , so the dynamic graph model needs to be converted to nn.Graph: class CustomGraph ( flow . nn . Graph ): def __init__ ( self ): super () . __init__ () self . model = model self . loss_fn = loss_fn self . add_optimizer ( optimizer ) # TODO: Set ZeRO def build ( self , x , y ): preds = self . model ( x ) loss = self . loss_fn ( preds , y ) loss . backward () return preds Definine the Training Process graph_model = CustomGraph () for _ in range ( 100 ): x = flow . randn ( 128 , 256 ) . to ( DEVICE ) y = flow . ones ( 128 , 1 , dtype = flow . int64 ) . to ( DEVICE ) global_x = x . to_global ( placement = P , sbp = S0 ) global_y = y . to_global ( placement = P , sbp = S0 ) graph_model ( global_x , global_y ) Then start training through launch Module","title":"Definine the Training Process of Data Parallelism"},{"location":"cookies/zero.html#enable-zero-in-nngraph","text":"ZeRO can be enabled through the interface config.set_zero_redundancy_optimizer_mode .","title":"Enable ZeRO in nn.Graph"},{"location":"cookies/zero.html#enable-stage-1-of-zero","text":"class CustomGraph ( flow . nn . Graph ): def __init__ ( self ): super () . __init__ () ... # Enable stage 1 of ZeRO self . config . enable_zero ( True , stage = 1 ) ... Note When using the model for continuous training and prediction: After the training is performed once, ZeRO will automatically change the SBP parameter of the model from Broadcast to Split; when performing prediction, Split will be used for automatic inference without configuring ZeRO.","title":"Enable Stage 1 of ZeRO"},{"location":"cookies/zero.html#enable-stage-2-of-zero","text":"class CustomGraph ( flow . nn . Graph ): def __init__ ( self ): super () . __init__ () ... # Enable stage 2 of ZeRO self . config . enable_zero ( True , stage = 2 ) ... Generally speaking, the optimization of stage 2 has large optimization of memory and small speed impact, so it is recommended to use stage 2 optimization. It can be enabled in a simpler way: class CustomGraph ( flow . nn . Graph ): def __init__ ( self ): super () . __init__ () ... # Enable stage 2 of ZeRO self . config . enable_zero () ...","title":"Enable Stage 2 of ZeRO"},{"location":"cookies/zero.html#enable-stage-3-of-zero","text":"class CustomGraph ( flow . nn . Graph ): def __init__ ( self ): super () . __init__ () ... # Enable stage 3 of ZeRO self . config . enable_zero ( True , stage = 3 ) ... Although enabling the third stage can minimize the memory consumption, it will increase the communication cost which will bring lower speed.","title":"Enable Stage 3 of ZeRO"},{"location":"parallelism/01_introduction.html","text":"COMMON DISTRIBUTED PARALLEL STRATEGY \u00b6 Why Distributed Training is Prevailing \u00b6 In recent years, deep learning has been widely used in various fields, including computer vision, language understanding, speech recognition, advertising recommendation and so on. A common feature in these different areas is the growing size of models, such as the GPT-3 model, which has 175 billion parameters. Even with 1,024 of 80 GB A100 cards, the full GPT-3 training time would take a month. The enlargement of model scale requires the development of hardware (computing power, memory). However, because of the existence of memory walls, the computational power and capacity of a single device, limited by the laws of physics, it is increasingly difficult to continuously improve the integration of chips and to keep up with the demands of model expansion. In order to solve the problem of insufficient increase speed of computing power, it is necessary to use multi-node cluster for distributed training in order to improve computing speed. Common Parallel Strategies \u00b6 Simply stacking machines does not increase computing power necessarily. Because the training of neural networks can not be simply \"divide the work done by one device to multiple ones\". It requires not only multiple devices to perform computing, but also data transmission between devices, only by coordinating the computing and communication in the cluster, can we do efficient distributed training. We will explain the difference between data parallelism and model parallelism with an example of matrix multiplication. Let's look at the following logical matrix multiplication examples: If a layer in the neural network is doing matrix multiplication, where the shape of the input \\(x\\) is \\(4\\times5\\) and the shape of the model parameter \\(w\\) is \\(5\\times8\\) , then the matrix multiplication output shape is \\(4\\times8\\) . The schematic diagram is as follows: In the single machine single card training situaiton, the above matrix multiplication first calculates \\(out\\) , passes \\(out\\) to the next layer, and finally calculates \\(loss\\) , and then in the backpropagation process, gets \\(\\frac{\\partial loss}{\\partial w}\\) , which then be used to update \\(w\\) . In distributed training, there are \" Data Parallelism \" and \" Model Parallelism \" strategies depending on whether \\(x\\) or \\(w\\) is partitioned. In the next section, we will introduce common strategies for parallelism. Data Parallelism \u00b6 Data parallelism slices \\(x\\) , while the model parameter \\(w\\) on each device is complete and consistent. As shown in the figure below, \\(x\\) is split evenly into two devices by dimension 0, each with a full \\(w\\) . In this way, the output on each device is only half the logical output, which shape is \\(2\\times8\\) . The output on both devices combind together to produce the logically complete output. Note that because the data is distributed to two devices, the backpropagation process will get different values for \\(\\frac{\\partial loss}{\\partial w}\\) , if the models are updated directly using the gradients on each device, it would cause the models on the two devices to be inconsistent, and the training would be meaningless(Which model should be used?). Therefore, in the process of backpropagation under data parallelism strategy, the gradients on each device should do AllReduce before use, which ensures the model on each device is always consistent. When the dataset is large and the model is small, and the communication cost for the gradients synchronization is small in the backpropagation process, so it is more advantageous to choose data parallelism in this situation. For example, the common vision classification model, such as ResNet50, is more suitable to use data parallelism strategy. Model Parallelism \u00b6 When the neural network is very large, the cost of gradients synchronization will be very high, moreover, the network may be too large to be stored in a single computing device, then the model parallelism strategy can be used. The so called model parallelism is that the data on each device is complete and consistent, while the model \\(w\\) is split into different devices, each device only has a part of the model, all the parts of model on the computing device put together forms the complete model. As shown in the figure below, \\(w\\) is split evenly into two devices by the first dimension, each with a full \\(x\\) . The output on both devices also needs to be combind together to get the logical output. The benefit of model parallelism is that it eliminates the gradient AllReduce between multiple devices. However, since each device requires complete input data, the input is broadcasted among multiple devices with some communication cost. For example, the \\(out~(4\\times8)\\) shown above needs to be broadcast to both devices if it is the input of the next layer. Language models, such as BERT, often use model parallelism. Pipelining Parallelism \u00b6 When the neural network is too large to be stored on a single device, in addition to the above parallel strategies, we can also choose pipelining parallel strategy. Pipelining paralelism divides the network into stages and places it to different computing devices, each of which completes the training in a \"relay\" manner. The figure below shows how to run with pipelining parallelism with a logical four-layer network ( T1 to T4 ). The four-layer network is divided into two computing devices, so that the T1 and T2 are placed to GPU0 and T3 and T4 are placed to GPU1 . After computing the first two layers on GPU0 , its output is treated as the input of GPU1 to continue computation of the next two layers. Hybrid Parallelism \u00b6 You can also mix with a variety of parallelism strategies when training a network, take GPT-3 as an example, the parallelism strategy for training could be like this: This large GPT network is partitioned into 64 stages, with each stage running on 6 DGX-A100s. The workload among the 6 machines is trained with data parallelism, while the workload among GPUs inside each machine is trained with model parallelism. The 3072 A100s in the entire cluster are divided into a matrix of \\(6\\times8\\times64\\) , and then train the model using data parallelism, model parallelism and pipeline parallelism simultaneously. The choice of parallelism strategy affects the efficiency of training. Whether the interface of framework supports parallelism well also determines the efficiency of algorithm engineer. OneFlow's system-level design and innovation for distributed training will help users to get comfortable well with distributed training. The related examples will be shown in other articles on this topic.","title":"Common Parallel Strategies"},{"location":"parallelism/01_introduction.html#common-distributed-parallel-strategy","text":"","title":"COMMON DISTRIBUTED PARALLEL STRATEGY"},{"location":"parallelism/01_introduction.html#why-distributed-training-is-prevailing","text":"In recent years, deep learning has been widely used in various fields, including computer vision, language understanding, speech recognition, advertising recommendation and so on. A common feature in these different areas is the growing size of models, such as the GPT-3 model, which has 175 billion parameters. Even with 1,024 of 80 GB A100 cards, the full GPT-3 training time would take a month. The enlargement of model scale requires the development of hardware (computing power, memory). However, because of the existence of memory walls, the computational power and capacity of a single device, limited by the laws of physics, it is increasingly difficult to continuously improve the integration of chips and to keep up with the demands of model expansion. In order to solve the problem of insufficient increase speed of computing power, it is necessary to use multi-node cluster for distributed training in order to improve computing speed.","title":"Why Distributed Training is Prevailing"},{"location":"parallelism/01_introduction.html#common-parallel-strategies","text":"Simply stacking machines does not increase computing power necessarily. Because the training of neural networks can not be simply \"divide the work done by one device to multiple ones\". It requires not only multiple devices to perform computing, but also data transmission between devices, only by coordinating the computing and communication in the cluster, can we do efficient distributed training. We will explain the difference between data parallelism and model parallelism with an example of matrix multiplication. Let's look at the following logical matrix multiplication examples: If a layer in the neural network is doing matrix multiplication, where the shape of the input \\(x\\) is \\(4\\times5\\) and the shape of the model parameter \\(w\\) is \\(5\\times8\\) , then the matrix multiplication output shape is \\(4\\times8\\) . The schematic diagram is as follows: In the single machine single card training situaiton, the above matrix multiplication first calculates \\(out\\) , passes \\(out\\) to the next layer, and finally calculates \\(loss\\) , and then in the backpropagation process, gets \\(\\frac{\\partial loss}{\\partial w}\\) , which then be used to update \\(w\\) . In distributed training, there are \" Data Parallelism \" and \" Model Parallelism \" strategies depending on whether \\(x\\) or \\(w\\) is partitioned. In the next section, we will introduce common strategies for parallelism.","title":"Common Parallel Strategies"},{"location":"parallelism/01_introduction.html#data-parallelism","text":"Data parallelism slices \\(x\\) , while the model parameter \\(w\\) on each device is complete and consistent. As shown in the figure below, \\(x\\) is split evenly into two devices by dimension 0, each with a full \\(w\\) . In this way, the output on each device is only half the logical output, which shape is \\(2\\times8\\) . The output on both devices combind together to produce the logically complete output. Note that because the data is distributed to two devices, the backpropagation process will get different values for \\(\\frac{\\partial loss}{\\partial w}\\) , if the models are updated directly using the gradients on each device, it would cause the models on the two devices to be inconsistent, and the training would be meaningless(Which model should be used?). Therefore, in the process of backpropagation under data parallelism strategy, the gradients on each device should do AllReduce before use, which ensures the model on each device is always consistent. When the dataset is large and the model is small, and the communication cost for the gradients synchronization is small in the backpropagation process, so it is more advantageous to choose data parallelism in this situation. For example, the common vision classification model, such as ResNet50, is more suitable to use data parallelism strategy.","title":"Data Parallelism"},{"location":"parallelism/01_introduction.html#model-parallelism","text":"When the neural network is very large, the cost of gradients synchronization will be very high, moreover, the network may be too large to be stored in a single computing device, then the model parallelism strategy can be used. The so called model parallelism is that the data on each device is complete and consistent, while the model \\(w\\) is split into different devices, each device only has a part of the model, all the parts of model on the computing device put together forms the complete model. As shown in the figure below, \\(w\\) is split evenly into two devices by the first dimension, each with a full \\(x\\) . The output on both devices also needs to be combind together to get the logical output. The benefit of model parallelism is that it eliminates the gradient AllReduce between multiple devices. However, since each device requires complete input data, the input is broadcasted among multiple devices with some communication cost. For example, the \\(out~(4\\times8)\\) shown above needs to be broadcast to both devices if it is the input of the next layer. Language models, such as BERT, often use model parallelism.","title":"Model Parallelism"},{"location":"parallelism/01_introduction.html#pipelining-parallelism","text":"When the neural network is too large to be stored on a single device, in addition to the above parallel strategies, we can also choose pipelining parallel strategy. Pipelining paralelism divides the network into stages and places it to different computing devices, each of which completes the training in a \"relay\" manner. The figure below shows how to run with pipelining parallelism with a logical four-layer network ( T1 to T4 ). The four-layer network is divided into two computing devices, so that the T1 and T2 are placed to GPU0 and T3 and T4 are placed to GPU1 . After computing the first two layers on GPU0 , its output is treated as the input of GPU1 to continue computation of the next two layers.","title":"Pipelining Parallelism"},{"location":"parallelism/01_introduction.html#hybrid-parallelism","text":"You can also mix with a variety of parallelism strategies when training a network, take GPT-3 as an example, the parallelism strategy for training could be like this: This large GPT network is partitioned into 64 stages, with each stage running on 6 DGX-A100s. The workload among the 6 machines is trained with data parallelism, while the workload among GPUs inside each machine is trained with model parallelism. The 3072 A100s in the entire cluster are divided into a matrix of \\(6\\times8\\times64\\) , and then train the model using data parallelism, model parallelism and pipeline parallelism simultaneously. The choice of parallelism strategy affects the efficiency of training. Whether the interface of framework supports parallelism well also determines the efficiency of algorithm engineer. OneFlow's system-level design and innovation for distributed training will help users to get comfortable well with distributed training. The related examples will be shown in other articles on this topic.","title":"Hybrid Parallelism"},{"location":"parallelism/02_sbp.html","text":"GLOBAL VIEW \u00b6 The concept of global view in OneFlow is introduced to simplify distributed training. In short, the cluster is abstracted as a \"Super Computing Device\" under OneFlow global view. Instead of caring about the details of computing and communication in a cluster, users can program like on a single node, and OneFlow can train the model in a distributed way. OneFlow's global view relies on several important concepts: Placement , SBP and SBP Signature . Placement \u00b6 The Tensors of OneFlow has a placement attribute in global view; the placement specifies which local physical device the Tensor is placed on. OneFlow will automatically number the devices in the cluster. For example, if there are four hosts in a cluster and each host has eight GPU cards, so that 32 cards in total. The 32 devices in the cluser will be numbered 0 to 31. To place a Tensor on the first four cards on machine 0, simply configure: placement(\"cuda\", [0, 1, 2, 3]) . To place a Tensor on the last four cards on machine 0, simply configure: placement(\"cuda\", [4, 5, 6, 7]) . Placement makes it easy for OneFlow to support pipelining parallelism, and we\u2019ll see examples of placement in other articles on this topic. SBP \u00b6 SBP is a unique concept in OneFlow, which describes the mapping of data from a \"Super Computing Device\" perspective to data on local physical devices in a cluster. It is a combination of the initials of three words: split , broadcast , partial . In detail: split means that the local Tensor is obtained by splitting the global Tensor along a certain dimension. An axis parameter is used to indicate the dimension of the split. If multiple local Tensors are concatenated along the dimension of Split, the global Tensor can be restored. broadcast indicates that each local Tensor is exactly a copy of the global Tensor. partial indicates that although the local Tensor has the same shape as the global Tensor, the value in the local Tensor is a part of the value in the corresponding position in the global Tensor, if you add multiple local Tensors at the same positions, you can restore the global Tensor. Besides sum , min or max and some other opreations are made available for partial . The figures below show some examples of SBP, including split(0) , split(1) , broadcast and partial sum . When you create a Global Tensor, you can specify the SBP of the Tensor. The example will be seen in the next article: Global Tensor . SBP Signature \u00b6 SBP describes the mapping relationship between the data under the global view and the data on the local physical devices. When doing distributed training, OneFlow distributes the data to the local physical devices, computes the results according to the SBP attributes of the data. For an isolated Tensor, we can set its SBP attributes at will. However, for an operator with input and output data, we can not arbitrarily set the SBP attributes of its input and output. This is because arbitrarily setting the SBP attributes of an operator\u2019s input and output may not conform to the algorithm of the operator under global view. Let us discuss this problem with the example of matrix multiplication. Look at how the input and output SBP of matrix multiplication are combined to be legal and illegal in a distributed system with tow devices. Suppose, from the global view, that a matrix \\(A\\) with the shape $ \\((m, k)\\) is multiplied by a matrix \\(B\\) with the shape \\((k, n)\\) to get $y $, the shape of \\(y\\) must be \\((m, n)\\) . According to the rule of matrix multiplication, we can divide the matrix \\(A\\) into two matrices \\(A_0\\) and \\(A_1\\) by dimension 0, with the shapes of \\((m_0, k)\\) , \\((m_1, k)\\) respectively: Device 1: \\[ \\begin{matrix} A_0 \\times B = Y_0 \\\\ (m_0, k) (k, n) (m_0, n) \\end{matrix} \\] Device 2: \\[ \\begin{matrix} A_1 \\times B = Y_1 \\\\ (m_1, k) (k, n) (m_1, n) \\end{matrix} \\] It\u2019s easy to configure the relationship among local Tensors \\(A_0\\) , \\(A_1\\) and the Tensor \\(A\\) , which is under the global view. And also the relationship between \\(Y_0\\) , \\(Y_1\\) and the global view data \\(Y\\) : \\[ \\begin{matrix} A &= concat&(A_0 ,& A_1) \\\\ (m,k) & & (m_0, k) & (m_1, k) \\end{matrix} \\] \\[ \\begin{matrix} Y &= concat&(Y_0 ,& Y_1) \\\\ (m,n) & & (m_0, n) & (m_1, n) \\end{matrix} \\] Note: The concat above represents a concatenate operation. In this way, it is possible to execute the operation and get the correct result from the global view by distributing the data to each local physical device. The long story we talked above, described in SBP, are surprisingly simple: \\(A\\) is split(0) , \\(B\\) is broadcast , and \\(Y\\) is split(0) . We can see that for matrix multiplication, the SBP of its input and output combined in the above way, is legal. For matrix multiplication, there are more than one valid SBP combinations, such as: \\(A\\) is broadcast , \\(B\\) is split(1) , and \\(Y\\) is split(1) . Or: \\(A\\) is split(1) , \\(B\\) is split(0) , and \\(Y\\) is partial sum . While we showed multiple valid SBP combinations above, not all SBP combinations are valid. For example, for matrix multiplication, if \\(A\\) , \\(B\\) are both split(0) , then: \\[ \\begin{matrix} A &= concat&(A_0 ,& A_1) \\\\ (m,k) & & (m_0, k) & (m_1, k) \\end{matrix} \\] \\[ \\begin{matrix} B &= concat&(B_0 ,& B_1) \\\\ (k,n) & & (k_0, n) & (k_1, n) \\end{matrix} \\] Because the shapes of \\(A_0\\) and \\(B_0\\) do not meet the requirements of matrix multiplication, it is impossible to compute the matrix multiplication on local physical devices. We can say that the combination of \\(A\\) as split(0) and \\(B\\) as split(0) is illegal. We defines a specific, valid SBP combination of the inputs and outputs of an operator, as shown above, as a SBP Signature of this operator. \u200b Automatic derivation of SBP Signature \u00b6 \u200b With the concept of SBP Signature, we may ask several questions: \u200b - Does the user need to know all SBP Signature of operators before they use Oneflow for distributed training? - Does the user set the input SBP for each layer of the network as an algorithm engineer? \u200b For the previous question, the user certainly does not need to know all the SBP Signature of the operator. It is the responsibility of operator author to list all possible SBP Signature of an operator. According to the algorithm of the operator, the operator author has already listed and preset all possible SBP Signatures of the operator when developing it. \u200b We can also get the answer of the second question: because there is a preset SBP Signature, as long as a certain layer operator has an input SBP, OneFlow can deduce the SBP output by this layer of operator according to the SBP Signature. The output of the upstream operator is also the input of the downstream operator. In this way, the SBP input by the downstream operator is determined, and then its output can be determined according to the SBP Signature... It continues to derive and propagate. So in general the user does not need to set the SBP input for each layer of the network. Explicit specification is only required when the layer is initially input, or when it is necessary to force the SBP of a specific layer. \u200b The users may also have new questions: \u200b - There are multiple valid SBP Signatures for an operator. Which one will Oneflow choose when it runs? What is it based on? \u200b For this problem, you need to understand the SBP Signature automatic derivation mechanism in Oneflow. The automatic derivation of SBP Signature refers to: given all the valid SBP Signatures of operators, OneFlow has a set of algorithms that will score each valid SBP Signature based on the transmission cost, and select it with the least transmission cost. This maximizes the throughput efficiency of the system \u200b Boxing mechanism \u00b6 \u200b Strictly the Boxing mechanism of OneFlow is actually transparent to users. When users use OneFlow for distributed training, they do not need to know it. \u200b But some thoughtful users will ask some questions after understanding the automatic derivation of SBP Signature: \u200b - With the SBP Signature automatically selected by OneFlow if the output of the upper layer operator does not match the SBP attribute of the input of the lower layer operator , what should we do? \u200b For example, in the following code, the output SBP of the upper operator matmul was originally split(0) , but the input of the lower operator matmul was converted to broadcast . At this time, the SBP of upper layer's output and of lower layer's input are inconsistent. \u200b import oneflow as flow P0 = flow . placement ( \"cuda\" , ranks = [ 0 , 1 ]) P1 = flow . placement ( \"cuda\" , ranks = [ 2 , 3 ]) a0_sbp = flow . sbp . split ( 0 ) b0_sbp = flow . sbp . broadcast y0_sbp = flow . sbp . broadcast b1_sbp = flow . sbp . split ( 1 ) A0 = flow . randn ( 4 , 5 , placement = P0 , sbp = a0_sbp ) B0 = flow . randn ( 5 , 8 , placement = P0 , sbp = b0_sbp ) Y0 = flow . matmul ( A0 , B0 ) Y0 = Y0 . to_global ( placement = P1 , sbp = y0_sbp ) B1 = flow . randn ( 8 , 6 , placement = P1 , sbp = b1_sbp ) Y2 = flow . matmul ( Y0 , B1 ) \u200b In this case, Oneflow will detect the inconsistency and insert an operator between the upstream output and the downstream input to do the relevant conversion work. This type of operator that is automatically added for conversion is called Boxing operator \u3002 \u200b The corresponding relationship between the logical diagram and thephysical execution diagram of the above code is as follows: \u200b \u200b Conclusion \u00b6 \u200b placement , SBP and SBP Signature are the important guarantee of OneFlow distributed global view, which makes OneFlow distributed training as simple as on a single machine single card. \u200b Usually, the users only need to set SBP in the initial network layer, which can omit the trouble of handwritten communication operations in traditional distributed training. It is worth mentioning that, in addtion to the automatic derivation mechanism of SBP Signature introduced in this article, the OneFlow team is developing an automatic parallel method to find the global optimal solution, and it is under internal testing. After it goes online, the users can get a good distributed training effect without any SBP configuration. So stay tuned. \u200b In the next article Global Tensor , we\u2019ll show you an example of programming under the global view.","title":"Global View"},{"location":"parallelism/02_sbp.html#global-view","text":"The concept of global view in OneFlow is introduced to simplify distributed training. In short, the cluster is abstracted as a \"Super Computing Device\" under OneFlow global view. Instead of caring about the details of computing and communication in a cluster, users can program like on a single node, and OneFlow can train the model in a distributed way. OneFlow's global view relies on several important concepts: Placement , SBP and SBP Signature .","title":"GLOBAL VIEW"},{"location":"parallelism/02_sbp.html#placement","text":"The Tensors of OneFlow has a placement attribute in global view; the placement specifies which local physical device the Tensor is placed on. OneFlow will automatically number the devices in the cluster. For example, if there are four hosts in a cluster and each host has eight GPU cards, so that 32 cards in total. The 32 devices in the cluser will be numbered 0 to 31. To place a Tensor on the first four cards on machine 0, simply configure: placement(\"cuda\", [0, 1, 2, 3]) . To place a Tensor on the last four cards on machine 0, simply configure: placement(\"cuda\", [4, 5, 6, 7]) . Placement makes it easy for OneFlow to support pipelining parallelism, and we\u2019ll see examples of placement in other articles on this topic.","title":"Placement"},{"location":"parallelism/02_sbp.html#sbp","text":"SBP is a unique concept in OneFlow, which describes the mapping of data from a \"Super Computing Device\" perspective to data on local physical devices in a cluster. It is a combination of the initials of three words: split , broadcast , partial . In detail: split means that the local Tensor is obtained by splitting the global Tensor along a certain dimension. An axis parameter is used to indicate the dimension of the split. If multiple local Tensors are concatenated along the dimension of Split, the global Tensor can be restored. broadcast indicates that each local Tensor is exactly a copy of the global Tensor. partial indicates that although the local Tensor has the same shape as the global Tensor, the value in the local Tensor is a part of the value in the corresponding position in the global Tensor, if you add multiple local Tensors at the same positions, you can restore the global Tensor. Besides sum , min or max and some other opreations are made available for partial . The figures below show some examples of SBP, including split(0) , split(1) , broadcast and partial sum . When you create a Global Tensor, you can specify the SBP of the Tensor. The example will be seen in the next article: Global Tensor .","title":"SBP"},{"location":"parallelism/02_sbp.html#sbp-signature","text":"SBP describes the mapping relationship between the data under the global view and the data on the local physical devices. When doing distributed training, OneFlow distributes the data to the local physical devices, computes the results according to the SBP attributes of the data. For an isolated Tensor, we can set its SBP attributes at will. However, for an operator with input and output data, we can not arbitrarily set the SBP attributes of its input and output. This is because arbitrarily setting the SBP attributes of an operator\u2019s input and output may not conform to the algorithm of the operator under global view. Let us discuss this problem with the example of matrix multiplication. Look at how the input and output SBP of matrix multiplication are combined to be legal and illegal in a distributed system with tow devices. Suppose, from the global view, that a matrix \\(A\\) with the shape $ \\((m, k)\\) is multiplied by a matrix \\(B\\) with the shape \\((k, n)\\) to get $y $, the shape of \\(y\\) must be \\((m, n)\\) . According to the rule of matrix multiplication, we can divide the matrix \\(A\\) into two matrices \\(A_0\\) and \\(A_1\\) by dimension 0, with the shapes of \\((m_0, k)\\) , \\((m_1, k)\\) respectively: Device 1: \\[ \\begin{matrix} A_0 \\times B = Y_0 \\\\ (m_0, k) (k, n) (m_0, n) \\end{matrix} \\] Device 2: \\[ \\begin{matrix} A_1 \\times B = Y_1 \\\\ (m_1, k) (k, n) (m_1, n) \\end{matrix} \\] It\u2019s easy to configure the relationship among local Tensors \\(A_0\\) , \\(A_1\\) and the Tensor \\(A\\) , which is under the global view. And also the relationship between \\(Y_0\\) , \\(Y_1\\) and the global view data \\(Y\\) : \\[ \\begin{matrix} A &= concat&(A_0 ,& A_1) \\\\ (m,k) & & (m_0, k) & (m_1, k) \\end{matrix} \\] \\[ \\begin{matrix} Y &= concat&(Y_0 ,& Y_1) \\\\ (m,n) & & (m_0, n) & (m_1, n) \\end{matrix} \\] Note: The concat above represents a concatenate operation. In this way, it is possible to execute the operation and get the correct result from the global view by distributing the data to each local physical device. The long story we talked above, described in SBP, are surprisingly simple: \\(A\\) is split(0) , \\(B\\) is broadcast , and \\(Y\\) is split(0) . We can see that for matrix multiplication, the SBP of its input and output combined in the above way, is legal. For matrix multiplication, there are more than one valid SBP combinations, such as: \\(A\\) is broadcast , \\(B\\) is split(1) , and \\(Y\\) is split(1) . Or: \\(A\\) is split(1) , \\(B\\) is split(0) , and \\(Y\\) is partial sum . While we showed multiple valid SBP combinations above, not all SBP combinations are valid. For example, for matrix multiplication, if \\(A\\) , \\(B\\) are both split(0) , then: \\[ \\begin{matrix} A &= concat&(A_0 ,& A_1) \\\\ (m,k) & & (m_0, k) & (m_1, k) \\end{matrix} \\] \\[ \\begin{matrix} B &= concat&(B_0 ,& B_1) \\\\ (k,n) & & (k_0, n) & (k_1, n) \\end{matrix} \\] Because the shapes of \\(A_0\\) and \\(B_0\\) do not meet the requirements of matrix multiplication, it is impossible to compute the matrix multiplication on local physical devices. We can say that the combination of \\(A\\) as split(0) and \\(B\\) as split(0) is illegal. We defines a specific, valid SBP combination of the inputs and outputs of an operator, as shown above, as a SBP Signature of this operator. \u200b","title":"SBP Signature"},{"location":"parallelism/02_sbp.html#automatic-derivation-of-sbp-signature","text":"\u200b With the concept of SBP Signature, we may ask several questions: \u200b - Does the user need to know all SBP Signature of operators before they use Oneflow for distributed training? - Does the user set the input SBP for each layer of the network as an algorithm engineer? \u200b For the previous question, the user certainly does not need to know all the SBP Signature of the operator. It is the responsibility of operator author to list all possible SBP Signature of an operator. According to the algorithm of the operator, the operator author has already listed and preset all possible SBP Signatures of the operator when developing it. \u200b We can also get the answer of the second question: because there is a preset SBP Signature, as long as a certain layer operator has an input SBP, OneFlow can deduce the SBP output by this layer of operator according to the SBP Signature. The output of the upstream operator is also the input of the downstream operator. In this way, the SBP input by the downstream operator is determined, and then its output can be determined according to the SBP Signature... It continues to derive and propagate. So in general the user does not need to set the SBP input for each layer of the network. Explicit specification is only required when the layer is initially input, or when it is necessary to force the SBP of a specific layer. \u200b The users may also have new questions: \u200b - There are multiple valid SBP Signatures for an operator. Which one will Oneflow choose when it runs? What is it based on? \u200b For this problem, you need to understand the SBP Signature automatic derivation mechanism in Oneflow. The automatic derivation of SBP Signature refers to: given all the valid SBP Signatures of operators, OneFlow has a set of algorithms that will score each valid SBP Signature based on the transmission cost, and select it with the least transmission cost. This maximizes the throughput efficiency of the system \u200b","title":"Automatic derivation of SBP Signature"},{"location":"parallelism/02_sbp.html#boxing-mechanism","text":"\u200b Strictly the Boxing mechanism of OneFlow is actually transparent to users. When users use OneFlow for distributed training, they do not need to know it. \u200b But some thoughtful users will ask some questions after understanding the automatic derivation of SBP Signature: \u200b - With the SBP Signature automatically selected by OneFlow if the output of the upper layer operator does not match the SBP attribute of the input of the lower layer operator , what should we do? \u200b For example, in the following code, the output SBP of the upper operator matmul was originally split(0) , but the input of the lower operator matmul was converted to broadcast . At this time, the SBP of upper layer's output and of lower layer's input are inconsistent. \u200b import oneflow as flow P0 = flow . placement ( \"cuda\" , ranks = [ 0 , 1 ]) P1 = flow . placement ( \"cuda\" , ranks = [ 2 , 3 ]) a0_sbp = flow . sbp . split ( 0 ) b0_sbp = flow . sbp . broadcast y0_sbp = flow . sbp . broadcast b1_sbp = flow . sbp . split ( 1 ) A0 = flow . randn ( 4 , 5 , placement = P0 , sbp = a0_sbp ) B0 = flow . randn ( 5 , 8 , placement = P0 , sbp = b0_sbp ) Y0 = flow . matmul ( A0 , B0 ) Y0 = Y0 . to_global ( placement = P1 , sbp = y0_sbp ) B1 = flow . randn ( 8 , 6 , placement = P1 , sbp = b1_sbp ) Y2 = flow . matmul ( Y0 , B1 ) \u200b In this case, Oneflow will detect the inconsistency and insert an operator between the upstream output and the downstream input to do the relevant conversion work. This type of operator that is automatically added for conversion is called Boxing operator \u3002 \u200b The corresponding relationship between the logical diagram and thephysical execution diagram of the above code is as follows: \u200b \u200b","title":"Boxing mechanism"},{"location":"parallelism/02_sbp.html#conclusion","text":"\u200b placement , SBP and SBP Signature are the important guarantee of OneFlow distributed global view, which makes OneFlow distributed training as simple as on a single machine single card. \u200b Usually, the users only need to set SBP in the initial network layer, which can omit the trouble of handwritten communication operations in traditional distributed training. It is worth mentioning that, in addtion to the automatic derivation mechanism of SBP Signature introduced in this article, the OneFlow team is developing an automatic parallel method to find the global optimal solution, and it is under internal testing. After it goes online, the users can get a good distributed training effect without any SBP configuration. So stay tuned. \u200b In the next article Global Tensor , we\u2019ll show you an example of programming under the global view.","title":"Conclusion"},{"location":"parallelism/03_consistent_tensor.html","text":"GLOBAL TENSOR \u00b6 The Mapping Between Global View and Physical View \u00b6 Create Global Tensor \u00b6 To interactively experience global tensor on a two-GPU machine, you can launch python separately in two consoles in the following way. Note Click the Terminal 0 or Terminal 1 label to check the commands/code Terminal 0 export MASTER_ADDR = 127 .0.0.1 MASTER_PORT = 17789 WORLD_SIZE = 2 RANK = 0 LOCAL_RANK = 0 python3 Terminal 1 export MASTER_ADDR = 127 .0.0.1 MASTER_PORT = 17789 WORLD_SIZE = 2 RANK = 1 LOCAL_RANK = 1 python3 Setting environment variables prepares the machines for distributed computing. Please refer to the Extended Reading section at the end of this article for a detailed explanation and ways to launch distributed computing using provided tools. Create Global Tensor Directly \u00b6 In each of the two consoles, import oneflow and create x . flow.placement(\"cuda\", [0,1]) specifies the device to place the global tensors. \"cuda\" means \"on GPU\". The second parameter of placement is a dictionary. Its key is the index of machine, and its value is the index of the graphic cards. Therefore, {0:[0,1]} means that the global tensor is on the 0 th , 1 st graphic cards of the 0 th machine. Terminal 0 import oneflow as flow placement = flow . placement ( \"cuda\" , [ 0 , 1 ]) sbp = flow . sbp . split ( 0 ) x = flow . randn ( 4 , 5 , placement = placement , sbp = sbp ) x . shape Terminal 1 import oneflow as flow placement = flow . placement ( \"cuda\" , [ 0 , 1 ]) sbp = flow . sbp . split ( 0 ) x = flow . randn ( 4 , 5 , placement = placement , sbp = sbp ) x . shape Output: Terminal 0 flow.Size([4, 5]) Terminal 1 flow.Size([4, 5]) Get Local Tensor from Global Tensor \u00b6 Call to_local to check the local tensor on a device. Terminal 0 x . to_local () tensor ([[ 2.9186e-01 , - 3.9442e-01 , 4.7072e-04 , - 3.2216e-01 , 1.7788e-01 ], [ - 4.5284e-01 , 1.2361e-01 , - 3.5962e-01 , 2.6651e-01 , 1.2951e+00 ]], device = 'cuda:0' , dtype = oneflow . float32 ) Terminal 1 x . to_local () tensor ([[ - 0.4363 , 0.9985 , - 2.5387 , 0.3003 , 0.3803 ], [ 0.0556 , - 0.8077 , 1.1191 , - 2.1278 , 0.1468 ]], device = 'cuda:1' , dtype = oneflow . float32 ) Convert Local Tensor to Global Tensor \u00b6 Developers can create local tensor first, then convert it to global tensor with Tensor.to_global . Two local tensors with the shape of (2,5) are created separately on two devices. While after the to_global method, the global tensor with a shape of (4,5) is obtained. The reason for this transformation lies in that by setting the sbp with sbp=flow.sbp.split(0) , the two local tensors with the shape of (2, 5) are concatenated on the 0 th dimension. Terminal 0 import oneflow as flow x = flow . randn ( 2 , 5 ) placement = flow . placement ( \"cuda\" , [ 0 , 1 ]) sbp = flow . sbp . split ( 0 ) x_global = x . to_global ( placement = placement , sbp = sbp ) x_global . shape Terminal 1 import oneflow as flow x = flow . randn ( 2 , 5 ) placement = flow . placement ( \"cuda\" , [ 0 , 1 ]) sbp = flow . sbp . split ( 0 ) x_global = x . to_global ( placement = placement , sbp = sbp ) x_global . shape Practice with SBP Signature \u00b6 Data Parallelism \u00b6 The following code is an example of data parallelism of common distributed strategy Terminal 0 import oneflow as flow placement = flow . placement ( \"cuda\" , [ 0 , 1 ]) x = flow . randn ( 4 , 5 , placement = placement , sbp = flow . sbp . split ( 0 )) w = flow . randn ( 5 , 8 , placement = placement , sbp = flow . sbp . broadcast ) y = flow . matmul ( x , w ) y . sbp y . shape Terminal 1 import oneflow as flow placement = flow . placement ( \"cuda\" , [ 0 , 1 ]) x = flow . randn ( 4 , 5 , placement = placement , sbp = flow . sbp . split ( 0 )) w = flow . randn ( 5 , 8 , placement = placement , sbp = flow . sbp . broadcast ) y = flow . matmul ( x , w ) y . sbp y . shape flow.matmul supports many SBP signatures of inputs. When the SBP of x and w are split(0) and broadcast respectively, the SBP of output y is split(0) with logical shape of (4, 8) . Output: Terminal 0 (oneflow.sbp.split(dim=0),) flow.Size([4, 8]) Terminal 1 (oneflow.sbp.split(dim=0),) flow.Size([4, 8]) Model Parallelism \u00b6 The following code is an example of model parallelism of common distributed strategy . Terminal 0 import oneflow as flow placement = flow . placement ( \"cuda\" , [ 0 , 1 ]) x = flow . randn ( 4 , 5 , placement = placement , sbp = flow . sbp . broadcast ) w = flow . randn ( 5 , 8 , placement = placement , sbp = flow . sbp . split ( 1 )) y = flow . matmul ( x , w ) y . sbp y . shape Terminal 1 import oneflow as flow placement = flow . placement ( \"cuda\" , [ 0 , 1 ]) x = flow . randn ( 4 , 5 , placement = placement , sbp = flow . sbp . broadcast ) w = flow . randn ( 5 , 8 , placement = placement , sbp = flow . sbp . split ( 1 )) y = flow . matmul ( x , w ) y . sbp y . shape flow.matmul supports many SBP signatures of inputs. When the SBP of x and w are broadcast and split(0) respectively, the SBP of output y is split(1) with logical shape of (4, 8) . Output: Terminal 0 (oneflow.sbp.split(axis=1),) flow.Size([4, 8]) Terminal 1 (oneflow.sbp.split(axis=1),) flow.Size([4, 8]) Extended Reading \u00b6 Environment Variables in Multi-Machine Training \u00b6 As in the examples shown above, developers can manually launch the distributed training by setting the environment variables. In this way, developers can clearly see the effects and outputs in an interactive Python environment which is friendly for debugging. In production practice, developers can instead launch the distributed training with oneflow.distributed.launch . This module automatically sets necessary environment variables based on command-line arguments. MASTER_ADDR : The IP address of the 0 th machine in the cluster MASTER_PORT : The listening port of the 0 th machine in a multi-machine case. Note that this port should not be occupied by another process WORLD_SIZE : The number of computing devices in the whole cluster. Since there is currently no support for different numbers of GPUs on each machine, the number of WORLD_SIZE is actually \\(number\\:of\\:machines \\times number\\:of\\:GPUs\\:on\\:one\\:machine\\) . In our example, we have one machine and two GPUs on it, so WORLD_SIZE=2 RANK and LOCAL_RANK are indexes for processes. The difference is that RANK is a \"global perspective\" index, while LOCAL_RANK is a \"local perspective\" index. They are the same when only one machine is involved. In the above examples, we launch two processes on the same machine, so the RANK and LOCAL_RANK are the same. When launching the distributed training on multiple machines, the upper bound of LOCAL_RANK keeps the same with the number of computing devices on a single machine. The upper bound of RANK keeps the same with the sum of all computing devices in the cluster, with the indexing of processes starts from 0. (Both upper bounds are non-inclusive since indexing starts from 0) Assume that there are two machines and there are two graphic cards on each machine, we can sort out the correspondence between LOCAL_RANK and RANK RANK LOCAL_RANK GPU 0 on Machine 0 0 0 GPU 1 on Machine 0 1 1 GPU 0 on Machine 1 2 0 GPU 1 on Machine 1 3 1 Boxing\uff08Automatic Conversion of SBP\uff09 \u00b6 From the examples above, we learned that an operator can automatically set the SBP of the output tensor based on the SBP of the input tensor and the built-in SBP Signature of the operator. But what if the SBP of the output tensor does not satisfy the requirements of the next-layer operator? Assume that in model parallelism, there are two layers of matrix multiplication, and both layers use model parallelism. The SBP ( split(1) ) of the output from the first layer is not what the second layer expects ( broadcast ). In this case, OneFlow automatically inserts Boxing operation (AllGather) between the output of the first layer and the input of the second layer to perform necessary data movement. Converting split(1) to broadcast is equivalent to an AllGather operation, as shown in the figure below. Because of the Boxing mechanism, the developer only needs to set the SBP signature in a few key places (such as the source operator). The rest is all handled by the OneFlow framework and there is no need to insert the colletive communication operations manually.","title":"Global Tensor"},{"location":"parallelism/03_consistent_tensor.html#global-tensor","text":"","title":"GLOBAL TENSOR"},{"location":"parallelism/03_consistent_tensor.html#the-mapping-between-global-view-and-physical-view","text":"","title":"The Mapping Between Global View and Physical View"},{"location":"parallelism/03_consistent_tensor.html#create-global-tensor","text":"To interactively experience global tensor on a two-GPU machine, you can launch python separately in two consoles in the following way. Note Click the Terminal 0 or Terminal 1 label to check the commands/code Terminal 0 export MASTER_ADDR = 127 .0.0.1 MASTER_PORT = 17789 WORLD_SIZE = 2 RANK = 0 LOCAL_RANK = 0 python3 Terminal 1 export MASTER_ADDR = 127 .0.0.1 MASTER_PORT = 17789 WORLD_SIZE = 2 RANK = 1 LOCAL_RANK = 1 python3 Setting environment variables prepares the machines for distributed computing. Please refer to the Extended Reading section at the end of this article for a detailed explanation and ways to launch distributed computing using provided tools.","title":"Create Global Tensor"},{"location":"parallelism/03_consistent_tensor.html#create-global-tensor-directly","text":"In each of the two consoles, import oneflow and create x . flow.placement(\"cuda\", [0,1]) specifies the device to place the global tensors. \"cuda\" means \"on GPU\". The second parameter of placement is a dictionary. Its key is the index of machine, and its value is the index of the graphic cards. Therefore, {0:[0,1]} means that the global tensor is on the 0 th , 1 st graphic cards of the 0 th machine. Terminal 0 import oneflow as flow placement = flow . placement ( \"cuda\" , [ 0 , 1 ]) sbp = flow . sbp . split ( 0 ) x = flow . randn ( 4 , 5 , placement = placement , sbp = sbp ) x . shape Terminal 1 import oneflow as flow placement = flow . placement ( \"cuda\" , [ 0 , 1 ]) sbp = flow . sbp . split ( 0 ) x = flow . randn ( 4 , 5 , placement = placement , sbp = sbp ) x . shape Output: Terminal 0 flow.Size([4, 5]) Terminal 1 flow.Size([4, 5])","title":"Create Global Tensor Directly"},{"location":"parallelism/03_consistent_tensor.html#get-local-tensor-from-global-tensor","text":"Call to_local to check the local tensor on a device. Terminal 0 x . to_local () tensor ([[ 2.9186e-01 , - 3.9442e-01 , 4.7072e-04 , - 3.2216e-01 , 1.7788e-01 ], [ - 4.5284e-01 , 1.2361e-01 , - 3.5962e-01 , 2.6651e-01 , 1.2951e+00 ]], device = 'cuda:0' , dtype = oneflow . float32 ) Terminal 1 x . to_local () tensor ([[ - 0.4363 , 0.9985 , - 2.5387 , 0.3003 , 0.3803 ], [ 0.0556 , - 0.8077 , 1.1191 , - 2.1278 , 0.1468 ]], device = 'cuda:1' , dtype = oneflow . float32 )","title":"Get Local Tensor from Global Tensor"},{"location":"parallelism/03_consistent_tensor.html#convert-local-tensor-to-global-tensor","text":"Developers can create local tensor first, then convert it to global tensor with Tensor.to_global . Two local tensors with the shape of (2,5) are created separately on two devices. While after the to_global method, the global tensor with a shape of (4,5) is obtained. The reason for this transformation lies in that by setting the sbp with sbp=flow.sbp.split(0) , the two local tensors with the shape of (2, 5) are concatenated on the 0 th dimension. Terminal 0 import oneflow as flow x = flow . randn ( 2 , 5 ) placement = flow . placement ( \"cuda\" , [ 0 , 1 ]) sbp = flow . sbp . split ( 0 ) x_global = x . to_global ( placement = placement , sbp = sbp ) x_global . shape Terminal 1 import oneflow as flow x = flow . randn ( 2 , 5 ) placement = flow . placement ( \"cuda\" , [ 0 , 1 ]) sbp = flow . sbp . split ( 0 ) x_global = x . to_global ( placement = placement , sbp = sbp ) x_global . shape","title":"Convert Local Tensor to Global Tensor"},{"location":"parallelism/03_consistent_tensor.html#practice-with-sbp-signature","text":"","title":"Practice with SBP Signature"},{"location":"parallelism/03_consistent_tensor.html#data-parallelism","text":"The following code is an example of data parallelism of common distributed strategy Terminal 0 import oneflow as flow placement = flow . placement ( \"cuda\" , [ 0 , 1 ]) x = flow . randn ( 4 , 5 , placement = placement , sbp = flow . sbp . split ( 0 )) w = flow . randn ( 5 , 8 , placement = placement , sbp = flow . sbp . broadcast ) y = flow . matmul ( x , w ) y . sbp y . shape Terminal 1 import oneflow as flow placement = flow . placement ( \"cuda\" , [ 0 , 1 ]) x = flow . randn ( 4 , 5 , placement = placement , sbp = flow . sbp . split ( 0 )) w = flow . randn ( 5 , 8 , placement = placement , sbp = flow . sbp . broadcast ) y = flow . matmul ( x , w ) y . sbp y . shape flow.matmul supports many SBP signatures of inputs. When the SBP of x and w are split(0) and broadcast respectively, the SBP of output y is split(0) with logical shape of (4, 8) . Output: Terminal 0 (oneflow.sbp.split(dim=0),) flow.Size([4, 8]) Terminal 1 (oneflow.sbp.split(dim=0),) flow.Size([4, 8])","title":"Data Parallelism"},{"location":"parallelism/03_consistent_tensor.html#model-parallelism","text":"The following code is an example of model parallelism of common distributed strategy . Terminal 0 import oneflow as flow placement = flow . placement ( \"cuda\" , [ 0 , 1 ]) x = flow . randn ( 4 , 5 , placement = placement , sbp = flow . sbp . broadcast ) w = flow . randn ( 5 , 8 , placement = placement , sbp = flow . sbp . split ( 1 )) y = flow . matmul ( x , w ) y . sbp y . shape Terminal 1 import oneflow as flow placement = flow . placement ( \"cuda\" , [ 0 , 1 ]) x = flow . randn ( 4 , 5 , placement = placement , sbp = flow . sbp . broadcast ) w = flow . randn ( 5 , 8 , placement = placement , sbp = flow . sbp . split ( 1 )) y = flow . matmul ( x , w ) y . sbp y . shape flow.matmul supports many SBP signatures of inputs. When the SBP of x and w are broadcast and split(0) respectively, the SBP of output y is split(1) with logical shape of (4, 8) . Output: Terminal 0 (oneflow.sbp.split(axis=1),) flow.Size([4, 8]) Terminal 1 (oneflow.sbp.split(axis=1),) flow.Size([4, 8])","title":"Model Parallelism"},{"location":"parallelism/03_consistent_tensor.html#extended-reading","text":"","title":"Extended Reading"},{"location":"parallelism/03_consistent_tensor.html#environment-variables-in-multi-machine-training","text":"As in the examples shown above, developers can manually launch the distributed training by setting the environment variables. In this way, developers can clearly see the effects and outputs in an interactive Python environment which is friendly for debugging. In production practice, developers can instead launch the distributed training with oneflow.distributed.launch . This module automatically sets necessary environment variables based on command-line arguments. MASTER_ADDR : The IP address of the 0 th machine in the cluster MASTER_PORT : The listening port of the 0 th machine in a multi-machine case. Note that this port should not be occupied by another process WORLD_SIZE : The number of computing devices in the whole cluster. Since there is currently no support for different numbers of GPUs on each machine, the number of WORLD_SIZE is actually \\(number\\:of\\:machines \\times number\\:of\\:GPUs\\:on\\:one\\:machine\\) . In our example, we have one machine and two GPUs on it, so WORLD_SIZE=2 RANK and LOCAL_RANK are indexes for processes. The difference is that RANK is a \"global perspective\" index, while LOCAL_RANK is a \"local perspective\" index. They are the same when only one machine is involved. In the above examples, we launch two processes on the same machine, so the RANK and LOCAL_RANK are the same. When launching the distributed training on multiple machines, the upper bound of LOCAL_RANK keeps the same with the number of computing devices on a single machine. The upper bound of RANK keeps the same with the sum of all computing devices in the cluster, with the indexing of processes starts from 0. (Both upper bounds are non-inclusive since indexing starts from 0) Assume that there are two machines and there are two graphic cards on each machine, we can sort out the correspondence between LOCAL_RANK and RANK RANK LOCAL_RANK GPU 0 on Machine 0 0 0 GPU 1 on Machine 0 1 1 GPU 0 on Machine 1 2 0 GPU 1 on Machine 1 3 1","title":"Environment Variables in Multi-Machine Training"},{"location":"parallelism/03_consistent_tensor.html#boxingautomatic-conversion-of-sbp","text":"From the examples above, we learned that an operator can automatically set the SBP of the output tensor based on the SBP of the input tensor and the built-in SBP Signature of the operator. But what if the SBP of the output tensor does not satisfy the requirements of the next-layer operator? Assume that in model parallelism, there are two layers of matrix multiplication, and both layers use model parallelism. The SBP ( split(1) ) of the output from the first layer is not what the second layer expects ( broadcast ). In this case, OneFlow automatically inserts Boxing operation (AllGather) between the output of the first layer and the input of the second layer to perform necessary data movement. Converting split(1) to broadcast is equivalent to an AllGather operation, as shown in the figure below. Because of the Boxing mechanism, the developer only needs to set the SBP signature in a few key places (such as the source operator). The rest is all handled by the OneFlow framework and there is no need to insert the colletive communication operations manually.","title":"Boxing\uff08Automatic Conversion of SBP\uff09"},{"location":"parallelism/04_2d-sbp.html","text":"2D SBP \u00b6 After reading the Global View and Global Tensor , you may have learned about the basic concepts of SBP and SBP Signature, and can get started with related tasks. In fact, both these two documents refers to 1D SBP . Since you have known about 1D SBP, this document introduces 2D SBP, which can more flexibly deal with more complex distributed training scenarios. 2D Devices Array \u00b6 We are already familiar with the placement configuration of 1D SBP. In the scenario of 1D SBP, configure the cluster through the oneflow.placement interface. For example, use the 0~3 GPU graphics in the cluster: >>> placement1 = flow . placement ( \"cuda\" , ranks = [ 0 , 1 , 2 , 3 ]) The above \"cuda\" specifies the device type, and ranks=[0, 1, 2, 3] specifies the computing devices in the cluster. In fact, ranks can be not only a one-dimensional int list, but also a multi-dimensional int array: >>> placement2 = flow . placement ( \"cuda\" , ranks = [[ 0 , 1 ], [ 2 , 3 ]]) When ranks is in the form of a one-dimensional list like ranks=[0, 1, 2, 3] , all devices in the cluster form a 1D device vector, which is where the 1D SBP name comes from. When ranks is in the form of a multi-dimensional array, the devices in the cluster are grouped into a multi-dimensional array of devices. ranks=[[0, 1], [2, 3]] means that the four computing devices in the cluster are divided into a \\(2 \\times 2\\) device array. 2D SBP \u00b6 When constructing a Global Tensor, we need to specify both placement and SBP . When the cluster in placement is a 2-dimensional device array, SBP must also correspond to it, being a tuple with a length of 2. The 0 th and 1 st elements in this tuple respectively describes the distribution of Global Tensor in the 0 th and 1 st dimensions of the device array. For example, The following code configures a \\(2 \\times 2\\) device array, and sets the 2D SBP to (broadcast, split(0)) . >>> a = flow . Tensor ([[ 1 , 2 ],[ 3 , 4 ]]) >>> placement = flow . placement ( \"cuda\" , ranks = [[ 0 , 1 ], [ 2 , 3 ]]) >>> sbp = ( flow . sbp . broadcast , flow . sbp . split ( 0 )) >>> a_to_global = a . to_global ( placement = placement , sbp = sbp ) It means that logically the data, over the entire device array, is broadcast in the 0 th dimension (\"viewed vertically\"); split(0) in the 1 st dimension (\"viewed across\"). See the following figure: In the above figure, the left side is the global data, and the right side is the data of each device on the device array. As you can see, from the perspective of the 0 th dimension, they are all in broadcast relations: The data in (group0, device0) and (group1, device0) are consistent, and they are in broadcast relations to each other The data in (group0, device1) and (group1, device1) are consistent, and they are in broadcast relations to each other From the perspective of the 1 st dimension, they are all in split(0) relations: (group0, device0) and (group0, device1) are in split(0) relations to each other (group1, device0) and (group1, device1) are in split(0) relations to each other It may be difficult to directly understand the correspondence between logical data and physical data in the final device array. When thinking about 2D SBP, you can imagine an intermediate state (gray part in the above figure) there. Take (broadcast, split(0)) as an example: First, the original logical tensor is broadcast to 2 groups through broadcast , and the intermediate state is obtained On the basis of the intermediate state, split(0) is continued to be done on the groups to get the status of each physical tensor in the final device array 2D SBP Signature \u00b6 1D SBP has the concept of SBP signature, similarly, the operator also has 2D SBP signature. Based on mastering the concept of 1D SBP and its signature concept, 2D SBP signature is very simple and you only need to follow one principle: Independently derive in the respective dimensions Let's take matrix multiplication as an example. First, let's review the case of 1D SBP. Suppose that \\(x \\times w = y\\) can have the following SBP Signature: \\[ broadcast \\times split(1) = split(1) \\] and \\[ split(0) \\times broadcast = split(0) \\] Now, suppose we set the 2D SBP for \\(x\\) to \\((broadcast, split(0))\\) and set the 2D SBP for \\(w\\) to \\((split(1), broadcast)\\) , then in the context of the 2D SBP, operate \\(x \\times w = y\\) to obtain the SBP attribute for \\(y\\) is \\((split(1), split(0))\\) . That is to say, the following 2D SBPs constitute the 2D SBP Signature of matrix multiplication: \\[ (broadcast, split(0)) \\times (split(1), broadcast) = (split(1), split(0)) \\] An Example of Using 2D SBP \u00b6 In this section, we are going to use a simple example to demonstrate how to conduct distributed training using 2D SBP. Same as the example above, assume that there is a \\(2 \\times 2\\) device array. Given that readers may not have multiple GPU devices at present, we will use CPU to simulate the case of \\(2 \\times 2\\) device array. We adopt the parallelism strategy (broadcast, split(0)) in the above figure to the input tensor. First of all, import the dependencies: import oneflow as flow import oneflow.nn as nn Then, define the placement and SBP that will be used: PLACEMENT = flow . placement ( \"cpu\" , [[ 0 , 1 ], [ 2 , 3 ]]) BROADCAST = ( flow . sbp . broadcast , flow . sbp . broadcast ) BS0 = ( flow . sbp . broadcast , flow . sbp . split ( 0 )) The parameter ranks of PLACEMENT is a two-dimensional list, which represents that the devices in the cluster are divided into a device array of \\(2 \\times 2\\) . As mentioned earlier, the SBP needs to correspond to it and be specified as a tuple with a length of 2. BROADCAST means broadcasting on both the 0 th and 1 st dimensions of the device array, and the meaning of BS0 is the same as the description above. Assume that we have the following model: model = nn . Sequential ( nn . Linear ( 8 , 4 ), nn . ReLU (), nn . Linear ( 4 , 2 )) Broadcast the model on the cluster: model = model . to_global ( placement = PLACEMENT , sbp = BROADCAST ) And construct the data and carry out forward inference: x = flow . randn ( 1 , 2 , 8 ) global_x = x . to_global ( placement = PLACEMENT , sbp = BS0 ) pred = model ( global_x ) Here, we create a local tensor with shape (1, 2, 8) , and obtain the corresponding global tensor through Tensor.to_global method. Finally, input it to the model for inference. After obtaining the local tensor on current physical device through Tensor.to_local method, we can output its shape and value to verify whether the data has been processed correctly: local_x = global_x . to_local () print ( f ' { local_x . device } , { local_x . shape } , \\n { local_x } ' ) The output result is: cpu:2, oneflow.Size([1, 2, 8]), tensor([[[ 0.6068, 0.1986, -0.6363, -0.5572, -0.2388, 1.1607, -0.7186, 1.2161], [-0.1632, -1.5293, -0.6637, -1.0219, 0.1464, 1.1574, -0.0811, -1.6568]]], dtype=oneflow.float32) cpu:3, oneflow.Size([1, 2, 8]), tensor([[[-0.7676, 0.4519, -0.8810, 0.5648, 1.5428, 0.5752, 0.2466, -0.7708], [-1.2131, 1.4590, 0.2749, 0.8824, -0.8286, 0.9989, 0.5599, -0.5099]]], dtype=oneflow.float32) cpu:1, oneflow.Size([1, 2, 8]), tensor([[[-0.7676, 0.4519, -0.8810, 0.5648, 1.5428, 0.5752, 0.2466, -0.7708], [-1.2131, 1.4590, 0.2749, 0.8824, -0.8286, 0.9989, 0.5599, -0.5099]]], dtype=oneflow.float32) cpu:0, oneflow.Size([1, 2, 8]), tensor([[[ 0.6068, 0.1986, -0.6363, -0.5572, -0.2388, 1.1607, -0.7186, 1.2161], [-0.1632, -1.5293, -0.6637, -1.0219, 0.1464, 1.1574, -0.0811, -1.6568]]], dtype=oneflow.float32) Through comparing these local tensors on different \"devices\", we can see that it conforms to the state described in the figure above, which proves that the data has been splitted correctly. It should be noted that we cannot directly use python xxx.py to run the above code, but need to launch through oneflow.distributed.launch . This module can easily start distributed training. Execute the following command in the terminal (It is assumed that the above code has been saved to a file named \"2d_sbp.py\" in the current directory) python3 -m oneflow.distributed.launch --nproc_per_node = 4 2d_sbp.py Here, the parameter nproc_per_node is assigned as 4 to create 4 processes, simulating a total of 4 GPUs. For detailed usage of this module, please read: DISTRIBUTED TRAINING LAUNCHER . The complete code is as follows: Code PLACEMENT = flow . placement ( \"cpu\" , [[ 0 , 1 ], [ 2 , 3 ]]) BROADCAST = ( flow . sbp . broadcast , flow . sbp . broadcast ) BS0 = ( flow . sbp . broadcast , flow . sbp . split ( 0 )) model = nn . Sequential ( nn . Linear ( 8 , 4 ), nn . ReLU (), nn . Linear ( 4 , 2 )) model = model . to_global ( placement = PLACEMENT , sbp = BROADCAST ) x = flow . randn ( 1 , 2 , 8 ) global_x = x . to_global ( placement = PLACEMENT , sbp = BS0 ) pred = model ( global_x ) local_x = global_x . to_local () print ( f ' { local_x . device } , { local_x . shape } , \\n { local_x } ' )","title":"2D SBP"},{"location":"parallelism/04_2d-sbp.html#2d-sbp","text":"After reading the Global View and Global Tensor , you may have learned about the basic concepts of SBP and SBP Signature, and can get started with related tasks. In fact, both these two documents refers to 1D SBP . Since you have known about 1D SBP, this document introduces 2D SBP, which can more flexibly deal with more complex distributed training scenarios.","title":"2D SBP"},{"location":"parallelism/04_2d-sbp.html#2d-devices-array","text":"We are already familiar with the placement configuration of 1D SBP. In the scenario of 1D SBP, configure the cluster through the oneflow.placement interface. For example, use the 0~3 GPU graphics in the cluster: >>> placement1 = flow . placement ( \"cuda\" , ranks = [ 0 , 1 , 2 , 3 ]) The above \"cuda\" specifies the device type, and ranks=[0, 1, 2, 3] specifies the computing devices in the cluster. In fact, ranks can be not only a one-dimensional int list, but also a multi-dimensional int array: >>> placement2 = flow . placement ( \"cuda\" , ranks = [[ 0 , 1 ], [ 2 , 3 ]]) When ranks is in the form of a one-dimensional list like ranks=[0, 1, 2, 3] , all devices in the cluster form a 1D device vector, which is where the 1D SBP name comes from. When ranks is in the form of a multi-dimensional array, the devices in the cluster are grouped into a multi-dimensional array of devices. ranks=[[0, 1], [2, 3]] means that the four computing devices in the cluster are divided into a \\(2 \\times 2\\) device array.","title":"2D Devices Array"},{"location":"parallelism/04_2d-sbp.html#2d-sbp_1","text":"When constructing a Global Tensor, we need to specify both placement and SBP . When the cluster in placement is a 2-dimensional device array, SBP must also correspond to it, being a tuple with a length of 2. The 0 th and 1 st elements in this tuple respectively describes the distribution of Global Tensor in the 0 th and 1 st dimensions of the device array. For example, The following code configures a \\(2 \\times 2\\) device array, and sets the 2D SBP to (broadcast, split(0)) . >>> a = flow . Tensor ([[ 1 , 2 ],[ 3 , 4 ]]) >>> placement = flow . placement ( \"cuda\" , ranks = [[ 0 , 1 ], [ 2 , 3 ]]) >>> sbp = ( flow . sbp . broadcast , flow . sbp . split ( 0 )) >>> a_to_global = a . to_global ( placement = placement , sbp = sbp ) It means that logically the data, over the entire device array, is broadcast in the 0 th dimension (\"viewed vertically\"); split(0) in the 1 st dimension (\"viewed across\"). See the following figure: In the above figure, the left side is the global data, and the right side is the data of each device on the device array. As you can see, from the perspective of the 0 th dimension, they are all in broadcast relations: The data in (group0, device0) and (group1, device0) are consistent, and they are in broadcast relations to each other The data in (group0, device1) and (group1, device1) are consistent, and they are in broadcast relations to each other From the perspective of the 1 st dimension, they are all in split(0) relations: (group0, device0) and (group0, device1) are in split(0) relations to each other (group1, device0) and (group1, device1) are in split(0) relations to each other It may be difficult to directly understand the correspondence between logical data and physical data in the final device array. When thinking about 2D SBP, you can imagine an intermediate state (gray part in the above figure) there. Take (broadcast, split(0)) as an example: First, the original logical tensor is broadcast to 2 groups through broadcast , and the intermediate state is obtained On the basis of the intermediate state, split(0) is continued to be done on the groups to get the status of each physical tensor in the final device array","title":"2D SBP"},{"location":"parallelism/04_2d-sbp.html#2d-sbp-signature","text":"1D SBP has the concept of SBP signature, similarly, the operator also has 2D SBP signature. Based on mastering the concept of 1D SBP and its signature concept, 2D SBP signature is very simple and you only need to follow one principle: Independently derive in the respective dimensions Let's take matrix multiplication as an example. First, let's review the case of 1D SBP. Suppose that \\(x \\times w = y\\) can have the following SBP Signature: \\[ broadcast \\times split(1) = split(1) \\] and \\[ split(0) \\times broadcast = split(0) \\] Now, suppose we set the 2D SBP for \\(x\\) to \\((broadcast, split(0))\\) and set the 2D SBP for \\(w\\) to \\((split(1), broadcast)\\) , then in the context of the 2D SBP, operate \\(x \\times w = y\\) to obtain the SBP attribute for \\(y\\) is \\((split(1), split(0))\\) . That is to say, the following 2D SBPs constitute the 2D SBP Signature of matrix multiplication: \\[ (broadcast, split(0)) \\times (split(1), broadcast) = (split(1), split(0)) \\]","title":"2D SBP Signature"},{"location":"parallelism/04_2d-sbp.html#an-example-of-using-2d-sbp","text":"In this section, we are going to use a simple example to demonstrate how to conduct distributed training using 2D SBP. Same as the example above, assume that there is a \\(2 \\times 2\\) device array. Given that readers may not have multiple GPU devices at present, we will use CPU to simulate the case of \\(2 \\times 2\\) device array. We adopt the parallelism strategy (broadcast, split(0)) in the above figure to the input tensor. First of all, import the dependencies: import oneflow as flow import oneflow.nn as nn Then, define the placement and SBP that will be used: PLACEMENT = flow . placement ( \"cpu\" , [[ 0 , 1 ], [ 2 , 3 ]]) BROADCAST = ( flow . sbp . broadcast , flow . sbp . broadcast ) BS0 = ( flow . sbp . broadcast , flow . sbp . split ( 0 )) The parameter ranks of PLACEMENT is a two-dimensional list, which represents that the devices in the cluster are divided into a device array of \\(2 \\times 2\\) . As mentioned earlier, the SBP needs to correspond to it and be specified as a tuple with a length of 2. BROADCAST means broadcasting on both the 0 th and 1 st dimensions of the device array, and the meaning of BS0 is the same as the description above. Assume that we have the following model: model = nn . Sequential ( nn . Linear ( 8 , 4 ), nn . ReLU (), nn . Linear ( 4 , 2 )) Broadcast the model on the cluster: model = model . to_global ( placement = PLACEMENT , sbp = BROADCAST ) And construct the data and carry out forward inference: x = flow . randn ( 1 , 2 , 8 ) global_x = x . to_global ( placement = PLACEMENT , sbp = BS0 ) pred = model ( global_x ) Here, we create a local tensor with shape (1, 2, 8) , and obtain the corresponding global tensor through Tensor.to_global method. Finally, input it to the model for inference. After obtaining the local tensor on current physical device through Tensor.to_local method, we can output its shape and value to verify whether the data has been processed correctly: local_x = global_x . to_local () print ( f ' { local_x . device } , { local_x . shape } , \\n { local_x } ' ) The output result is: cpu:2, oneflow.Size([1, 2, 8]), tensor([[[ 0.6068, 0.1986, -0.6363, -0.5572, -0.2388, 1.1607, -0.7186, 1.2161], [-0.1632, -1.5293, -0.6637, -1.0219, 0.1464, 1.1574, -0.0811, -1.6568]]], dtype=oneflow.float32) cpu:3, oneflow.Size([1, 2, 8]), tensor([[[-0.7676, 0.4519, -0.8810, 0.5648, 1.5428, 0.5752, 0.2466, -0.7708], [-1.2131, 1.4590, 0.2749, 0.8824, -0.8286, 0.9989, 0.5599, -0.5099]]], dtype=oneflow.float32) cpu:1, oneflow.Size([1, 2, 8]), tensor([[[-0.7676, 0.4519, -0.8810, 0.5648, 1.5428, 0.5752, 0.2466, -0.7708], [-1.2131, 1.4590, 0.2749, 0.8824, -0.8286, 0.9989, 0.5599, -0.5099]]], dtype=oneflow.float32) cpu:0, oneflow.Size([1, 2, 8]), tensor([[[ 0.6068, 0.1986, -0.6363, -0.5572, -0.2388, 1.1607, -0.7186, 1.2161], [-0.1632, -1.5293, -0.6637, -1.0219, 0.1464, 1.1574, -0.0811, -1.6568]]], dtype=oneflow.float32) Through comparing these local tensors on different \"devices\", we can see that it conforms to the state described in the figure above, which proves that the data has been splitted correctly. It should be noted that we cannot directly use python xxx.py to run the above code, but need to launch through oneflow.distributed.launch . This module can easily start distributed training. Execute the following command in the terminal (It is assumed that the above code has been saved to a file named \"2d_sbp.py\" in the current directory) python3 -m oneflow.distributed.launch --nproc_per_node = 4 2d_sbp.py Here, the parameter nproc_per_node is assigned as 4 to create 4 processes, simulating a total of 4 GPUs. For detailed usage of this module, please read: DISTRIBUTED TRAINING LAUNCHER . The complete code is as follows: Code PLACEMENT = flow . placement ( \"cpu\" , [[ 0 , 1 ], [ 2 , 3 ]]) BROADCAST = ( flow . sbp . broadcast , flow . sbp . broadcast ) BS0 = ( flow . sbp . broadcast , flow . sbp . split ( 0 )) model = nn . Sequential ( nn . Linear ( 8 , 4 ), nn . ReLU (), nn . Linear ( 4 , 2 )) model = model . to_global ( placement = PLACEMENT , sbp = BROADCAST ) x = flow . randn ( 1 , 2 , 8 ) global_x = x . to_global ( placement = PLACEMENT , sbp = BS0 ) pred = model ( global_x ) local_x = global_x . to_local () print ( f ' { local_x . device } , { local_x . shape } , \\n { local_x } ' )","title":"An Example of Using 2D SBP"},{"location":"parallelism/04_launch.html","text":"DISTRIBUTED TRAINING LAUNCHER \u00b6 OneFlow provides the oneflow.distributed.launch module to help users start distributed training more conveniently. Users can start distributed training by the following commands: python3 -m oneflow.distributed.launch [ Boot Option ] training_script.py For example, to start the training on single-node double-GPUs: python3 -m oneflow.distributed.launch --nproc_per_node 2 ./script.py For another example, start two machines, and each machine has two graphics for training. Run on machine 0: python3 -m oneflow.distributed.launch --nproc_per_node = 2 \\ --nnodes = 2 \\ --node_rank = 0 \\ --nproc_per_node = 2 \\ --master_addr = \"192.168.1.1\" \\ --master_port = 7788 \\ script.py Run on machine 1: python3 -m oneflow.distributed.launch --nproc_per_node = 2 \\ --nnodes = 2 \\ --node_rank = 1 \\ --nproc_per_node = 2 \\ --master_addr = \"192.168.1.1\" \\ --master_port = 7788 \\ script.py Description of Common Options \u00b6 We can view the description of the options of the launch module after running python3 -m oneflow.distributed.launch -h . The following are some common options: --nnodes : number of nodes --node_rank : the serial number of the machines, starting from 0 --nproc_per_node : The number of processes per node to be started on each machine, which is recommended to be global with the number of GPUs --logdir : The relative storage path of the child process log The Relationship between Launch Module and Parallel Strategy \u00b6 The main function of oneflow.distributed.launch is to allow users to start distributed training more conveniently after them complete the distributed program. It saves the trouble of configuring environment variables in the cluster. But oneflow.distributed.launch does not determine Parallel Strategy . The Parallel Strategy is determined by the setup of the distribution method of data and the model, and the placement of those on the physical devices. OneFlow provides Global View and Global Tensor to flexibly configure parallel strategies. And for data parallelism, OneFlow provides the DistributedDataParallel module, which can change the single-node single-GPU script to the script of data parallel with minimal code modification.","title":"Distributed Training Launcher"},{"location":"parallelism/04_launch.html#distributed-training-launcher","text":"OneFlow provides the oneflow.distributed.launch module to help users start distributed training more conveniently. Users can start distributed training by the following commands: python3 -m oneflow.distributed.launch [ Boot Option ] training_script.py For example, to start the training on single-node double-GPUs: python3 -m oneflow.distributed.launch --nproc_per_node 2 ./script.py For another example, start two machines, and each machine has two graphics for training. Run on machine 0: python3 -m oneflow.distributed.launch --nproc_per_node = 2 \\ --nnodes = 2 \\ --node_rank = 0 \\ --nproc_per_node = 2 \\ --master_addr = \"192.168.1.1\" \\ --master_port = 7788 \\ script.py Run on machine 1: python3 -m oneflow.distributed.launch --nproc_per_node = 2 \\ --nnodes = 2 \\ --node_rank = 1 \\ --nproc_per_node = 2 \\ --master_addr = \"192.168.1.1\" \\ --master_port = 7788 \\ script.py","title":"DISTRIBUTED TRAINING LAUNCHER"},{"location":"parallelism/04_launch.html#description-of-common-options","text":"We can view the description of the options of the launch module after running python3 -m oneflow.distributed.launch -h . The following are some common options: --nnodes : number of nodes --node_rank : the serial number of the machines, starting from 0 --nproc_per_node : The number of processes per node to be started on each machine, which is recommended to be global with the number of GPUs --logdir : The relative storage path of the child process log","title":"Description of Common Options"},{"location":"parallelism/04_launch.html#the-relationship-between-launch-module-and-parallel-strategy","text":"The main function of oneflow.distributed.launch is to allow users to start distributed training more conveniently after them complete the distributed program. It saves the trouble of configuring environment variables in the cluster. But oneflow.distributed.launch does not determine Parallel Strategy . The Parallel Strategy is determined by the setup of the distribution method of data and the model, and the placement of those on the physical devices. OneFlow provides Global View and Global Tensor to flexibly configure parallel strategies. And for data parallelism, OneFlow provides the DistributedDataParallel module, which can change the single-node single-GPU script to the script of data parallel with minimal code modification.","title":"The Relationship between Launch Module and Parallel Strategy"},{"location":"parallelism/05_ddp.html","text":"DATA PARALLELISM TRAINING \u00b6 In Common Distributed Parallel Strategies , we introduced the characteristics of data parallel. OneFlow provides two ways to accomplish data parallel, and one of them is to use the original concept of Oneflow to run data parallel training by configurating global tensor. This is also the recommanded way to run data parallel training on Oneflow. Besides, to facilitate the users who are transferring from PyTorch to OneFlow, OneFlow offers the interface consistent with PyTorch torch.nn.parallel.DistributedDataParallel , oneflow.nn.parallel.DistributedDataParallel so that users can also conveniently extend single machine training to data parallel training. Run Data Parallel Training With SBP Configuration \u00b6 The following code runs data parallel training by configurating global tensor. Code import oneflow as flow import oneflow.nn as nn import flowvision import flowvision.transforms as transforms BATCH_SIZE = 64 EPOCH_NUM = 1 PLACEMENT = flow . placement ( \"cuda\" , [ 0 , 1 ]) S0 = flow . sbp . split ( 0 ) B = flow . sbp . broadcast DEVICE = \"cuda\" if flow . cuda . is_available () else \"cpu\" print ( \"Using {} device\" . format ( DEVICE )) training_data = flowvision . datasets . CIFAR10 ( root = \"data\" , train = True , transform = transforms . ToTensor (), download = True , ) train_dataloader = flow . utils . data . DataLoader ( training_data , BATCH_SIZE , shuffle = True ) model = flowvision . models . mobilenet_v2 () . to ( DEVICE ) model . classifer = nn . Sequential ( nn . Dropout ( 0.2 ), nn . Linear ( model . last_channel , 10 )) model = model . to_global ( placement = PLACEMENT , sbp = B ) loss_fn = nn . CrossEntropyLoss () . to ( DEVICE ) optimizer = flow . optim . SGD ( model . parameters (), lr = 1e-3 ) for t in range ( EPOCH_NUM ): print ( f \"Epoch { t + 1 } \\n -------------------------------\" ) size = len ( train_dataloader . dataset ) for batch , ( x , y ) in enumerate ( train_dataloader ): x = x . to_global ( placement = PLACEMENT , sbp = S0 ) y = y . to_global ( placement = PLACEMENT , sbp = S0 ) # Compute prediction error pred = model ( x ) loss = loss_fn ( pred , y ) # Backpropagation optimizer . zero_grad () loss . backward () optimizer . step () current = batch * BATCH_SIZE if batch % 5 == 0 : print ( f \"loss: { loss : >7f } [ { current : >5d } / { size : >5d } ]\" ) We can see that this script is almost identical to the training script for the single-machine and single-device configuration, with a few exceptions on some lines related to consitent tensor set-ups. These codes are: Set placement to place the training one GPU 0 and 1: PLACEMENT = flow . placement ( \"cuda\" , [ 0 , 1 ]) Broadcast the model on clusters: model = model . to_global ( placement = PLACEMENT , sbp = B ) Split the data on cluster with split(0) : x = x . to_global ( placement = PLACEMENT , sbp = S0 ) y = y . to_global ( placement = PLACEMENT , sbp = S0 ) This allows us to follow the instructions in Common Distributed Parallel Strategies Run Data Parallel Training With DistributedDataParallel \u00b6 The following codes provides a quick start for training data parallel with oneflow.nn.parallel.DistributedDataParallel : wget https://docs.oneflow.org/master/code/parallelism/ddp_train.py #Download python3 -m oneflow.distributed.launch --nproc_per_node 2 ./ddp_train.py #data parallel training Out\uff1a 50/500 loss:0.004111831542104483 50/500 loss:0.00025336415274068713 ... 500/500 loss:6.184563972055912e-11 500/500 loss:4.547473508864641e-12 w:tensor([[2.0000], [3.0000]], device='cuda:1', dtype=oneflow.float32, grad_fn=<accumulate_grad>) w:tensor([[2.0000], [3.0000]], device='cuda:0', dtype=oneflow.float32, grad_fn=<accumulate_grad>) Click \"Code\" below to expand the code of the above running script. Code import oneflow as flow from oneflow.nn.parallel import DistributedDataParallel as ddp train_x = [ flow . tensor ([[ 1 , 2 ], [ 2 , 3 ]], dtype = flow . float32 ), flow . tensor ([[ 4 , 6 ], [ 3 , 1 ]], dtype = flow . float32 ), ] train_y = [ flow . tensor ([[ 8 ], [ 13 ]], dtype = flow . float32 ), flow . tensor ([[ 26 ], [ 9 ]], dtype = flow . float32 ), ] class Model ( flow . nn . Module ): def __init__ ( self ): super () . __init__ () self . lr = 0.01 self . iter_count = 500 self . w = flow . nn . Parameter ( flow . tensor ([[ 0 ], [ 0 ]], dtype = flow . float32 )) def forward ( self , x ): x = flow . matmul ( x , self . w ) return x m = Model () . to ( \"cuda\" ) m = ddp ( m ) loss = flow . nn . MSELoss ( reduction = \"sum\" ) optimizer = flow . optim . SGD ( m . parameters (), m . lr ) for i in range ( 0 , m . iter_count ): rank = flow . env . get_rank () x = train_x [ rank ] . to ( \"cuda\" ) y = train_y [ rank ] . to ( \"cuda\" ) y_pred = m ( x ) l = loss ( y_pred , y ) if ( i + 1 ) % 50 == 0 : print ( f \" { i + 1 } / { m . iter_count } loss: { l } \" ) optimizer . zero_grad () l . backward () optimizer . step () print ( f \" \\n w: { m . w } \" ) There are only two differences between the data parallelism training code and the stand-alone single-card script: Use DistributedDataParallel to wrap the module object ( m = ddp(m) ) Use get_rank to get the current device number and distribute the data to the device. Then use launcher to run the script, leave everything else to OneFlow, which makes distributed training as simple as stand-alone single-card training: python3 - m oneflow . distributed . launch -- nproc_per_node 2 ./ ddp_train . py DistributedSampler \u00b6 The data used is manually distributed in this context to highlight DistributedDataParallel . However, in practical applications, you can directly use DistributedSampler with data parallel. DistributedSampler will instantiate the Dataloader in each process, and each Dataloader instance will load a part of the complete data to automatically complete the data distribution.","title":"Data Parallelism Training"},{"location":"parallelism/05_ddp.html#data-parallelism-training","text":"In Common Distributed Parallel Strategies , we introduced the characteristics of data parallel. OneFlow provides two ways to accomplish data parallel, and one of them is to use the original concept of Oneflow to run data parallel training by configurating global tensor. This is also the recommanded way to run data parallel training on Oneflow. Besides, to facilitate the users who are transferring from PyTorch to OneFlow, OneFlow offers the interface consistent with PyTorch torch.nn.parallel.DistributedDataParallel , oneflow.nn.parallel.DistributedDataParallel so that users can also conveniently extend single machine training to data parallel training.","title":"DATA PARALLELISM TRAINING"},{"location":"parallelism/05_ddp.html#run-data-parallel-training-with-sbp-configuration","text":"The following code runs data parallel training by configurating global tensor. Code import oneflow as flow import oneflow.nn as nn import flowvision import flowvision.transforms as transforms BATCH_SIZE = 64 EPOCH_NUM = 1 PLACEMENT = flow . placement ( \"cuda\" , [ 0 , 1 ]) S0 = flow . sbp . split ( 0 ) B = flow . sbp . broadcast DEVICE = \"cuda\" if flow . cuda . is_available () else \"cpu\" print ( \"Using {} device\" . format ( DEVICE )) training_data = flowvision . datasets . CIFAR10 ( root = \"data\" , train = True , transform = transforms . ToTensor (), download = True , ) train_dataloader = flow . utils . data . DataLoader ( training_data , BATCH_SIZE , shuffle = True ) model = flowvision . models . mobilenet_v2 () . to ( DEVICE ) model . classifer = nn . Sequential ( nn . Dropout ( 0.2 ), nn . Linear ( model . last_channel , 10 )) model = model . to_global ( placement = PLACEMENT , sbp = B ) loss_fn = nn . CrossEntropyLoss () . to ( DEVICE ) optimizer = flow . optim . SGD ( model . parameters (), lr = 1e-3 ) for t in range ( EPOCH_NUM ): print ( f \"Epoch { t + 1 } \\n -------------------------------\" ) size = len ( train_dataloader . dataset ) for batch , ( x , y ) in enumerate ( train_dataloader ): x = x . to_global ( placement = PLACEMENT , sbp = S0 ) y = y . to_global ( placement = PLACEMENT , sbp = S0 ) # Compute prediction error pred = model ( x ) loss = loss_fn ( pred , y ) # Backpropagation optimizer . zero_grad () loss . backward () optimizer . step () current = batch * BATCH_SIZE if batch % 5 == 0 : print ( f \"loss: { loss : >7f } [ { current : >5d } / { size : >5d } ]\" ) We can see that this script is almost identical to the training script for the single-machine and single-device configuration, with a few exceptions on some lines related to consitent tensor set-ups. These codes are: Set placement to place the training one GPU 0 and 1: PLACEMENT = flow . placement ( \"cuda\" , [ 0 , 1 ]) Broadcast the model on clusters: model = model . to_global ( placement = PLACEMENT , sbp = B ) Split the data on cluster with split(0) : x = x . to_global ( placement = PLACEMENT , sbp = S0 ) y = y . to_global ( placement = PLACEMENT , sbp = S0 ) This allows us to follow the instructions in Common Distributed Parallel Strategies","title":"Run Data Parallel Training With SBP Configuration"},{"location":"parallelism/05_ddp.html#run-data-parallel-training-with-distributeddataparallel","text":"The following codes provides a quick start for training data parallel with oneflow.nn.parallel.DistributedDataParallel : wget https://docs.oneflow.org/master/code/parallelism/ddp_train.py #Download python3 -m oneflow.distributed.launch --nproc_per_node 2 ./ddp_train.py #data parallel training Out\uff1a 50/500 loss:0.004111831542104483 50/500 loss:0.00025336415274068713 ... 500/500 loss:6.184563972055912e-11 500/500 loss:4.547473508864641e-12 w:tensor([[2.0000], [3.0000]], device='cuda:1', dtype=oneflow.float32, grad_fn=<accumulate_grad>) w:tensor([[2.0000], [3.0000]], device='cuda:0', dtype=oneflow.float32, grad_fn=<accumulate_grad>) Click \"Code\" below to expand the code of the above running script. Code import oneflow as flow from oneflow.nn.parallel import DistributedDataParallel as ddp train_x = [ flow . tensor ([[ 1 , 2 ], [ 2 , 3 ]], dtype = flow . float32 ), flow . tensor ([[ 4 , 6 ], [ 3 , 1 ]], dtype = flow . float32 ), ] train_y = [ flow . tensor ([[ 8 ], [ 13 ]], dtype = flow . float32 ), flow . tensor ([[ 26 ], [ 9 ]], dtype = flow . float32 ), ] class Model ( flow . nn . Module ): def __init__ ( self ): super () . __init__ () self . lr = 0.01 self . iter_count = 500 self . w = flow . nn . Parameter ( flow . tensor ([[ 0 ], [ 0 ]], dtype = flow . float32 )) def forward ( self , x ): x = flow . matmul ( x , self . w ) return x m = Model () . to ( \"cuda\" ) m = ddp ( m ) loss = flow . nn . MSELoss ( reduction = \"sum\" ) optimizer = flow . optim . SGD ( m . parameters (), m . lr ) for i in range ( 0 , m . iter_count ): rank = flow . env . get_rank () x = train_x [ rank ] . to ( \"cuda\" ) y = train_y [ rank ] . to ( \"cuda\" ) y_pred = m ( x ) l = loss ( y_pred , y ) if ( i + 1 ) % 50 == 0 : print ( f \" { i + 1 } / { m . iter_count } loss: { l } \" ) optimizer . zero_grad () l . backward () optimizer . step () print ( f \" \\n w: { m . w } \" ) There are only two differences between the data parallelism training code and the stand-alone single-card script: Use DistributedDataParallel to wrap the module object ( m = ddp(m) ) Use get_rank to get the current device number and distribute the data to the device. Then use launcher to run the script, leave everything else to OneFlow, which makes distributed training as simple as stand-alone single-card training: python3 - m oneflow . distributed . launch -- nproc_per_node 2 ./ ddp_train . py","title":"Run Data Parallel Training With DistributedDataParallel"},{"location":"parallelism/05_ddp.html#distributedsampler","text":"The data used is manually distributed in this context to highlight DistributedDataParallel . However, in practical applications, you can directly use DistributedSampler with data parallel. DistributedSampler will instantiate the Dataloader in each process, and each Dataloader instance will load a part of the complete data to automatically complete the data distribution.","title":"DistributedSampler"},{"location":"parallelism/06_pipeline.html","text":"PIPELINING PARALLELISM \u00b6 We have introduced the characteristics of pipelining parallelism in COMMON DISTRIBUTED PARALLEL STRATEGIES . From OneFlow's global view , pipelining can be achieved by simply setting the placement attribute of Tensor. The following code is a simple example that will run the network in QUICKSTART with pipelining parallelism. nn.Flatten , nn.Linear(28*28, 512) and nn.ReLU() run on GPU0, and the rest layers of the network run on GPU1. Code import oneflow as flow BATCH_SIZE = 16 BROADCAST = [ flow . sbp . broadcast ] P0 = flow . placement ( \"cuda\" , [ 0 ]) P1 = flow . placement ( \"cuda\" , [ 1 ]) class Stage0Module ( flow . nn . Module ): def __init__ ( self ): super () . __init__ () self . flatten = flow . nn . Flatten () self . linear0 = flow . nn . Linear ( 28 * 28 , 512 ) self . relu0 = flow . nn . ReLU () def forward ( self , x ): out = self . flatten ( x ) out = self . linear0 ( out ) out = self . relu0 ( out ) return out class Stage1Module ( flow . nn . Module ): def __init__ ( self ): super () . __init__ () self . linear1 = flow . nn . Linear ( 512 , 512 ) self . relu1 = flow . nn . ReLU () self . linear2 = flow . nn . Linear ( 512 , 10 ) self . relu2 = flow . nn . ReLU () def forward ( self , x ): out = self . linear1 ( x ) out = self . relu1 ( out ) out = self . linear2 ( out ) out = self . relu2 ( out ) return out class PipelineModule ( flow . nn . Module ): def __init__ ( self ): super () . __init__ () self . m_stage0 = Stage0Module () self . m_stage1 = Stage1Module () self . m_stage0 . to_global ( placement = P0 , sbp = BROADCAST ) self . m_stage1 . to_global ( placement = P1 , sbp = BROADCAST ) def forward ( self , x ): out_stage0 = self . m_stage0 ( x ) in_stage1 = out_stage0 . to_global ( placement = P1 , sbp = BROADCAST ) out_stage1 = self . m_stage1 ( in_stage1 ) return out_stage1 module_pipeline = PipelineModule () sgd = flow . optim . SGD ( module_pipeline . parameters (), lr = 0.001 ) class PipelineGraph ( flow . nn . Graph ): def __init__ ( self ): super () . __init__ () self . module_pipeline = module_pipeline self . module_pipeline . m_stage0 . config . set_stage ( stage_id = 0 , placement = P0 ) self . module_pipeline . m_stage1 . config . set_stage ( stage_id = 1 , placement = P1 ) self . loss_fn = flow . nn . CrossEntropyLoss () self . config . set_gradient_accumulation_steps ( 2 ) self . add_optimizer ( sgd ) def build ( self , x , y ): out = self . module_pipeline ( x ) loss = self . loss_fn ( out , y ) loss . backward () return loss graph_pipeline = PipelineGraph () x = flow . randn ( BATCH_SIZE , 1 , 28 , 28 ) x = x . to_global ( P0 , BROADCAST ) y = flow . randint ( 0 , 10 , ( BATCH_SIZE ,)) y = y . to_global ( P1 , BROADCAST ) for i in range ( 20 ): loss = graph_pipeline ( x , y ) print ( loss . to_local ()) When the code above is saved as a script ( pipeline.py ), it can be then launched by the launch module : python3 -m oneflow.distributed.launch --nproc_per_node 2 ./pipeline.py More Details \u00b6 Setting placement and SBP \u00b6 Setting up the placement and SBP at the begining: BROADCAST = [ flow . sbp . broadcast ] P0 = flow . placement ( \"cuda\" , [ 0 ]) P1 = flow . placement ( \"cuda\" , [ 1 ]) P0 and P1 represent the 0 th GPU and the 1 st GPU on the 0 th machine respectively. By calling nn.Module.to_global or Tensor.to_global , the model or tensor will be distributed to the devices specified before, breaking a network into stages. Here we define a PipelineModule that specifically sets the pipeline for each stage. class PipelineModule ( flow . nn . Module ): def __init__ ( self ): #... self . m_stage0 . to_global ( placement = P0 , sbp = BROADCAST ) self . m_stage1 . to_global ( placement = P1 , sbp = BROADCAST ) def forward ( self , x ): out_stage0 = self . m_stage0 ( x ) in_stage1 = out_stage0 . to_global ( placement = P1 , sbp = BROADCAST ) out_stage1 = self . m_stage1 ( in_stage1 ) return out_stage1 Transforming the Local Tensor to the Global Tensor \u00b6 The example uses randomly generated data as input. x = flow . randn ( BATCH_SIZE , 1 , 28 , 28 ) x = x . to_global ( P0 , BROADCAST ) The launch will start two processes when you launch the training by the launch module because the command-line parameter is --nproc_per_node 2 . Both processes will execute the code in the script. The statement x = flow.randn(BATCH_SIZE, 1, 28, 28) returns the Local Tensor (the local data only valid in current process). when running x = x.to_global(P0, BROADCAST) , OneFlow will automatically integrate the Local Tensor of all processes into the Global Tensor. In practice, each computing device can load data locally, and then convert the Local Tensor to the Global Tensor via to_global . Stage ID and Settings for Gradient Accumulation \u00b6 We can call the method config.set_stage of Module Config to set Stage ID and related Placement. The Stage ID are numbered from 0. We can call the method config.set_gradient_accumulation_steps to set the step size of gradient accumulation. The information needed to implement micro-batch in pipelining parallelism can be obtained by these two configurations. self . module_pipeline . m_stage0 . config . set_stage ( stage_id = 0 , placement = P0 ) self . module_pipeline . m_stage1 . config . set_stage ( stage_id = 1 , placement = P1 ) self . config . set_gradient_accumulation_steps ( 2 )","title":"Pipelining Parallelism"},{"location":"parallelism/06_pipeline.html#pipelining-parallelism","text":"We have introduced the characteristics of pipelining parallelism in COMMON DISTRIBUTED PARALLEL STRATEGIES . From OneFlow's global view , pipelining can be achieved by simply setting the placement attribute of Tensor. The following code is a simple example that will run the network in QUICKSTART with pipelining parallelism. nn.Flatten , nn.Linear(28*28, 512) and nn.ReLU() run on GPU0, and the rest layers of the network run on GPU1. Code import oneflow as flow BATCH_SIZE = 16 BROADCAST = [ flow . sbp . broadcast ] P0 = flow . placement ( \"cuda\" , [ 0 ]) P1 = flow . placement ( \"cuda\" , [ 1 ]) class Stage0Module ( flow . nn . Module ): def __init__ ( self ): super () . __init__ () self . flatten = flow . nn . Flatten () self . linear0 = flow . nn . Linear ( 28 * 28 , 512 ) self . relu0 = flow . nn . ReLU () def forward ( self , x ): out = self . flatten ( x ) out = self . linear0 ( out ) out = self . relu0 ( out ) return out class Stage1Module ( flow . nn . Module ): def __init__ ( self ): super () . __init__ () self . linear1 = flow . nn . Linear ( 512 , 512 ) self . relu1 = flow . nn . ReLU () self . linear2 = flow . nn . Linear ( 512 , 10 ) self . relu2 = flow . nn . ReLU () def forward ( self , x ): out = self . linear1 ( x ) out = self . relu1 ( out ) out = self . linear2 ( out ) out = self . relu2 ( out ) return out class PipelineModule ( flow . nn . Module ): def __init__ ( self ): super () . __init__ () self . m_stage0 = Stage0Module () self . m_stage1 = Stage1Module () self . m_stage0 . to_global ( placement = P0 , sbp = BROADCAST ) self . m_stage1 . to_global ( placement = P1 , sbp = BROADCAST ) def forward ( self , x ): out_stage0 = self . m_stage0 ( x ) in_stage1 = out_stage0 . to_global ( placement = P1 , sbp = BROADCAST ) out_stage1 = self . m_stage1 ( in_stage1 ) return out_stage1 module_pipeline = PipelineModule () sgd = flow . optim . SGD ( module_pipeline . parameters (), lr = 0.001 ) class PipelineGraph ( flow . nn . Graph ): def __init__ ( self ): super () . __init__ () self . module_pipeline = module_pipeline self . module_pipeline . m_stage0 . config . set_stage ( stage_id = 0 , placement = P0 ) self . module_pipeline . m_stage1 . config . set_stage ( stage_id = 1 , placement = P1 ) self . loss_fn = flow . nn . CrossEntropyLoss () self . config . set_gradient_accumulation_steps ( 2 ) self . add_optimizer ( sgd ) def build ( self , x , y ): out = self . module_pipeline ( x ) loss = self . loss_fn ( out , y ) loss . backward () return loss graph_pipeline = PipelineGraph () x = flow . randn ( BATCH_SIZE , 1 , 28 , 28 ) x = x . to_global ( P0 , BROADCAST ) y = flow . randint ( 0 , 10 , ( BATCH_SIZE ,)) y = y . to_global ( P1 , BROADCAST ) for i in range ( 20 ): loss = graph_pipeline ( x , y ) print ( loss . to_local ()) When the code above is saved as a script ( pipeline.py ), it can be then launched by the launch module : python3 -m oneflow.distributed.launch --nproc_per_node 2 ./pipeline.py","title":"PIPELINING PARALLELISM"},{"location":"parallelism/06_pipeline.html#more-details","text":"","title":"More Details"},{"location":"parallelism/06_pipeline.html#setting-placement-and-sbp","text":"Setting up the placement and SBP at the begining: BROADCAST = [ flow . sbp . broadcast ] P0 = flow . placement ( \"cuda\" , [ 0 ]) P1 = flow . placement ( \"cuda\" , [ 1 ]) P0 and P1 represent the 0 th GPU and the 1 st GPU on the 0 th machine respectively. By calling nn.Module.to_global or Tensor.to_global , the model or tensor will be distributed to the devices specified before, breaking a network into stages. Here we define a PipelineModule that specifically sets the pipeline for each stage. class PipelineModule ( flow . nn . Module ): def __init__ ( self ): #... self . m_stage0 . to_global ( placement = P0 , sbp = BROADCAST ) self . m_stage1 . to_global ( placement = P1 , sbp = BROADCAST ) def forward ( self , x ): out_stage0 = self . m_stage0 ( x ) in_stage1 = out_stage0 . to_global ( placement = P1 , sbp = BROADCAST ) out_stage1 = self . m_stage1 ( in_stage1 ) return out_stage1","title":"Setting placement and SBP"},{"location":"parallelism/06_pipeline.html#transforming-the-local-tensor-to-the-global-tensor","text":"The example uses randomly generated data as input. x = flow . randn ( BATCH_SIZE , 1 , 28 , 28 ) x = x . to_global ( P0 , BROADCAST ) The launch will start two processes when you launch the training by the launch module because the command-line parameter is --nproc_per_node 2 . Both processes will execute the code in the script. The statement x = flow.randn(BATCH_SIZE, 1, 28, 28) returns the Local Tensor (the local data only valid in current process). when running x = x.to_global(P0, BROADCAST) , OneFlow will automatically integrate the Local Tensor of all processes into the Global Tensor. In practice, each computing device can load data locally, and then convert the Local Tensor to the Global Tensor via to_global .","title":"Transforming the Local Tensor to the Global Tensor"},{"location":"parallelism/06_pipeline.html#stage-id-and-settings-for-gradient-accumulation","text":"We can call the method config.set_stage of Module Config to set Stage ID and related Placement. The Stage ID are numbered from 0. We can call the method config.set_gradient_accumulation_steps to set the step size of gradient accumulation. The information needed to implement micro-batch in pipelining parallelism can be obtained by these two configurations. self . module_pipeline . m_stage0 . config . set_stage ( stage_id = 0 , placement = P0 ) self . module_pipeline . m_stage1 . config . set_stage ( stage_id = 1 , placement = P1 ) self . config . set_gradient_accumulation_steps ( 2 )","title":"Stage ID and Settings for Gradient Accumulation"}]}